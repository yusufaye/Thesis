\section{Introduction}\label{sec:intro}

Video camera flows are a pervasive source of information. Cities are increasingly deploying closed-circuit cameras that are used for safety,
security, and traffic control applications~\cite{parascandola_2018,ratcliffe_2020,grassi2017parkmaster}. To process the incoming videos streams in real-time, live video analytics architectures have been proposed to support a multitude of applications across navigation, safety, and control~\cite{ananthanarayanan2017real,ao2018sprocket,zeng2020distream,jiang2018chameleon,zhang2017live}. However, the increasing amounts of video data produced by available cameras has forced the community to move away from centralized cloud-based architectures~\cite{yi2017lavea,ananthanarayanan2017real}.

Edge-based live video analytics are a promising approach to reduce bandwidth overheads caused by the transmission of raw video streams to centralized clouds~\cite{hu2023edge}. Unfortunately, in contrast with the quasi-infinite resources of data-centers, edge devices are often co-located with existing network equipment and deploy limited computational resources. As a result, they can rapidly become overloaded by the incoming video frames, causing data loss and reduced accuracy~\cite{jiang2018chameleon,jain2020spatula}. To address this challenge, various solutions have been proposed to distribute the workload across locations, ranging from vertically splitting the processing between edge devices and the central cloud, wherein excess traffic is offloaded to the cloud~\cite{hung2018videoedge,chen2015glimpse} to horizontally across different edge clusters, exploiting the dynamicity of processing requests incurred at each location~\cite{jain2020spatula,zhang2019hetero}. However, all these architectures assume that video flows are generated from fixed cameras (\eg, traffic cameras deployed at street corners) and that their workflows present predictable patterns~\cite{jiang2018chameleon}.

With the rising penetration of mobile cameras, an opportunity emerges to include these cameras in the design of video analytics architectures. Mobile devices, such as cars or drones, come equipped with high-quality camera sensors that have been demonstrated to be very effective for navigation, safety, and control applications~\cite{wang2018bandwidth,grassi2017parkmaster}. These cameras often have the unique advantage of being {\em in the right place at the right time}. Considering in particular that mobile cameras are generally owned by private individuals (\eg, on-board car cameras), there is evidence of the growing use of these camera streams for personal and societal security~\cite{rea2018dash,giovannini2021importance}. Integrating them into existing architectures will support analytics applications that interest camera owners, such as accident detection, crash analytics, and reconstruction. However, the scenes that mobile cameras capture vary more rapidly than for fixed cameras.

Owing to the high mobility and dynamic content of mobile cameras, we identify three key challenges in accommodating these cameras in video analytics architectures. First, the workload generated by mobile cameras is more dynamic and unpredictable, requiring constant adjustments to the processing infrastructure. Existing solutions such as Distream~\cite{zeng2020distream} recognize the need for adaptation to varying processing loads, but ultimately fall short of developing a solution that adjusts at the rate imposed by mobile cameras (we expose such problem in detail in~\Cref{sec:evaluation}). Second, the continuously changing scenes captured by mobile cameras make customary processing pipelines ineffective. For example, a typical pipeline used to process a video feed incoming from a fixed camera might employ a lightweight background subtractor to isolate moving objects, reducing the need for deploying more expensive object detection modules~\cite{zeng2020distream}. Finally, as mobile cameras appear and disappear from the deployment, they generate constant changes in the deployment configuration and the number of sources to process. This introduces the need to deploy different processing pipelines for different cameras. Overall, existing video analytics architectures are ill equipped to handle such video traffic.

In this chapter, we tackle the challenge of designing a video analytics architecture capable of handling a mix of fixed and mobile video camera flows. We build on the recent idea~\cite{zeng2020distream} that the dynamicity of the workload generated by video cameras can be exploited to balance the load across different edge clusters. However, we observe that:~(i)~While highly dynamic, the workload generated by mobile cameras is not completely unpredictable. In fact, the scenes captured by mobile cameras reveal sufficient patterns to make it possible to predict the amount of objects that will need to be processed in the next few seconds.~(ii)~Enabling the coexistence of diverse video analytics pipelines requires dividing the processing problem into smaller components that function independently, even when pipelines later converge to the same set of modules.~(iii)~The constant changes in the deployment configuration, such as the addition or removal of processing components, require an online approach to load balancing that can adapt to these changes without requiring a complete reboot of the processing pipelines.

We leverage these observations to build~\videojam{}, a load balancing solution for live video analytics.~\videojam{} deploys a set of load balancers co-located with every task in the analytics pipeline. Each load balancer monitors the incoming flow of frames or objects to process and periodically shares this information with its neighbors. Based on the information collected, the load balancers take independent decisions on how much traffic to process locally and whether to offload some of their workload to less-loaded neighbors. To make these decisions,~\videojam{} uses a lightweight machine learning model to predict the incoming workload for each processing component for the near future, as well as for its neighbors. Finally, the load balancers recover to eventual prediction errors via a congestion prevention signalling system.~\videojam{} operates independently of deployed configurations and dynamically adapts to handle eventual changes (\eg, new camera arrivals or departures) without requiring hard reboots balancing incoming traffic accordingly. Our approach features a unique combination of horizontal distribution and per-function type load balancing, powered by short-term forecasts of incoming loads.

We implement and evaluate~\videojam{} using a typical vehicular safety application, \ie, vehicle plate detection, adapted to both fixed and mobile cameras. Our evaluation shows that~\videojam{} can achieve a 2.91$\times$ lower response time and reduce 4.64$\times$ video frames loss than a state-of-the-art approach~\cite{zeng2020distream}, while also reducing network overheads. Further, we demonstrate the ability of~\videojam{} to dynamically adapt to changes in the deployment configuration, \ie, change in number of function replicas or number of cameras in the system, without requiring any hard reboots.~\videojam{} adapts to the new configuration in less than 28~seconds (\ie, +27\% the failure time), to the incurred change. We release~\videojam{} as open source software for the community to use and extend.
