\setchapterpreamble[u]{\margintoc}
\chapter{Related work}\labch{related_work}


\section{Edge Computing and Video Analytics}

Edge computing represents a transformative shift in distributed computing, wherein computational tasks and data storage are relocated closer to the data source. This paradigm addresses several limitations inherent in centralized cloud architectures, notably by reducing latency, minimizing bandwidth consumption, enhancing data security, and enabling real-time responsiveness. As edge devices, such as sensors, cameras, and IoT nodes, typically operate under constrained computational and energy resources, a variety of architectural models have emerged. These range from hybrid edge-cloud systems, which balance local and remote processing, to fully autonomous edge analytics platforms tailored for specific domains like industrial IoT.

The integration of Graphics Processing Units (GPUs) in edge computing has brought significant advantages. GPUs are designed to handle parallel processing, making them ideal for compute-intensive tasks such as machine learning, computer vision, and data analytics. The use of GPUs in edge computing provides several benefits, including improved performance, increased efficiency, and enhanced scalability. These advantages have paved the way for GPUs to become a cornerstone in enabling advanced edge computing capabilities. By harnessing their parallel processing power, edge systems now support a wide array of real-time applications ranging from autonomous vehicles and smart surveillance to immersive AR/VR experiences and responsive healthcare diagnostics. These applications benefit from edge computing's ability to process data in real-time, reducing latency and improving decision-making. For instance, in IoT, edge computing enables real-time monitoring and control of industrial equipment, improving efficiency and reducing downtime.

% The versatility of edge computing has catalyzed its adoption across numerous sectors. In the Internet of Things (IoT), edge computing facilitates rapid decision-making by processing sensor data locally. Autonomous vehicles rely on edge systems to interpret environmental inputs in real time, ensuring safe navigation. Healthcare applications benefit from immediate diagnostics and monitoring, while surveillance and security systems leverage edge analytics for prompt threat detection. Additionally, augmented and virtual reality (AR/VR) platforms utilize edge computing to deliver immersive experiences with minimal latency.

Among these applications, video analytics stands out as a particularly demanding and impactful use case~\cite{}. In contexts such as smart cities, autonomous driving, and public safety, edge-based video analytics enables timely interpretation of visual data. A typical video analytics pipeline comprises several stages: data ingestion, preprocessing, feature extraction, model inference, and post-processing. Each stage imposes distinct computational and memory requirements, which are often challenging to meet within the limited capabilities of edge devices.

These constraints underscore the need for innovative solutions. Edge devices may lack sufficient processing power, storage capacity, or communication bandwidth to support complex video analytics workloads. Consequently, researchers have explored a range of strategies to optimize resource utilization while preserving real-time performance. These include lightweight model architectures, distributed processing frameworks, and adaptive scheduling algorithms that dynamically allocate tasks across heterogeneous edge nodes.

To address these issues, researchers have proposed a spectrum of solutions aimed at optimizing performance while preserving the benefits of edge locality. Techniques such as model compression and pruning reduce the computational footprint of deep learning algorithms, enabling their deployment on resource-constrained devices. Distributed processing frameworks allow workloads to be partitioned and executed collaboratively across multiple edge nodes, thereby enhancing scalability and fault tolerance. Additionally, adaptive scheduling algorithms and energy-aware resource management strategies have been developed to balance performance with sustainability.



% Edge computing is a distributed computing paradigm that brings computing and data storage closer to the source where the data is produced. This approach aims to reduce latency, bandwidth usage, security risks, and improve real-time processing~\cite{}. Given the limited resources of edge devices, several architectures have emerged. Edge computing systems exist in a wide range of deployments, from hybrid edge and cloud to fully hosted IoT edge analytics~\cite{xu2023edge,hu2023edge}. Edge devices, such as sensors, cameras, and IoT devices, generate data that is then collected and processed by edge gateways or the cloud in a hybrid environment. There is a wide range of applications for edge computing, including: IoT, autonomous vehicles, healthcare, surveillance and security, AR/VR (augmented reality/virtual reality), etc.

% Video analytics is a key application of edge computing, particularly in areas such as surveillance, autonomous vehicles, and smart cities. The typical architecture of a video analytics pipeline includes data ingestion, preprocessing, feature extraction, model inference, and post-processing. However, video analytics in edge environments presents unique challenges, such as real-time processing requirements and resource limitations. For example, edge devices may not have sufficient power, processing power, storage, or communication resources to handle video analytics workloads. A significant range of research has been conducted to improve resource utilization while maintaining high real-time performance. Studies cover various aspects, from architecture deployment to resource and energy consumption efficiency.

% The benefits of edge computing for video analytics include reduced latency, improved real-time processing, and bandwidth conservation. For example, in a surveillance system, edge computing can enable real-time object detection and tracking, allowing for quick responses to security threats. In addition, edge computing can reduce the amount of data that needs to be transmitted to the cloud or central server, thereby reducing bandwidth usage. However, edge computing also presents several challenges, including resource limitations, security, and management. To address these challenges, researchers and practitioners have proposed various techniques, such as edge computing-based video analytis frameworks, model optimization, and distributed processing. For example, a study by Zhang et al. (2020) [4] proposes a distributed video analytis framework that leverages edge computing and cloud computing to optimize processing and reduce latency.

% Edge computing is a distributed computing paradigm that brings computation and data storage closer to the source where data is produced. This approach aims to reduce latency, bandwidth usage, security, and improve real-time processing~\cite{}. With the limited resources of edge devices multiple architectures have emerged. Edge computing systems exist across a spectrum of deployments, from hybrid edge and cloud to fully hosted IoT edge analytics~\cite{xu2023edge,hu2023edge}. Edge devices, such as sensors, cameras, and IoT devices, generate data, which is then collected and processed by edge gateways or cloud in hybrid. There is a wide range of applications for edge computing, including: IoT, Autonomous vehicles, Healthcare, Surveillance and security, AR/VR (Augmented Reality/Virtual Reality), etc.

% Video analytics is a critical application of edge computing, particularly in domains such as surveillance, autonomous vehicles, and smart cities. The typical pipeline architecture of video analytics consists of data ingestion, preprocessing, feature extraction, model inference, and post-processing. However, video analytics in edge environments presents unique challenges, such as real-time processing requirements and resource limitations. For instance, edge devices may not have sufficient energy, processing, storage, or communication resources to handle video analytics workloads. Un ensemble important de recherches ont ete produits pour ameliorer l'utilisation des resources while maitaining a high performance in real-time. The studies vary from architecture deployment to efficient resource and power consumption.

% The benefits of edge computing for video analytics include reduced latency, improved real-time processing, and conservation of bandwidth. For example, in a surveillance system, edge computing can enable real-time object detection and tracking, allowing for swift response to security threats. Additionally, edge computing can reduce the amount of data that needs to be transmitted to the cloud or central server, thereby reducing bandwidth usage. However, edge computing also presents several challenges, including resource limitations, security, and management. 

% To address these challenges, researchers and practitioners have proposed various techniques, such as edge-based video analytics frameworks, model optimization, and distributed processing. For instance, a study by Zhang et al. (2020) [4] proposes a distributed video analytics framework that leverages edge computing and cloud computing to optimize processing and reduce latency.


\section{Load Balancing for Live Video Analytics}
% Load Balancing Techniques: Survey existing load balancing techniques for edge computing, with a focus on those relevant to video analytics. Discuss distributed load balancing approaches and the use of machine learning models in load balancing.

Recent advancements in video analytics have led to the development of various techniques aimed at enhancing application performance. These efforts span multiple dimensions, including architectural design, pipeline optimization, and data privacy. As the demand for real-time, scalable video processing grows—particularly in edge environments—efficient resource management and adaptability have become central challenges.

\paragraph{Video Analytics System Architecture.}
Early solutions focused on optimizing computational resources for video streams from fixed cameras. For example, Chameleon dynamically reconfigures pipeline placement to reduce resource consumption with minimal accuracy loss. Similarly, Spatula exploits spatial and temporal correlations among camera feeds to lower network and computation costs. However, these approaches are limited by their reliance on static camera inputs and do not address the complexities introduced by mobile video sources.

\paragraph{Deployment Strategies.}
To overcome the limitations of static architectures, newer frameworks have adopted distributed deployment models. Distream exemplifies this shift by partitioning analytics pipelines between smart cameras and edge nodes, adapting to workload dynamics to maintain low latency and high throughput. Likewise, VideoStorm and the work referenced in [A] distribute vision modules across multiple edge nodes, enabling parallel processing and improved resource utilization. These strategies offer greater flexibility and scalability, making them more suitable for dynamic environments.

\paragraph{Load Balancing in Edge Video Analytics.}
As video analytics applications scale across distributed infrastructures, load balancing becomes essential for maintaining accuracy and efficiency. Traditional methods, such as those used in VideoStorm, Spatula, Hetero-edge, and VideoEdge, relied on static configurations and centralized cloud offloading. These approaches often resulted in network bottlenecks and were slow to adapt to changing workloads.
Recent developments have introduced more dynamic load balancing mechanisms. Distream, for example, offers pipeline-level load balancing that adapts to the variability of video content. However, its reliance on predictable long-term patterns—such as daily traffic fluctuations—limits its responsiveness to abrupt changes in video content or camera behavior.

\paragraph{Workload Prediction.}

To enhance load balancing, predictive models based on machine learning have been explored. Reinforcement learning, as used in [X], enables real-time task assignment but demands substantial computational resources and continuous training to mitigate concept drift. Linear regression models, as in [Z], offer a simpler alternative but may lack the sophistication needed for complex scene dynamics.
Given the constraints of edge devices and the unpredictability of mobile camera inputs, there is a growing need for lightweight forecasting models. These models should be capable of short-term trend prediction, operate efficiently on resource-constrained hardware, and support real-time decision-making.


The evolution of video analytics has moved from static, centralized systems to dynamic, distributed architectures. While significant progress has been made in deployment and load balancing strategies, future research must focus on developing lightweight, adaptive forecasting techniques. These solutions should be tailored to the constraints of edge environments and the variability introduced by mobile video sources, ensuring scalable and responsive video analytics.


% In recent years, several techniques have been developed to improve the performance of video analytics applications~\cite{ibrahim2021survey,xu2023edge,hu2023edge}. Such a topic has been tackled from different perspectives, including the design of different data processing architectures~\cite{jain2020spatula,zhang2017live,jiang2018chameleon}, the improvement of pipelines' processing~\cite{fouladi2017encoding,chen2015glimpse,padmanabhan2021towards,padmanabhan2023gemel} and the privacy of the extracted data~\cite{cangialosi2022privid,poddar2020visor,wu2021pecam}.

% \paragraph{Architecture scaling.}
% Different approaches have been proposed to efficiently manage the computational resources for video analytics~\cite{jain2020spatula,zhang2017live,jiang2018chameleon,201465videostorm}. Chameleon, presented in~\cite{jiang2018chameleon}, frequently reconfigures the placement of video analytics pipelines to reduce resource consumption with small loss in accuracy. Another example is Spatula~\cite{jain2020spatula}, which exploits the spatial and temporal correlations among different camera flows to reduce the network and computation costs. However, such solutions only consider video flows coming from fixed cameras.

% \paragraph{Deployment strategies.} 
% Other solutions mainly focused on the deployment strategies of video analytics applications~\cite{zeng2020distream,201465videostorm,rachuri2021decentralized}. Distream~\cite{zeng2020distream} is a distributed framework based capable of adapting to workload dynamics to achieve low-latency, high-throughput and scalable live video analytics. Pipelines are deployed on both the smart cameras and the edge, and are jointly partitioned so that part is computed on the smart cameras, while the rest is sent towards the edge, which has more computing power at its disposal. The deployment of application pipelines is adapted to the varying processing load, however there is a lack of adaptability required by the rate of mobile cameras.
% The work in~\cite{rachuri2021decentralized} presents experimental results showing that smartly distributing and processing vision modules in parallel across available edge compute nodes, can ultimately lead to better resource utilization and improved performance. The same approach is also used by VideoStorm~\cite{201465videostorm} which places different video functions across multiple available workers to satisfy users' requests. We assume a deployment of pipelines in line with this latter work given the higher flexibility, higher scalability and the better use of resources of this approach. 

% \paragraph{Load balancing strategies.}
% Once video analytics applications have been deployed on a distributed edge infrastructure, load balancing strategies play a fundamental role in guaranteeing requirements of accuracy and efficiency. As a result, The concept of implementing load balancing for video analytics applications has gained popularity, particularly with the emergence of edge video analytics and its associated limitations. Historically, load balancing was performed between edge nodes and central clouds (\eg, VideoStorm~\cite{201465videostorm}), but this method became impractical due to increased network traffic and potential network bottlenecks. Load balancing between edge locations has been the subject of several works, including Spatula~\cite{jain2020spatula}, Hetero-edge~\cite{zhang2019hetero}, and VideoEdge~\cite{hung2018videoedge} (albeit VideoEdge still relies on offloading to remote clouds). However, these works focused on the production of static configurations, where each processed video is directed to a predetermined path through the deployed processing functions. As any change of configuration impacts the deployed functions, configuration updates occur over longer timescales, making these approaches unsuitable for highly variable loads. More recently, Distream~\cite{zeng2020distream} recognized the need for rapidly adapting to varying loads within a video, proposing an adaptable load balancing solution that splits the load balancing decisions at the pipeline level. However, it assumes that workflows present predictable longer-term patterns, allowing reconfiguration decisions only to be taken at longer timescales, for example when traffic conditions change during the day due to commute patterns.

% \paragraph{Workload prediction.}
% Workload predictions, based on machine learning models, have been proved to be effective in the design of load balancing policies~\cite{heinze2014auto,gedik2013elastic,kombi2017preventive,zeng2020distream}. In~\cite{yuan2021online}, authors used reinforcement learning for performing real-time estimation for dynamic assigning task to the optimal server. While the work in~\cite{kombi2017preventive}, focused on load forecasting by using linear regression model. However, reinforcement learning solution, even though effective, require a significant amount of resources and continuous online training to avoid concept-drift problems~\cite{zhang2020reinforcement}. Such a solution method is not suitable for the computational-constrained devices at the edge. In addition, the rapid changes in scenes captured by mobile cameras are more difficult to predict. Therefore, there is a need of lightweight forecasting models that can predict short-term trends, suitable for edge devices and fast enough for real-time prediction.



\section{Inference Serving and Resource Management in Edge Cloud Computing}

With the rise of deep learning applications deployed as online services, efficiently managing inference workloads in GPU datacenters has become a pressing concern. Unlike training tasks, inference jobs present unique constraints, requiring high accuracy, low latency, and cost-effectiveness. These goals often conflict, making it essential to design scheduling systems that can balance trade-offs without compromising overall performance.

\paragraph{Limitations of Early Serving Systems.}
Initial solutions such as Clipper and TensorFlow-Serving focused on simplifying model deployment and supporting multiple frameworks. While they introduced dynamic scaling and model abstraction, they did not account for resource interference prior to deployment. Clockwork attempted to provide predictable performance by leveraging the deterministic nature of DNN inference under full GPU load. However, its design restricted execution to one inference at a time, resulting in underutilized GPU resources.

\paragraph{Multi-Tenant DNN Inference on Shared GPUs.}
A subsequent line of research investigates techniques for optimizing model performance in environments characterized by concurrent execution, where multiple models or processes must contend for shared GPU resources. This body of work typically examines scheduling algorithms, resource allocation strategies, and system-level adaptations designed to maintain throughput and minimize latency under constrained computational conditions. For example, Colti demonstrated that colocating DNN tasks can improve throughput. Another work proposed scheduling operators across dedicated CUDA streams, splitting execution into stages to balance concurrency and latency. Miriam introduced elastic kernels for edge GPUs, allowing dynamic remapping based on task priority. While these methods improve utilization, they operate reactively and do not explore optimal colocation strategies beforehand.

\paragraph{Interference-Aware Inference Serving.}
Recent studies have shifted toward proactive scheduling by modeling and predicting interference between concurrently running models. Some approaches estimate latency degradation using coarse features like buffer usage and PCIe traffic, but lack granularity. Scrooge profiles concurrency thresholds for identical models, though its scalability is limited by the number of possible combinations. Proteus introduces adaptive batching and model variant selection to balance accuracy and latency, but its rigid placement strategy restricts colocation flexibility. These systems represent progress toward interference-aware scheduling, yet often rely on profiling techniques that are either too coarse or computationally intensive.

\paragraph{Fine-Grained GPU Scheduling for DNN Inference: Kernel-Level Insights and Limitations}
Recent approaches to GPU scheduling for deep neural network inference, such as iGnifer and Usher, emphasize low-level profiling to mitigate performance interference in shared environments. By analyzing metrics like core launches, cache usage, and kernel occupancy, these systems reveal that traditional coarse-grained models overlook critical scheduling dynamics. Usher further distinguishes between compute- and memory-intensive workloads, noting that large language models often strain memory bandwidth. Both rely on NVIDIA's Multi-Process Service (MPS) for resource partitioning, which limits their applicability on edge platforms lacking MPS support. This highlights the need for interference-aware strategies that operate independently of cloud-specific infrastructure.

The evolution of inference scheduling reflects a shift from general-purpose deployment tools to sophisticated, interference-aware systems. While early solutions prioritized ease of use, newer frameworks aim to anticipate and mitigate resource contention dynamically. We need a much fine-grained profiling, flexible model placement, and adaptive execution strategies to meet the complex demands of modern inference workloads.
