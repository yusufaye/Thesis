\setchapterpreamble[u]{\margintoc}
\chapter{Related work}\label{ch:related_work}

This section reviews the latest research and advances in edge cloud computing, focusing on three main themes: video analytics, workload distribution, and AI application scheduling. We begin by defining cloud computing and the motivations for adopting edge-based solutions, particularly in the field of video analytics. The following sections delve deeper into research on live video analytics, followed by a detailed discussion on workload balancing (\cref{sec:load_balancing_for_live_video_analytics}) and, finally, query processing, scheduling, and model placement strategies (\cref{sec:inference_serving_and_resource_management_in_edge_cloud_computing}).


\section{Edge Computing and Video Analytics}

Cloud computing is a model that uses centralized data centers to provide scalable storage and processing capabilities on a large scale over the Internet. Although this model has facilitated significant digital transformation, it presents several challenges for latency-sensitive and bandwidth-intensive applications, for emerging real-time applications.

The increasing need for real-time responsiveness, data privacy, and network efficiency has prompted a move toward edge computing, where processing occurs closer to the data source. This shift is especially beneficial for video analytics, which frequently relies on continuous, high-volume data streams from distributed sensors and cameras. This architectural transformation addresses several limitations inherent in centralized cloud infrastructures, notably by reducing latency, minimizing bandwidth consumption, enhancing data security, and enabling real-time responsiveness. Given that edge devices, such as sensors, cameras, and \acrfull{iot} nodes, typically operate under constrained computational and energy resources, a variety of architectural models have emerged. These include hybrid edge-cloud systems, which balance local and remote processing, as well as fully autonomous edge analytics platforms designed for domain-specific applications, particularly within industrial \acrshort{iot} environments.

\paragraph{Edge computing applications.}
Edge computing has emerged as a transformative technology across diverse sectors, enhancing operational efficiency and enabling real-time decision-making. In smart agriculture, it facilitates precision farming through sensor, drone-based crop monitoring, promoting optimal resource allocation and early disease detection~\cite{akhtar2021smart,junaidi2025deep,dhifaoui2022cloud}. For instance, artificial intelligence (AI) can significantly improve efficiency in the following areas: crop cultivation, yield forecasting, disease monitoring, supply chain operations, food waste reduction, and resource management~\cite{pandey2024towards}.
Urban areas reap considerable benefits from edge computing systems that manage traffic, pollution, temperature rises, and security. These systems use real-time sensor data to reduce traffic congestion and improve public safety through proactive monitoring and response~\cite{xu2023mobile,hossain2018edge}.
Edge computing in industrial environments enables predictive maintenance by monitoring machines in real time to prevent breakdowns, while promoting seamless human-robot collaboration for safer and more efficient operations. These innovations are driving the vision of Industry 5.0, which advocates for smart, human-centered manufacturing~\cite{sharma2024edge}.
In healthcare, wearable devices enable continuous monitoring of vital signs, allowing prompt medical responses to anomalies~\cite{rancea2024edge}. The overview highlights the crucial role of edge computing in improving responsiveness, accuracy, and strategic decision-making across various industries and applications.

\paragraph{Video Analytics as a Critical Edge Application.}
Among the diverse applications of edge computing, video analytics emerges as a particularly demanding and impactful use case. In domains such as smart cities, autonomous driving, and public safety, edge-based video analytics enables the timely interpretation of visual data~\cite{hu2023edge,xu2023edge}. A typical video analytics pipeline comprises several stages: data ingestion, preprocessing, feature extraction, model inference, and post-processing. Each stage imposes distinct computational and memory requirements, which are often challenging to satisfy within the limited capabilities of edge devices.

\paragraph{Resource Constraints and Architectural Challenges.}
The computational limitations of edge devices, manifested in restricted processing power, limited storage capacity, and constrained communication bandwidth, pose significant challenges to the deployment of complex video analytics workloads~\cite{badidi2023opportunities,kumar2021resource,jiang2018chameleon,wang2018bandwidth}. These constraints necessitate the development of innovative strategies aimed at optimizing resource utilization while preserving the real-time performance and locality benefits of edge computing. The authors in~\cite{kumar2021resource}, proposes a new edge computing scheme using lightweight object recognition and adaptive transmission, which improves road traffic monitoring by significantly reducing data transmission while maintaining effective object identification. Chameleon~\cite{jiang2018chameleon} optimizes resource efficiency by adapting configurations based on temporal and spatial correlations in video data, significantly reducing resource consumption while maintaining effective accuracy. Similarly, Spatula~\cite{jain2020spatula} exploits spatial and temporal correlations among camera feeds to lower network and computation costs. While the article in~\cite{wang2018bandwidth} presents bandwidth-saving strategies for real-time drone video analysis using advanced computing and deep neural networks, demonstrating effective transmission reduction with minimal impact on accuracy and latency.~\newline
On top of these solutions for dealing with edge computing challenges, other studies have suggested different ways to improve performance while keeping its benefits. Techniques such as model compression and pruning have been employed to reduce the computational footprint of deep learning algorithms, thereby enabling their execution on resource-constrained devices~\cite{eccles2024dnnshifter}. Distributed processing frameworks facilitate the partitioning and collaborative execution of workloads across multiple edge nodes, improving scalability and fault tolerance~\cite{rachuri2021decentralized,zeng2020distream,201465videostorm}. Furthermore, adaptive scheduling algorithms and energy-aware resource management strategies have been developed to dynamically allocate tasks and balance performance with sustainability. Collectively, these approaches contribute to the realization of robust, efficient, and scalable edge computing systems capable of supporting increasingly complex and latency-sensitive applications.


\section{Load Balancing for Live Video Analytics}\label{sec:load_balancing_for_live_video_analytics}
% Load Balancing Techniques: Survey existing load balancing techniques for edge computing, with a focus on those relevant to video analytics. Discuss distributed load balancing approaches and the use of machine learning models in load balancing.

Recent advancements in video analytics have led to the development of various techniques aimed at enhancing application performance. These efforts span multiple dimensions, including architectural design, pipeline optimization, and data privacy. As the demand for real-time, scalable video processing grows, particularly in edge environments, thus efficient resource management and adaptability have become central challenges.

\paragraph{Deployment Strategies.}
To overcome the limitations of static architectures, newer frameworks have adopted distributed deployment models. Distream~\cite{zeng2020distream} exemplifies this shift by partitioning analytics pipelines between smart cameras and edge nodes, adapting to workload dynamics to maintain low latency and high throughput. The work presented in~\cite{rachuri2021decentralized} shows that the intelligent distribution and processing of vision modules in parallel on available peripheral computing nodes can ultimately enable better use of resources and improve performance. Likewise, VideoStorm~\cite{201465videostorm} which places different video functions across multiple available workers to satisfy users' requests. These strategies offer greater flexibility and scalability, making them more suitable for dynamic environments.

\paragraph{Load Balancing in Edge Video Analytics.}
As video analytics applications scale across distributed infrastructures, load balancing becomes essential for maintaining accuracy and low latency.
Traditional methods relied on static configurations and centralized cloud offloading~\cite{jain2020spatula,201465videostorm,zhang2019hetero,hung2018videoedge}. VideoStorm~\cite{201465videostorm} offers a system that uses approximation and delay tolerance to balance the load across the distributed system. It dynamically adapts the workload based on available resources and simultaneous network conditions to ensure effective load balancing. Alternatively, Spatula~\cite{jain2020spatula} utilizes spatial and temporal correlations between cameras to improve load balancing in large-scale camera networks. By selectively analyzing only the cameras and images most likely to capture the target identity (\eg, people or vehicles) as they pass through different viewpoints, Spatula efficiently distributes the computing workload across the network. In contrast, VideoEdge~\cite{hung2018videoedge} optimizes workload distribution across a hierarchy of clusters to maximize accuracy while balancing network and computing resource demands.
Hetero-edge~\cite{zhang2019hetero} uses Apache Storm and breaks edge applications into DAG-based tasks and map them across heterogeneous servers (CPUs, \acrshort{gpu}s) for efficient execution. By intelligently distributing tasks based on server capabilities, it achieves efficient load balancing and significantly reduces latency for real-time vision applications.\\
Recent developments have introduced more dynamic load balancing mechanisms. Distream~\cite{zeng2020distream} introduces a distributed video analytics system that dynamically partitions tasks between smart cameras and edge clusters, enabling real-time scalability and responsiveness. Its pipeline-level load balancing adjusts to fluctuating video content, delivering lower latency and higher throughput than conventional approaches.

\paragraph{Workload Prediction.}
To enhance load balancing, predictive models based on machine learning have been explored~\cite{zeng2020distream,yuan2021online,kombi2017preventive}.
Distream~\cite{zeng2020distream} employs an LSTM-based workload predictor that anticipates future load spikes, thereby avoiding inefficient task migrations and maintaining a balanced workload distribution.
Reinforcement learning, as applied in~\cite{yuan2021online}, supports real-time task assignment and shows promising results, though it typically involves considerable computational effort and ongoing training to manage evolving data patterns.
Linear regression models, as used in~\cite{kombi2017preventive}, offer a simple and efficient way to forecast incoming tasks based on recent trends, making them a practical choice for scenarios with relatively stable scene dynamics.
Given the constraints of peripheral devices and the unpredictability of data provided by mobile cameras, there is a growing need for lightweight prediction models that can predict short-term trends, run efficiently on resource-limited hardware, and support real-time decision-making.

The evolution of video analytics has moved from static, centralized systems to dynamic, distributed architectures. While significant progress has been made in deployment and load balancing strategies, future research must focus on developing lightweight, adaptive forecasting techniques. These solutions should be tailored to the constraints of edge environments and the variability introduced by mobile video sources, ensuring scalable and responsive video analytics.


\section{Inference Serving and Resource Management in Edge Cloud Computing}\label{sec:inference_serving_and_resource_management_in_edge_cloud_computing}

With the rise of deep learning applications deployed as online services, efficiently managing inference workloads in \acrshort{gpu} datacenters has become a pressing concern. Unlike training tasks, inference jobs present unique constraints, requiring high accuracy, low latency, and cost-effectiveness. These goals often conflict, making it essential to design scheduling systems that can balance trade-offs without compromising overall performance.

\paragraph{Inference Serving Systems.}
In the field of efficient resource allocation for machine learning inference systems, several studies have proposed relevant solutions. Clipper~\cite{2017clipper} proposes an inference service system designed for real-time applications, supporting a variety of ML frameworks and models to simplify their deployment and commissioning. TensorFlow-Serving~\cite{olston2017tensorflowserving}, meanwhile, enables real-time model serving by dynamically adjusting the number of replicas based on traffic, promoting continuous adaptation to load variations. Clockwork~\cite{gujarati2020servingdnnslikeclockwork} offers an approach focused on performance predictability, exploiting the deterministic nature of full \acrshort{gpu} capacity \acrshort{dnn} inference and orchestrating inference execution with data loading via separate \acrshort{cuda} streams. Proteus~\cite{ahmad2024proteus}, a more recent work, introduces a high-throughput inference-serving system that dynamically scales model accuracy by optimizing model selection, placement, and query distribution.\\
While these designs effectively optimize throughput and accuracy, their reliance on rigid mappings between models and devices tends to overlook opportunities for co-location and parallel execution, highlighting the need for more flexible placement strategies that better leverage shared \acrshort{gpu} resources.

\paragraph{Multi-Tenant \acrshort{dnn} Inference on Shared \acrshort{gpu}s.}
A subsequent line of research investigates techniques for optimizing model performance in environments characterized by concurrent execution, where multiple models or processes must contend for shared \acrshort{gpu} resources~\cite{mobin2023colti,yu2021automated,zhao2023miriam,francisco2021infaas}. This body of work typically examines scheduling algorithms, resource allocation strategies, and system-level adaptations designed to maintain throughput and minimize latency under constrained computational conditions. For example, INFaaS~\cite{francisco2021infaas} develops a model-less inference system that automates variant selection and resource allocation to achieve performance and accuracy targets, although it faces scalability challenges in profiling and predicting interference. Colti~\cite{mobin2023colti} offers a technique that improves the efficiency of training and inference tasks by colocating deep neural networks on \acrshort{gpu}s, thereby increasing overall throughput and reducing execution times. Another work in~\cite{yu2021automated}, leverage the fact that \acrshort{dnn} inference consists of a series of operators (convolution, dense, etc.) and exploits their independence between different \acrshort{dnn}s to schedule their execution concurrently, assigning each model to a dedicated stream, splitting operator sequences into shorter steps, and orchestrating their execution to balance resources and minimize latency. REEF~\cite{han2022microsecond}, a \acrshort{dnn} inference serving system that enables efficient concurrent execution and kernel preemption on \acrshort{gpu}s, allowing multiple models with different priorities to share resources while minimizing latency and maintaining predictable performance. Miriam~\cite{zhao2023miriam}, in the other hand, introduces a framework for \acrshort{gpu}s that enables the simultaneous execution of \acrshort{dnn} tasks with varying real-time requirements, using elastic kernels, which are smaller, more flexible units that can be dynamically scheduled and reassigned according to their criticality and priority. Overall, these approaches focus on post-deployment optimization of models sharing the same \acrshort{gpu}, without addressing the issue of optimal initial placement or co-location strategy. Other work stands out by directly addressing this deployment phase, integrating co-location and inter-\acrshort{gpu} planning strategies from the outset to anticipate interference and improve overall efficiency.

\paragraph{Interference-Aware Inference Serving.}
Recent studies have focused on proactive scheduling by modeling and predicting interference between concurrently running models. 
Some approaches, such as the work in~\cite{mendoza2021interference}, estimate latency degradation using coarse features like model utilization of global buffer and PCIE connection, which provides a unified way to predict latency degradation across platforms. 
In contrast, Scrooge~\cite{hu2021scrooge} proposes profiling to find the optimal level of competition among identical \acrshort{dnn}s in order to maximize throughput. This leads to an important conclusion: although effective for homogeneous configurations, this approach highlights the difficulty of scaling profiling across various combinations of different \acrshort{dnn}s sharing resources.
Operator-level scheduling frameworks, like Abacus~\cite{cui2021Abacus} addresses \acrshort{dnn} interference by scheduling operators (\eg, Convolution, ReLU) from multiple models to run concurrently. It treats each \acrshort{dnn} as a sequence of operators and groups them for joint execution, aiming to maintain quality-of-service guarantees. This operator-level scheduling simplifies coordination and enables deterministic execution.
More detailed analysis and low level consideration for \acrshort{dnn} scheduling on \acrshort{gpu}s, as iGniter~\cite{xu2022igniter}, employs hardware-level metrics to characterize interference, but may not fully capture nuances of kernel launches and core scheduling.
A lower-level \acrshort{gpu} details like kernels, such as Usher~\cite{shubha2024usher}, analyze achieved kernel occupancy and DRAM usage during \acrshort{dnn} inference and introduce model classification schemes, providing more accurate insights into interference.
Further work is needed to develop more accurate and widely applicable interference-aware scheduling strategies that can operate in diverse environments, including edge computing scenarios, with a deeper understanding of their limitations.