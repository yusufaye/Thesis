\setchapterpreamble[u]{\margintoc}
\chapter{Related work}\labch{related_work}

This section reviews the latest research and advances in edge cloud computing, focusing on three main themes: video analytics, workload distribution, and AI application scheduling. We begin by defining cloud computing and the motivations for adopting edge-based solutions, particularly in the field of video analytics. The following sections delve deeper into research on live video analytics, followed by a detailed discussion on workload balancing (\cref{sec:load_balancing_for_live_video_analytics}) and, finally, query processing, scheduling, and model placement strategies (\cref{sec:inference_serving_and_resource_management_in_edge_cloud_computing}).

% This section outlines the structure of our exploration into edge cloud computing, focusing on three core themes: video analytics, workload distribution, and AI application planning. We begin by defining cloud computing and the motivations driving a shift toward edge-based solutions, particularly in video analytics. Subsequent parts will delve into research on live video analytics, followed by a detailed discussion on workload balancing (Section X), and finally, query serving, scheduling, and model placement strategies (Section Z).


% Talk about what exist in general in the field of video analytics
% 


\section{Edge Computing and Video Analytics}

Cloud computing is a model that uses centralized data centers to provide scalable storage and processing capabilities on a large scale over the Internet. Although this model has facilitated significant digital transformation, it presents several challenges for latency-sensitive and bandwidth-intensive applications, for emerging real-time applications.

The increasing need for real-time responsiveness, data privacy, and network efficiency has prompted a move toward edge computing, where processing occurs closer to the data source. This shift is especially beneficial for video analytics, which frequently relies on continuous, high-volume data streams from distributed sensors and cameras. This architectural transformation addresses several limitations inherent in centralized cloud infrastructures, notably by reducing latency, minimizing bandwidth consumption, enhancing data security, and enabling real-time responsiveness. Given that edge devices, such as sensors, cameras, and Internet of Things (IoT) nodes, typically operate under constrained computational and energy resources, a variety of architectural models have emerged. These include hybrid edge-cloud systems, which balance local and remote processing, as well as fully autonomous edge analytics platforms designed for domain-specific applications, particularly within industrial IoT environments.

% \paragraph{GPU Integration and Computational Enhancement.}
% The incorporation of Graphics Processing Units (GPUs) into edge computing frameworks has significantly expanded the computational capabilities of edge systems. GPUs are inherently suited for parallel processing, rendering them ideal for compute-intensive tasks such as machine learning, computer vision, and large-scale data analytics. Their deployment at the edge yields notable improvements in performance, efficiency, and scalability. As a result, GPUs have become instrumental in enabling advanced edge applications, including autonomous vehicles, smart surveillance systems, immersive augmented and virtual reality (AR/VR) experiences, and responsive healthcare diagnostics. These applications benefit from the edge's capacity to process data locally and in real time, thereby reducing latency and enhancing decision-making accuracy. In industrial IoT contexts, for example, edge computing facilitates real-time monitoring and control of equipment, leading to increased operational efficiency and reduced downtime.

\paragraph{Edge computing applications.}
Edge computing has emerged as a transformative technology across diverse sectors, enhancing operational efficiency and enabling real-time decision-making. In smart agriculture, it facilitates precision farming through sensor, drone-based crop monitoring, promoting optimal resource allocation and early disease detection~\cite{akhtar2021smart,junaidi2025deep,dhifaoui2022cloud}. For instance, The authors in~\cite{pandey2024towards} emphasize that artificial intelligence (AI) can significantly improve efficiency in the following areas: crop cultivation, yield forecasting, disease monitoring, supply chain operations, food waste reduction, and resource management.
Urban areas reap considerable benefits from edge computing systems that manage traffic, pollution, temperature rises, and security. These systems use real-time sensor data to reduce traffic congestion and improve public safety through proactive monitoring and response~\cite{xu2023mobile,hossain2018edge}.
Edge computing in industrial environments enables predictive maintenance by monitoring machines in real time to prevent breakdowns, while promoting seamless human-robot collaboration for safer and more efficient operations. These innovations are driving the vision of Industry 5.0, which advocates for smart, human-centered manufacturing~\cite{sharma2024edge}.
In healthcare, wearable devices enable continuous monitoring of vital signs, allowing prompt medical responses to anomalies~\cite{rancea2024edge}. The overview highlights the crucial role of edge computing in improving responsiveness, accuracy, and strategic decision-making across various industries and applications.

\paragraph{Video Analytics as a Critical Edge Application.}
Among the diverse applications of edge computing, video analytics emerges as a particularly demanding and impactful use case. In domains such as smart cities, autonomous driving, and public safety, edge-based video analytics enables the timely interpretation of visual data~\cite{hu2023edge,xu2023edge}. A typical video analytics pipeline comprises several stages: data ingestion, preprocessing, feature extraction, model inference, and post-processing. Each stage imposes distinct computational and memory requirements, which are often challenging to satisfy within the limited capabilities of edge devices.

\paragraph{Resource Constraints and Architectural Challenges.}
The computational limitations of edge devices, manifested in restricted processing power, limited storage capacity, and constrained communication bandwidth, pose significant challenges to the deployment of complex video analytics workloads~\cite{badidi2023opportunities,kumar2021resource,jiang2018chameleon,wang2018bandwidth}. These constraints necessitate the development of innovative strategies aimed at optimizing resource utilization while preserving the real-time performance and locality benefits of edge computing. The others in~\cite{kumar2021resource}, proposes a new edge computing scheme using lightweight object recognition and adaptive transmission, which improves road traffic monitoring by significantly reducing data transmission while maintaining effective object identification. Chameleon~\cite{jiang2018chameleon} optimizes resource efficiency by adapting configurations based on temporal and spatial correlations in video data, significantly reducing resource consumption while maintaining effective accuracy. While the article in~\cite{wang2018bandwidth} presents bandwidth-saving strategies for real-time drone video analysis using advanced computing and deep neural networks, demonstrating effective transmission reduction with minimal impact on accuracy and latency.
On top of these solutions for dealing with edge computing challenges, other studies have suggested different ways to improve performance while keeping its benefits. Techniques such as model compression and pruning have been employed to reduce the computational footprint of deep learning algorithms, thereby enabling their execution on resource-constrained devices~\cite{eccles2024dnnshifter}. Distributed processing frameworks facilitate the partitioning and collaborative execution of workloads across multiple edge nodes, improving scalability and fault tolerance~\cite{rachuri2021decentralized,zeng2020distream,201465videostorm}. Furthermore, adaptive scheduling algorithms and energy-aware resource management strategies have been developed to dynamically allocate tasks and balance performance with sustainability. Collectively, these approaches contribute to the realization of robust, efficient, and scalable edge computing systems capable of supporting increasingly complex and latency-sensitive applications.


\section{Load Balancing for Live Video Analytics}\label{sec:load_balancing_for_live_video_analytics}
% Load Balancing Techniques: Survey existing load balancing techniques for edge computing, with a focus on those relevant to video analytics. Discuss distributed load balancing approaches and the use of machine learning models in load balancing.

Recent advancements in video analytics have led to the development of various techniques aimed at enhancing application performance. These efforts span multiple dimensions, including architectural design, pipeline optimization, and data privacy. As the demand for real-time, scalable video processing grows, particularly in edge environments, thus efficient resource management and adaptability have become central challenges.

\paragraph{Video Analytics System Architecture.}
Early solutions focused on optimizing computational resources for video streams from fixed cameras. For example, Chameleon dynamically reconfigures pipeline placement to reduce resource consumption with minimal accuracy loss. Similarly, Spatula exploits spatial and temporal correlations among camera feeds to lower network and computation costs. However, these approaches are limited by their reliance on static camera inputs and do not address the complexities introduced by mobile video sources.

\paragraph{Deployment Strategies.}
To overcome the limitations of static architectures, newer frameworks have adopted distributed deployment models. Distream~\cite{zeng2020distream} exemplifies this shift by partitioning analytics pipelines between smart cameras and edge nodes, adapting to workload dynamics to maintain low latency and high throughput. The work presented in shows that the intelligent distribution and processing of vision modules in parallel on available peripheral computing nodes can ultimately enable better use of resources and improve performance. Likewise, VideoStorm~\cite{201465videostorm} which places different video functions across multiple available workers to satisfy users' requests. These strategies offer greater flexibility and scalability, making them more suitable for dynamic environments.

\paragraph{Load Balancing in Edge Video Analytics.}
As video analytics applications scale across distributed infrastructures, load balancing becomes essential for maintaining accuracy and low latency. Traditional methods, such as those used in VideoStorm~\cite{201465videostorm}, Spatula~\cite{jain2020spatula}, Hetero-edge~\cite{zhang2019hetero}, and VideoEdge~\cite{hung2018videoedge}, relied on static configurations and centralized cloud offloading. These approaches often resulted in network bottlenecks and were slow to adapt to changing workloads.
Recent developments have introduced more dynamic load balancing mechanisms. Distream, for example, offers pipeline-level load balancing that adapts to the variability of video content. However, its reliance on predictable long-term patterns such as daily traffic fluctuations limits its responsiveness to abrupt changes in video content or camera behavior.

\paragraph{Workload Prediction.}

To enhance load balancing, predictive models based on machine learning have been explored. Reinforcement learning, as used in~\cite{yuan2021online}, enables real-time task assignment but demands substantial computational resources and continuous training to mitigate concept drift. However, reinforcement learning solution, even though effective, require a significant amount of resources and continuous online training to avoid concept-drift problems~\cite{zhang2020reinforcement}. Linear regression models, as in~\cite{kombi2017preventive}, offer a simpler alternative but may lack the sophistication needed for complex scene dynamics.
Given the constraints of edge devices and the unpredictability of mobile camera inputs, there is a growing need for lightweight forecasting models. These models should be capable of short-term trend prediction, operate efficiently on resource-constrained hardware, and support real-time decision-making.

The evolution of video analytics has moved from static, centralized systems to dynamic, distributed architectures. While significant progress has been made in deployment and load balancing strategies, future research must focus on developing lightweight, adaptive forecasting techniques. These solutions should be tailored to the constraints of edge environments and the variability introduced by mobile video sources, ensuring scalable and responsive video analytics.


% In recent years, several techniques have been developed to improve the performance of video analytics applications~\cite{ibrahim2021survey,xu2023edge,hu2023edge}. Such a topic has been tackled from different perspectives, including the design of different data processing architectures~\cite{jain2020spatula,zhang2017live,jiang2018chameleon}, the improvement of pipelines' processing~\cite{fouladi2017encoding,chen2015glimpse,padmanabhan2021towards,padmanabhan2023gemel} and the privacy of the extracted data~\cite{cangialosi2022privid,poddar2020visor,wu2021pecam}.

% \paragraph{Architecture scaling.}
% Different approaches have been proposed to efficiently manage the computational resources for video analytics~\cite{jain2020spatula,zhang2017live,jiang2018chameleon,201465videostorm}. Chameleon, presented in~\cite{jiang2018chameleon}, frequently reconfigures the placement of video analytics pipelines to reduce resource consumption with small loss in accuracy. Another example is Spatula~\cite{jain2020spatula}, which exploits the spatial and temporal correlations among different camera flows to reduce the network and computation costs. However, such solutions only consider video flows coming from fixed cameras.

% \paragraph{Deployment strategies.} 
% Other solutions mainly focused on the deployment strategies of video analytics applications~\cite{zeng2020distream,201465videostorm,rachuri2021decentralized}. Distream~\cite{zeng2020distream} is a distributed framework based capable of adapting to workload dynamics to achieve low-latency, high-throughput and scalable live video analytics. Pipelines are deployed on both the smart cameras and the edge, and are jointly partitioned so that part is computed on the smart cameras, while the rest is sent towards the edge, which has more computing power at its disposal. The deployment of application pipelines is adapted to the varying processing load, however there is a lack of adaptability required by the rate of mobile cameras.
% The work in~\cite{rachuri2021decentralized} presents experimental results showing that smartly distributing and processing vision modules in parallel across available edge compute nodes, can ultimately lead to better resource utilization and improved performance. The same approach is also used by VideoStorm~\cite{201465videostorm} which places different video functions across multiple available workers to satisfy users' requests. We assume a deployment of pipelines in line with this latter work given the higher flexibility, higher scalability and the better use of resources of this approach. 

% \paragraph{Load balancing strategies.}
% Once video analytics applications have been deployed on a distributed edge infrastructure, load balancing strategies play a fundamental role in guaranteeing requirements of accuracy and efficiency. As a result, The concept of implementing load balancing for video analytics applications has gained popularity, particularly with the emergence of edge video analytics and its associated limitations. Historically, load balancing was performed between edge nodes and central clouds (\eg, VideoStorm~\cite{201465videostorm}), but this method became impractical due to increased network traffic and potential network bottlenecks. Load balancing between edge locations has been the subject of several works, including Spatula~\cite{jain2020spatula}, Hetero-edge~\cite{zhang2019hetero}, and VideoEdge~\cite{hung2018videoedge} (albeit VideoEdge still relies on offloading to remote clouds). However, these works focused on the production of static configurations, where each processed video is directed to a predetermined path through the deployed processing functions. As any change of configuration impacts the deployed functions, configuration updates occur over longer timescales, making these approaches unsuitable for highly variable loads. More recently, Distream~\cite{zeng2020distream} recognized the need for rapidly adapting to varying loads within a video, proposing an adaptable load balancing solution that splits the load balancing decisions at the pipeline level. However, it assumes that workflows present predictable longer-term patterns, allowing reconfiguration decisions only to be taken at longer timescales, for example when traffic conditions change during the day due to commute patterns.

% \paragraph{Workload prediction.}
% Workload predictions, based on machine learning models, have been proved to be effective in the design of load balancing policies~\cite{heinze2014auto,gedik2013elastic,kombi2017preventive,zeng2020distream}. In~\cite{yuan2021online}, authors used reinforcement learning for performing real-time estimation for dynamic assigning task to the optimal server. While the work in~\cite{kombi2017preventive}, focused on load forecasting by using linear regression model. However, reinforcement learning solution, even though effective, require a significant amount of resources and continuous online training to avoid concept-drift problems~\cite{zhang2020reinforcement}. Such a solution method is not suitable for the computational-constrained devices at the edge. In addition, the rapid changes in scenes captured by mobile cameras are more difficult to predict. Therefore, there is a need of lightweight forecasting models that can predict short-term trends, suitable for edge devices and fast enough for real-time prediction.


\section{Inference Serving and Resource Management in Edge Cloud Computing}\label{sec:inference_serving_and_resource_management_in_edge_cloud_computing}

With the rise of deep learning applications deployed as online services, efficiently managing inference workloads in GPU datacenters has become a pressing concern. Unlike training tasks, inference jobs present unique constraints, requiring high accuracy, low latency, and cost-effectiveness. These goals often conflict, making it essential to design scheduling systems that can balance trade-offs without compromising overall performance.

\paragraph{Inference Serving Systems.}
In the field of efficient resource allocation for machine learning inference systems, several studies have proposed relevant solutions. Clipper~\cite{2017clipper} is an inference service system designed for real-time applications, supporting a variety of ML frameworks and models to simplify their deployment and commissioning. TensorFlow-Serving~\cite{olston2017tensorflowserving}, meanwhile, enables real-time model serving by dynamically adjusting the number of replicas based on traffic, promoting continuous adaptation to load variations. Clockwork~\cite{gujarati2020servingdnnslikeclockwork} offers an approach focused on performance predictability, exploiting the deterministic nature of full GPU capacity DNN inference and orchestrating inference execution with data loading via separate CUDA streams. Proteus~\cite{ahmad2024proteus}, a more recent work, introduces a high-throughput inference-serving system that dynamically scales model accuracy by optimizing model selection, placement, and query distribution; its design assigns a single model variant per device, which constrains resource utilization and overlooks opportunities for co-location and parallel execution. This limitation highlights the need for more flexible model placement strategies that exploit shared GPU resources to further enhance throughput and efficiency. This category of inference systems, while effective at optimizing throughput and accuracy, often relies on rigid model-device mappings, limiting opportunities for co-location and simultaneous execution.

% The incorporation of Graphics Processing Units (GPUs) into edge computing frameworks has significantly expanded the computational capabilities of edge systems. GPUs are inherently suited for parallel processing, rendering them ideal for compute-intensive tasks such as machine learning, computer vision, and large-scale data analytics. Their deployment at the edge yields notable improvements in performance, efficiency, and scalability. As a result, GPUs have become instrumental in enabling advanced edge applications, including autonomous vehicles, smart surveillance systems, immersive augmented and virtual reality (AR/VR) experiences, and responsive healthcare diagnostics. These applications benefit from the edge's capacity to process data locally and in real time, thereby reducing latency and enhancing decision-making accuracy. In industrial IoT contexts, for example, edge computing facilitates real-time monitoring and control of equipment, leading to increased operational efficiency and reduced downtime.

\paragraph{Multi-Tenant DNN Inference on Shared GPUs.}
A subsequent line of research investigates techniques for optimizing model performance in environments characterized by concurrent execution, where multiple models or processes must contend for shared GPU resources~\cite{mobin2023colti,yu2021automated,zhao2023miriam,francisco2021infaas}. This body of work typically examines scheduling algorithms, resource allocation strategies, and system-level adaptations designed to maintain throughput and minimize latency under constrained computational conditions. For example, INFaaS~\cite{francisco2021infaas} develops a model-less inference system that automates variant selection and resource allocation to achieve performance and accuracy targets, although it faces scalability challenges in profiling and predicting interference. Colti~\cite{mobin2023colti} offers a technique that improves the efficiency of training and inference tasks by colocating deep neural networks on GPUs, thereby increasing overall throughput and reducing execution times. Another work in~\cite{yu2021automated}, leverage the fact that DNN inference consists of a series of operators (convolution, dense, etc.) and exploits their independence between different DNNs to schedule their execution concurrently, assigning each model to a dedicated stream, splitting operator sequences into shorter steps, and orchestrating their execution to balance resources and minimize latency. REEF~\cite{han2022microsecond}, a DNN inference serving system that enables efficient concurrent execution and kernel preemption on GPUs, allowing multiple models with different priorities to share resources while minimizing latency and maintaining predictable performance. Miriam~\cite{zhao2023miriam}, in the other hand, introduces a framework for GPUs that enables the simultaneous execution of DNN tasks with varying real-time requirements, using elastic kernels, which are smaller, more flexible units that can be dynamically scheduled and reassigned according to their criticality and priority. Overall, these approaches focus on post-deployment optimization of models sharing the same GPU, without addressing the issue of optimal initial placement or co-location strategy. Other work stands out by directly addressing this deployment phase, integrating co-location and inter-GPU planning strategies from the outset to anticipate interference and improve overall efficiency.

\paragraph{Interference-Aware Inference Serving.}
% Recent studies have shifted toward proactive scheduling by modeling and predicting interference between concurrently running models. Some approaches estimate latency degradation using coarse features like buffer usage and PCIe traffic, but lack granularity. Scrooge~\cite{hu2021scrooge} profiles concurrency thresholds for identical models, though its scalability is limited by the number of possible combinations. Proteus~\cite{ahmad2024proteus} introduces adaptive batching and model variant selection to balance accuracy and latency, but its rigid placement strategy restricts colocation flexibility. These systems represent progress toward interference-aware scheduling, yet often rely on profiling techniques that are either too coarse or computationally intensive.

% \paragraph{Fine-Grained GPU Scheduling for DNN Inference: Kernel-Level Insights and Limitations}
% Recent approaches to GPU scheduling for deep neural network inference, such as iGniter~\cite{xu2022igniter} and Usher, emphasize low-level profiling to mitigate performance interference in shared environments. By analyzing metrics like core launches, cache usage, and kernel occupancy, these systems reveal that traditional coarse-grained models overlook critical scheduling dynamics. Usher further distinguishes between compute- and memory-intensive workloads, noting that large language models often strain memory bandwidth. Both rely on NVIDIA's Multi-Process Service (MPS) for resource partitioning, which limits their applicability on edge platforms lacking MPS support. This highlights the need for interference-aware strategies that operate independently of cloud-specific infrastructure.

% The evolution of inference scheduling reflects a shift from general-purpose deployment tools to sophisticated, interference-aware systems. While early solutions prioritized ease of use, newer frameworks aim to anticipate and mitigate resource contention dynamically. However, we need a much fine-grained profiling, flexible model placement, and adaptive execution strategies to meet the complex demands of modern inference workloads.

% Recent research in inference-serving systems has focused on mitigating latency degradation and model interference in shared GPU environments. Unified latency models have been proposed to guide model placement across CPU/GPU platforms, while Scrooge introduces profiling techniques to identify optimal concurrency levels for colocated DNNs. Proteus advances this by dynamically selecting model variants and distributing workloads across heterogeneous hardware to balance throughput and accuracy, leveraging adaptive batching for improved efficiency.

% At a finer granularity, Abacus and INFaaS explore operator-level scheduling and model-less deployment strategies, respectively, to preserve QoS and streamline variant selection. Meanwhile, iGniter and Usher adopt low-level, kernel-aware approaches to characterize and mitigate interference using hardware metrics and kernel occupancy analysis. Both systems utilize NVIDIA’s MPS for spatial sharing, though its absence on edge platforms highlights the need for alternative scheduling mechanisms. Collectively, these works underscore the importance of resource-aware orchestration and interference-sensitive design in scalable inference-serving architectures.


% Recent research has proposed various strategies to address latency degradation and model interference in inference-serving systems. Unified latency models help guide model placement across CPU/GPU platforms, while Scrooge introduces profiling to identify optimal concurrency levels for colocated DNNs. Proteus advances the field by dynamically selecting model variants, distributing them across heterogeneous hardware, and leveraging adaptive batching to improve throughput under latency constraints. Abacus takes an operator-level scheduling approach to preserve QoS by grouping operators from multiple DNNs for concurrent execution, and INFaaS offers a model-less deployment framework that dynamically selects and colocates variants based on performance and load conditions.

% At a finer granularity, iGniter and Usher provide interference-sensitive solutions by analyzing GPU hardware metrics and kernel-level behavior. iGniter uses indicators like L2 cache usage and core launches to characterize contention, while Usher introduces a classification scheme to distinguish between compute- and memory-intensive workloads, enhancing scheduling precision. Both systems utilize NVIDIA’s Multi-Process Service (MPS) to enable spatial sharing of GPU resources in cloud environments. Collectively, these works reflect a growing trend toward fine-grained, performance-sensitive inference serving, emphasizing proactive scheduling and scalable architecture design.



% Recent efforts in DNN inference serving have increasingly focused on proactive scheduling strategies that anticipate and mitigate performance interference among concurrently running models. A number of approaches have explored latency prediction using coarse-grained model features, enabling basic placement decisions across heterogeneous platforms. Systems like Scrooge and Proteus extend this direction by incorporating profiling and adaptive batching to optimize throughput and resource utilization, particularly in cloud environments.

% Other works have examined interference at finer levels of granularity. Abacus and INFaaS propose operator-level and model-less scheduling frameworks, respectively, aiming to balance quality-of-service and deployment flexibility. These systems emphasize dynamic variant selection and workload distribution, contributing to more responsive and scalable inference serving.

% More recent studies, including iGniter and Usher, delve into kernel-level profiling to better capture GPU resource contention. By analyzing metrics such as cache usage, kernel occupancy, and DRAM bandwidth, these approaches offer deeper insights into the underlying causes of interference. Their reliance on NVIDIA’s Multi-Process Service (MPS) highlights the importance of spatial sharing mechanisms, though such infrastructure is not universally available, especially in edge scenarios.

% While these contributions mark significant progress in understanding and managing DNN interference, further work is needed to unify scheduling strategies across hardware platforms and execution granularities, particularly in environments with limited profiling capabilities or infrastructure support.



% Recent research has increasingly focused on proactive scheduling strategies to mitigate interference in DNN inference. Approaches like Therus and Scrooge use coarse-grained profiling and latency estimation to guide model placement and concurrency decisions. Proteus expands this by dynamically scaling accuracy and batching queries to optimize throughput across heterogeneous platforms.

Recent studies have focused on proactive scheduling by modeling and predicting interference between concurrently running models. 
Some approaches, such as the work in~\cite{mendoza2021interference}, estimate latency degradation using coarse features like model utilization of global buffer and PCIE connection, which provides a unified way to predict latency degradation across platforms. 
In contrast, Scrooge~\cite{hu2021scrooge} proposes profiling to find the optimal level of competition among identical DNNs in order to maximize throughput. This leads to an important conclusion: although effective for homogeneous configurations, this approach highlights the difficulty of scaling profiling across various combinations of different DNNs sharing resources.
Operator-level scheduling frameworks, like Abacus~\cite{cui2021Abacus} addresses DNN interference by scheduling operators from multiple models to run concurrently. It treats each DNN as a sequence of operators and groups them for joint execution, aiming to maintain quality-of-service guarantees. This operator-level scheduling simplifies coordination and enables deterministic execution.
More detailed analysis and low level consideration for DNN scheduling on GPUs, as iGniter~\cite{xu2022igniter}, employs hardware-level metrics to characterize interference, but may not fully capture nuances of kernel launches and core scheduling.
A lower-level GPU details like kernels, such as Usher~\cite{shubha2024usher}, analyze achieved kernel occupancy and DRAM usage during DNN inference and introduce model classification schemes, providing more accurate insights into interference.
Further work is needed to develop more accurate and widely applicable interference-aware scheduling strategies that can operate in diverse environments, including edge computing scenarios, with a deeper understanding of their limitations.