\setchapterpreamble[u]{\margintoc}
\chapter{Related work}\labch{related_work}


\section{Edge Computing and Video Analytics}
% \paragraph{The Evolution of Edge Computing Architectures.}

Edge computing represents a paradigm shift in distributed systems, wherein computational tasks and data storage are relocated closer to the data source. This architectural transformation addresses several limitations inherent in centralized cloud infrastructures, notably by reducing latency, minimizing bandwidth consumption, enhancing data security, and enabling real-time responsiveness. Given that edge devices, such as sensors, cameras, and Internet of Things (IoT) nodes, typically operate under constrained computational and energy resources, a variety of architectural models have emerged. These include hybrid edge-cloud systems, which balance local and remote processing, as well as fully autonomous edge analytics platforms designed for domain-specific applications, particularly within industrial IoT environments.

\paragraph{GPU Integration and Computational Enhancement.}
The incorporation of Graphics Processing Units (GPUs) into edge computing frameworks has significantly expanded the computational capabilities of edge systems. GPUs are inherently suited for parallel processing, rendering them ideal for compute-intensive tasks such as machine learning, computer vision, and large-scale data analytics. Their deployment at the edge yields notable improvements in performance, efficiency, and scalability. As a result, GPUs have become instrumental in enabling advanced edge applications, including autonomous vehicles, smart surveillance systems, immersive augmented and virtual reality (AR/VR) experiences, and responsive healthcare diagnostics. These applications benefit from the edge's capacity to process data locally and in real time, thereby reducing latency and enhancing decision-making accuracy. In industrial IoT contexts, for example, edge computing facilitates real-time monitoring and control of equipment, leading to increased operational efficiency and reduced downtime.

\paragraph{Video Analytics as a Critical Edge Application.}
Among the diverse applications of edge computing, video analytics emerges as a particularly demanding and impactful use case. In domains such as smart cities, autonomous driving, and public safety, edge-based video analytics enables the timely interpretation of visual data. A typical video analytics pipeline comprises several stages: data ingestion, preprocessing, feature extraction, model inference, and post-processing. Each stage imposes distinct computational and memory requirements, which are often challenging to satisfy within the limited capabilities of edge devices.

\paragraph{Resource Constraints and Architectural Challenges.}
The computational limitations of edge devices, manifested in restricted processing power, limited storage capacity, and constrained communication bandwidth, pose significant challenges to the deployment of complex video analytics workloads. These constraints necessitate the development of innovative strategies aimed at optimizing resource utilization while preserving the real-time performance and locality benefits of edge computing.

\paragraph{Optimization Strategies and Emerging Solutions.}
To address these challenges, researchers have proposed a spectrum of solutions designed to enhance performance without compromising the advantages of edge deployment. Techniques such as model compression and pruning have been employed to reduce the computational footprint of deep learning algorithms, thereby enabling their execution on resource-constrained devices. Distributed processing frameworks facilitate the partitioning and collaborative execution of workloads across multiple edge nodes, improving scalability and fault tolerance. Furthermore, adaptive scheduling algorithms and energy-aware resource management strategies have been developed to dynamically allocate tasks and balance performance with sustainability. Collectively, these approaches contribute to the realization of robust, efficient, and scalable edge computing systems capable of supporting increasingly complex and latency-sensitive applications.


\section{Load Balancing for Live Video Analytics}
% Load Balancing Techniques: Survey existing load balancing techniques for edge computing, with a focus on those relevant to video analytics. Discuss distributed load balancing approaches and the use of machine learning models in load balancing.

Recent advancements in video analytics have led to the development of various techniques aimed at enhancing application performance. These efforts span multiple dimensions, including architectural design, pipeline optimization, and data privacy. As the demand for real-time, scalable video processing grows, particularly in edge environments, thus efficient resource management and adaptability have become central challenges.

\paragraph{Video Analytics System Architecture.}
Early solutions focused on optimizing computational resources for video streams from fixed cameras. For example, Chameleon dynamically reconfigures pipeline placement to reduce resource consumption with minimal accuracy loss. Similarly, Spatula exploits spatial and temporal correlations among camera feeds to lower network and computation costs. However, these approaches are limited by their reliance on static camera inputs and do not address the complexities introduced by mobile video sources.

\paragraph{Deployment Strategies.}
To overcome the limitations of static architectures, newer frameworks have adopted distributed deployment models. Distream~\cite{zeng2020distream} exemplifies this shift by partitioning analytics pipelines between smart cameras and edge nodes, adapting to workload dynamics to maintain low latency and high throughput. The work presented in~\cite{} shows that the intelligent distribution and processing of vision modules in parallel on available peripheral computing nodes can ultimately enable better use of resources and improve performance. Likewise, VideoStorm~\cite{201465videostorm} which places different video functions across multiple available workers to satisfy users' requests. These strategies offer greater flexibility and scalability, making them more suitable for dynamic environments.

\paragraph{Load Balancing in Edge Video Analytics.}
As video analytics applications scale across distributed infrastructures, load balancing becomes essential for maintaining accuracy and low latency. Traditional methods, such as those used in VideoStorm~\cite{201465videostorm}, Spatula~\cite{jain2020spatula}, Hetero-edge~\cite{zhang2019hetero}, and VideoEdge~\cite{hung2018videoedge}, relied on static configurations and centralized cloud offloading. These approaches often resulted in network bottlenecks and were slow to adapt to changing workloads.
Recent developments have introduced more dynamic load balancing mechanisms. Distream, for example, offers pipeline-level load balancing that adapts to the variability of video content. However, its reliance on predictable long-term patterns such as daily traffic fluctuations limits its responsiveness to abrupt changes in video content or camera behavior.

\paragraph{Workload Prediction.}

To enhance load balancing, predictive models based on machine learning have been explored. Reinforcement learning, as used in~\cite{yuan2021online}, enables real-time task assignment but demands substantial computational resources and continuous training to mitigate concept drift. However, reinforcement learning solution, even though effective, require a significant amount of resources and continuous online training to avoid concept-drift problems~\cite{zhang2020reinforcement}. Linear regression models, as in~\cite{kombi2017preventive}, offer a simpler alternative but may lack the sophistication needed for complex scene dynamics.
Given the constraints of edge devices and the unpredictability of mobile camera inputs, there is a growing need for lightweight forecasting models. These models should be capable of short-term trend prediction, operate efficiently on resource-constrained hardware, and support real-time decision-making.


The evolution of video analytics has moved from static, centralized systems to dynamic, distributed architectures. While significant progress has been made in deployment and load balancing strategies, future research must focus on developing lightweight, adaptive forecasting techniques. These solutions should be tailored to the constraints of edge environments and the variability introduced by mobile video sources, ensuring scalable and responsive video analytics.


% In recent years, several techniques have been developed to improve the performance of video analytics applications~\cite{ibrahim2021survey,xu2023edge,hu2023edge}. Such a topic has been tackled from different perspectives, including the design of different data processing architectures~\cite{jain2020spatula,zhang2017live,jiang2018chameleon}, the improvement of pipelines' processing~\cite{fouladi2017encoding,chen2015glimpse,padmanabhan2021towards,padmanabhan2023gemel} and the privacy of the extracted data~\cite{cangialosi2022privid,poddar2020visor,wu2021pecam}.

% \paragraph{Architecture scaling.}
% Different approaches have been proposed to efficiently manage the computational resources for video analytics~\cite{jain2020spatula,zhang2017live,jiang2018chameleon,201465videostorm}. Chameleon, presented in~\cite{jiang2018chameleon}, frequently reconfigures the placement of video analytics pipelines to reduce resource consumption with small loss in accuracy. Another example is Spatula~\cite{jain2020spatula}, which exploits the spatial and temporal correlations among different camera flows to reduce the network and computation costs. However, such solutions only consider video flows coming from fixed cameras.

% \paragraph{Deployment strategies.} 
% Other solutions mainly focused on the deployment strategies of video analytics applications~\cite{zeng2020distream,201465videostorm,rachuri2021decentralized}. Distream~\cite{zeng2020distream} is a distributed framework based capable of adapting to workload dynamics to achieve low-latency, high-throughput and scalable live video analytics. Pipelines are deployed on both the smart cameras and the edge, and are jointly partitioned so that part is computed on the smart cameras, while the rest is sent towards the edge, which has more computing power at its disposal. The deployment of application pipelines is adapted to the varying processing load, however there is a lack of adaptability required by the rate of mobile cameras.
% The work in~\cite{rachuri2021decentralized} presents experimental results showing that smartly distributing and processing vision modules in parallel across available edge compute nodes, can ultimately lead to better resource utilization and improved performance. The same approach is also used by VideoStorm~\cite{201465videostorm} which places different video functions across multiple available workers to satisfy users' requests. We assume a deployment of pipelines in line with this latter work given the higher flexibility, higher scalability and the better use of resources of this approach. 

% \paragraph{Load balancing strategies.}
% Once video analytics applications have been deployed on a distributed edge infrastructure, load balancing strategies play a fundamental role in guaranteeing requirements of accuracy and efficiency. As a result, The concept of implementing load balancing for video analytics applications has gained popularity, particularly with the emergence of edge video analytics and its associated limitations. Historically, load balancing was performed between edge nodes and central clouds (\eg, VideoStorm~\cite{201465videostorm}), but this method became impractical due to increased network traffic and potential network bottlenecks. Load balancing between edge locations has been the subject of several works, including Spatula~\cite{jain2020spatula}, Hetero-edge~\cite{zhang2019hetero}, and VideoEdge~\cite{hung2018videoedge} (albeit VideoEdge still relies on offloading to remote clouds). However, these works focused on the production of static configurations, where each processed video is directed to a predetermined path through the deployed processing functions. As any change of configuration impacts the deployed functions, configuration updates occur over longer timescales, making these approaches unsuitable for highly variable loads. More recently, Distream~\cite{zeng2020distream} recognized the need for rapidly adapting to varying loads within a video, proposing an adaptable load balancing solution that splits the load balancing decisions at the pipeline level. However, it assumes that workflows present predictable longer-term patterns, allowing reconfiguration decisions only to be taken at longer timescales, for example when traffic conditions change during the day due to commute patterns.

% \paragraph{Workload prediction.}
% Workload predictions, based on machine learning models, have been proved to be effective in the design of load balancing policies~\cite{heinze2014auto,gedik2013elastic,kombi2017preventive,zeng2020distream}. In~\cite{yuan2021online}, authors used reinforcement learning for performing real-time estimation for dynamic assigning task to the optimal server. While the work in~\cite{kombi2017preventive}, focused on load forecasting by using linear regression model. However, reinforcement learning solution, even though effective, require a significant amount of resources and continuous online training to avoid concept-drift problems~\cite{zhang2020reinforcement}. Such a solution method is not suitable for the computational-constrained devices at the edge. In addition, the rapid changes in scenes captured by mobile cameras are more difficult to predict. Therefore, there is a need of lightweight forecasting models that can predict short-term trends, suitable for edge devices and fast enough for real-time prediction.



\section{Inference Serving and Resource Management in Edge Cloud Computing}

With the rise of deep learning applications deployed as online services, efficiently managing inference workloads in GPU datacenters has become a pressing concern. Unlike training tasks, inference jobs present unique constraints, requiring high accuracy, low latency, and cost-effectiveness. These goals often conflict, making it essential to design scheduling systems that can balance trade-offs without compromising overall performance.

\paragraph{Limitations of Early Serving Systems.}
Initial solutions such as Clipper~\cite{2017clipper} and TensorFlow-Serving focused on simplifying model deployment and supporting multiple frameworks. While they introduced dynamic scaling and model abstraction, they did not account for resource interference prior to deployment. Clockwork~\cite{gujarati2020servingdnnslikeclockwork} attempted to provide predictable performance by leveraging the deterministic nature of DNN inference under full GPU load. However, its design restricted execution to one inference at a time, resulting in underutilized GPU resources.

\paragraph{Multi-Tenant DNN Inference on Shared GPUs.}
A subsequent line of research investigates techniques for optimizing model performance in environments characterized by concurrent execution, where multiple models or processes must contend for shared GPU resources. This body of work typically examines scheduling algorithms, resource allocation strategies, and system-level adaptations designed to maintain throughput and minimize latency under constrained computational conditions. For example, Colti~\cite{} demonstrated that colocating DNN tasks can improve throughput. Another work proposed scheduling operators across dedicated CUDA streams, splitting execution into stages to balance concurrency and latency. Miriam introduced elastic kernels for edge GPUs, allowing dynamic remapping based on task priority. While these methods improve utilization, they operate reactively and do not explore optimal colocation strategies beforehand.

\paragraph{Interference-Aware Inference Serving.}
Recent studies have shifted toward proactive scheduling by modeling and predicting interference between concurrently running models. Some approaches estimate latency degradation using coarse features like buffer usage and PCIe traffic, but lack granularity. Scrooge~\cite{hu2021scrooge} profiles concurrency thresholds for identical models, though its scalability is limited by the number of possible combinations. Proteus~\cite{ahmad2024proteus} introduces adaptive batching and model variant selection to balance accuracy and latency, but its rigid placement strategy restricts colocation flexibility. These systems represent progress toward interference-aware scheduling, yet often rely on profiling techniques that are either too coarse or computationally intensive.

\paragraph{Fine-Grained GPU Scheduling for DNN Inference: Kernel-Level Insights and Limitations}
Recent approaches to GPU scheduling for deep neural network inference, such as iGniter~\cite{xu2022igniter} and Usher, emphasize low-level profiling to mitigate performance interference in shared environments. By analyzing metrics like core launches, cache usage, and kernel occupancy, these systems reveal that traditional coarse-grained models overlook critical scheduling dynamics. Usher further distinguishes between compute- and memory-intensive workloads, noting that large language models often strain memory bandwidth. Both rely on NVIDIA's Multi-Process Service (MPS)~\cite{} for resource partitioning, which limits their applicability on edge platforms lacking MPS support. This highlights the need for interference-aware strategies that operate independently of cloud-specific infrastructure.

The evolution of inference scheduling reflects a shift from general-purpose deployment tools to sophisticated, interference-aware systems. While early solutions prioritized ease of use, newer frameworks aim to anticipate and mitigate resource contention dynamically. However, we need a much fine-grained profiling, flexible model placement, and adaptive execution strategies to meet the complex demands of modern inference workloads.
