
As more deep learning-based applications are released as online services, managing and scheduling large-scale inference workloads in GPU datacenters has become increasingly critical. Unlike resource-intensive training workloads, inference jobs have unique characteristics and requirements that demand new scheduling solutions. The goals of inference scheduling are multifaceted, including accuracy efficiency, which can be achieved by selecting the best model for each input query and intelligently allocating resources; latency efficiency, which requires optimizing resource allocation to meet response time requirements, even for bursty and fluctuating query requests; and cost-efficiency, which involves minimizing monetary costs when using public cloud resources. These objectives are interdependent, and improving one goal may compromise another if not designed properly, highlighting the need for flexible and comprehensive scheduling systems that can balance tradeoffs between accuracy, latency, and cost.

Despite the growing need for efficient resource allocation, several solutions have been proposed to address this challenge. Clipper is a notable example of an ML inference serving system designed for real-time applications, providing support for a variety of machine learning frameworks and models, and aiming to simplify and accelerate the deployment and serving of ML models. In the other hands, TensorFlow Serving, a system developed for serving machine learning models for making predictions in real-time. It automatically adjust to changes in traffic by adding or removing replicas. Nevertheless, non of these designs consider interference prior to deployment models share resources, instead relying on an adaptation mechanism that can lead to further performance degradation. In the other hand, Clockwork was proposed to provide predictable performance for model serving systems, by acknowledging that DNN inference has deterministic performance when running with full GPU capacity. However, their design only execute one inference at a time even when there are multiple models loaded on the GPU. In fact, Clockwork workers only overlap execution of an inference with data loading through two different CUDA Streams. Which leads to GPU underutilization as during inference all kernels that are launched would leave some resources.

Some studies have recognized that deploying multiple models on the same GPU causes interference and can lead to performance loss. For instance, others have developed a unified approach to predict latency degradation for colocated models and across a variety of platforms (CPU/GPU). This latency degradation can be used in inference serving systems to evaluate model placement. However, their approach is not fine-grained as it is based solely on model features that are the utilization of the global buffer and PCIE connection for running on the GPU device. While we recognize that these parameters play an important role, considering them does not provide a more accurate measure of their execution during inference.
Another recent investigation into cloud-based inference serving highlights the challenges of model interference when multiple DNNs are run concurrently on a single GPU. The study proposes profiling to determine the optimal concurrency level, beyond which adding more DNNs reduces throughput and increases latency. Although the profiling cost is reported to be low, the method focuses solely on colocating identical DNNs, making it infeasible to profile all potential combinations where different DNNs share the same resources. The sheer number of such combinations would be overwhelming to profile.

There are previous researches that have studied the causes of interference in deeper manner. For instance, Abacus which aims to efficiently schedule and execute simultaneous DNN queries in the lower framework level. By considering DNN as sequence of operator (e.g., Conv, ReLU, etc.), they adapt an operator grouping mechanism that group operators from different DNNs to run simultaneously while ensuring QoS guarantees. However, the interference is determined in a coarse grained manner by consider operators (DNN layers) rather than the actual kernels that are launched and executed in the GPU to fullfil each operators. In fact, an operator can launched multiple kernels to complete. What's more their proposed "duration model" is agnostic to the GPU hardware which plays a fundamental rule during inference. Collecting sufficient data by also considering the GPU types and different level of collocation would be very difficult. Furthermore, while waiting for all operators in an operator group to finish, in order to guarantee a deterministic execution order, will result in underutilization of the GPU and increased latency, as operators outside the group may wait unnecessarily. Also, they only consider low-level execution of operator after models are already deployed but didn't develop a proactive solution that determine the compatibility of models or able to measure the performance drop before deployment.

Recent studies have explored the causes of interference in deep neural network (DNN) inference with varying levels of granularity. For instance, Abacus introduces an operator-level scheduling framework that groups operators from multiple DNNs to execute concurrently, aiming to preserve quality-of-service (QoS) guarantees. While this approach models DNNs as sequences of operators (e.g., convolution, ReLU), it overlooks the finer granularity of GPU execution, where each operator may launch multiple kernels with distinct resource demands. Moreover, Abacus’s duration model is agnostic to GPU hardware characteristics, which are critical to inference performance. Collecting sufficient profiling data across diverse GPU architectures and collocation scenarios presents a significant challenge. Additionally, the system enforces deterministic execution by waiting for all operators in a group to complete, potentially leading to underutilization of GPU resources and increased latency. Notably, Abacus operates reactively, analyzing execution only after model deployment, without offering mechanisms to assess model compatibility or predict performance degradation beforehand.

In contrast, iGnifer adopts adopts a low-level perspective on GPU resource management by introducing an interference-sensitive inference server tailored for cloud environments. This system aims to mitigate performance degradation caused by concurrent model execution on shared GPU resources. To characterize interference, iGnifer employs several hardware-level metrics, including GPU L2 cache usage, the number of launched cores, and power consumption. While the number of launched cores is a meaningful indicator of contention, other parameters such as power and frequency are less predictive of interference severity. More influential factors—such as the configuration and scheduling of cores during kernel launches—play a critical role in shaping performance outcomes. These nuances are often overlooked in coarse-grained models, underscoring the need for more precise profiling techniques.
This is acknowledged in a recent article, Usher, which proposes a kernel-level approach to interference mitigation by analyzing the achieved kernel achieved occupancy and DRAM usage during DNN inference. Usher introduces a model classification scheme that distinguishes between compute-intensive and memory-intensive workloads, recognizing that large language models (LLMs), for instance, demand significantly more memory bandwidth than computational throughput. However, from the GPU’s perspective, each model ultimately translates into a set of kernels with varying resource demands, independent of the model’s high-level classification.

In addition, both Usher and iGnifer rely on NVIDIA’s Multi-Process Service (MPS) to enable spatial sharing of GPU resources among concurrent inference tasks. While MPS facilitates efficient resource partitioning in cloud environments, it is not supported on edge platforms such as NVIDIA Jetson. This limitation restricts the applicability of these approaches in edge computing scenarios, where hardware constraints and the absence of MPS demand alternative interference-aware scheduling strategies that operate without relying on such infrastructure.

====================

As deep learning-based applications increasingly become online services, managing and scheduling large-scale inference workloads in GPU datacenters has become critical. Unlike resource-intensive training workloads, inference jobs have unique characteristics and requirements that necessitate new scheduling solutions. The goals of inference scheduling are multifaceted, including accuracy efficiency, which involves selecting the best model for each input query and intelligently allocating resources. Additionally, latency efficiency is crucial, requiring optimization of resource allocation to meet response time requirements, even for bursty and fluctuating query requests. Cost efficiency also plays a significant role, as it involves minimizing monetary costs when using public   resources. These objectives are interdependent; improving one may compromise another if not designed properly, highlighting the need for flexible and comprehensive scheduling systems that can balance trade-offs between accuracy, latency, and cost.

Over the past few years, numerous works have been proposed for inference serving in cloud environments, with many studies addressing hardware optimization. However, as machine learning models are increasingly deployed in cloud, edge, or hybrid architectures, resource allocation has become a central problem, particularly when multiple Deep Neural Network (DNN) models need to be deployed on limited resources. 

Several studies have recognized that deploying multiple models on the same GPU can cause interference, leading to performance loss. For instance, some researchers have developed a unified approach to predict latency degradation for colocated models across various platforms (CPU/GPU). This latency degradation can be used in inference serving systems to evaluate model placement. However, their approach is not fine-grained, relying solely on model features like global buffer utilization and PCIe connection, which do not provide an accurate measure of execution during inference. 

Another investigation into cloud-based inference serving highlights the challenges of model interference when multiple DNNs run concurrently on a single GPU. This study proposes profiling to determine the optimal concurrency level, beyond which adding more DNNs reduces throughput and increases latency. Although the profiling cost is reported to be low, the method focuses solely on colocating identical DNNs, making it infeasible to profile all potential combinations of different DNNs sharing the same resources.

Despite the growing need for efficient resource allocation, several solutions have been proposed to address this challenge. Clipper is an ML inference serving system designed for real-time applications, providing support for various machine learning frameworks and models, aiming to simplify and accelerate the deployment and serving of ML models. TensorFlow Serving, developed for serving machine learning models for real-time predictions, automatically adjusts to changes in traffic by adding or removing replicas. However, none of these designs consider interference prior to deploying models that share resources, instead relying on adaptation mechanisms that can lead to further performance degradation.

In response to these challenges, Abacus aims to efficiently schedule and execute simultaneous DNN queries at the lower framework level. By treating DNNs as sequences of operators (layers), they adapt an operator grouping mechanism that allows operators from different DNNs to run simultaneously while ensuring Quality of Service (QoS) guarantees. However, their proposed duration model is agnostic to GPU hardware, making it challenging to collect sufficient data considering GPU types and different levels of collocation. Additionally, waiting for all operators in an operator group to finish can lead to GPU underutilization and increased latency. 

Clockwork also aims to provide predictable performance for model serving systems by acknowledging that DNN inference has deterministic performance. However, their design executes only one inference at a time, leading to GPU underutilization. 

Previous research has explored the root causes of interference. An interference-sensitive GPU inference server designed for cloud environments focused on mitigating interference when multiple models simultaneously use GPU resources. They evaluated interference using critical parameters like GPU L2 cache usage, the number of cores launched, and GPU power consumption. While the number of cores launched is important, other parameters such as power consumption and frequency are not the most significant factors causing interference. 

Recent articles have acknowledged that the configuration of cores during launches plays a more crucial role. For instance, one study used kernel occupancy rates and DRAM usage to characterize resource usage during DNN inference. Usher proposes a new approach leveraging kernel-level understanding during execution on the GPU, categorizing models as either computation-intensive or memory-intensive. However, both Usher and iGnifer approaches rely on Multi-Process Service (MPS), which allows multiple inference workloads to spatially share the GPU, a feature not available on edge platforms like Jetson. 

This comprehensive overview illustrates the complexities and ongoing challenges in managing inference workloads in GPU datacenters, emphasizing the need for innovative solutions that address interference and optimize resource allocation.