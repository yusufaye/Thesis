\setchapterpreamble[u]{\margintoc}
\chapter{Conclusion}
\labch{conclusion}

In conclusion, this thesis has presented two main contributions in the field of video analytics on distributed cloud platforms at the edge. We have introduced a new load balancing approach that adapts to heterogeneous camera sources, including fixed and mobile inputs. Furthermore, we developed a fine-grained model placement strategy that takes into account low-level \acrshort{gpu} operations and proactively addresses interference between models. The remainder of this section revisits the central research question, summarizes our proposed solutions, discusses the limitations of our work, and offers recommendations for future research directions.

\section{Research Problem and Summary of Contributions}

We have discussed in this manuscript some issues related to live video analytics and, more importantly, when dealing with resource-constrained infrastructures. In~\Cref{ch:related_work}, we discussed the many fields of research and application of edge-cloud computing, particularly video analytics applications, which are beginning to show their presence and importance in today's society. In this thesis, we have focused on two key points in particular: challenges to balancing computational demands, distributing video workloads across edge nodes with the integration of mobile cameras, and the deployment of \gls{dnn} models in resource-constrained environments, where the co-location of multiple computing tasks is often required.

Our first contribution, presented in~\Cref{ch:videojam}, identifies critical limitations in integrating mobile cameras with edge computing, primarily due to their unpredictable availability and dynamic movement, which compromise background subtractor and lead to unreliable object detection. These constraints are further exacerbated by fluctuating workloads and the need for rapid reconfiguration of processing pipelines as mobile cameras join or leave the network. To address these challenges, we introduce {\videojam}, a novel system designed to integrate heterogeneous camera sources, both fixed and mobile, while maintaining low-latency performance. {\videojam} incorporates short-term load prediction and function-level load balancing to manage high variability in computational demand. Empirical evaluations demonstrate that {\videojam} achieves 2.91× lower response times, reduces video data loss by over 4.64×, and minimizes bandwidth overhead, thereby enabling a more adaptive and resilient video analytics architecture.

In the second part of our work, presented in~\Cref{ch:roomie}, we examine deployment strategies for \glspl{dnn} in resource-constrained edge environments, particularly in mandatory co-location scenarios. Although many approaches have been proposed to optimize inference performance, most remain reactive, focusing on post-deployment planning or coarse profiling, and often fail to anticipate interference between models sharing \gls{gpu} resources. These methods typically neglect the fine-grained execution characteristics of \gls{dnn}s and rely on cloud-centric infrastructure, limiting their applicability to edge platforms. To address these shortcomings, we introduce {\roomie}, a kernel-aware orchestration system that profiles \acrshort{gpu} kernel execution sequences to accurately model interference patterns. By leveraging these profiles, {\roomie} intelligently manages model placement, scheduling, and resource allocation across edge clusters. Experimental results demonstrate that {\roomie} significantly improves overall throughput compared to state-of-the-art systems.

\section{Limitations}

While {\videojam} and {\roomie} successfully address key challenges related to edge video analysis and \gls{dnn} deployment, both systems have limitations that leave room for improvement and further research.
{\videojam} does not take into account the bandwidths of inter-functional links when determining load balancing policies. In heterogeneous network environments, where link speeds vary significantly, this omission can adversely affect the overall efficiency of the system. Furthermore, {\videojam} operates independently of the underlying deployment configuration (\eg, the number of function replicas), but it lacks mechanisms to handle overload scenarios in which incoming video sources exceed available processing capacity. Moreover, {\videojam} treats functions with similar objectives but different implementations (\eg, \acrshort{yolo} vs. \acrshort{ssd}) as identical entities, overlooking critical differences in accuracy, model size, and inference latency, factors that could significantly impact resource planning and allocation.

{\roomie}, on the other hand, faces scalability constraints, as its convergence time increases with the number of \glspl{dnn} considered for interference. This poses challenges for real-time applications that require rapid adaptation. Furthermore, {\roomie}'s current approach to profiling focuses primarily on kernel-level parameters, such as thread block size, shared memory, register usage, and occupancy. Its design does not yet incorporate a broader set of system-level factors that influence execution behavior, including PCIe bandwidth, L1/L2 cache performance, the number of \glspl{sm} available to the \gls{gpu}, and interleave latency between cores. These parameters, while less directly related to the core configuration at launch, play a crucial role in interference dynamics and overall system responsiveness.

Overall, these limitations highlight areas for future research and improvement, and we plan to address them in our future work.

\section{Recommendations and Future Work}

\subsection{Recommendations}

Our proposed systems, {\videojam} and {\roomie}, have made a significant contribution to edge-based video analytics, but they are not without limitations. We propose potential solutions to overcome these challenges.

{\videojam} offers an efficient workload balancing technique. In a situation where all functions are overloaded, we could implement additional features such as a dynamic resource allocation mechanism that can adjust resource allocation in real-time based on changing system conditions and application requirements. Furthermore, the interference-aware scheduling algorithm, {\roomie}, could be used by {\videojam} for these tasks to contribute to efficient and optimized \gls{dnn} scheduling and minimize interference. These two solutions are highly complementary and could be integrated to improve the efficiency and effectiveness of \gls{dnn} deployments in edge-based video analytics. Additionally, considering network state would enable the determination of the best load balancing policy by collecting transfer latencies (transmission) to know the available bandwidth.

To minimize the exponential latency of {\roomie}'s algorithm, we could use a machine learning-based model, such as the one developed by Usher~\cite{shubha2024usher}, to determine kernel duration. A possible initial choice would be a model that determines interference and predicts the final duration of any given kernel likely to interfere. This would then allow batch processing to be leveraged to minimize response latency. Alternatively, a model that takes into account all model sequences at once and predicts performance degradation for each of them could be used. We could utilize \gls{lstm} or \gls{tcn}~\cite{greff2016lstm,KerasTCN}, both of which have proven effective for time series data.

% Suggest how your work could be extended or improved.

% Propose new research questions or directions that emerged from your study.

\subsection{Future work}

The next phase of this research will focus on building a unified orchestration framework that treats each application as a pipeline—a structured sequence of interdependent processing stages rather than isolated \gls{dnn} deployments. This distinction is critical, as real-world video analytics applications typically involve multi-stage processing chains composed of heterogeneous functions, where tasks such as decoding, detection, tracking, and classification are tightly coupled within a unified pipeline. While {\videojam} already embraces this pipeline-centric view, future work will extend it to support dynamic deployment, scaling, and coordination of entire pipelines under resource-constrained conditions. This direction also addresses limitations observed in prior systems such as SplitStream~\cite{liang2024splitstream} and VideoStorm~\cite{201465videostorm}, which assume an abundance of edge devices and often overlook the complexities of co-location, interference, and adaptive scheduling.

\paragraph{Instance Lifecycle Management and Scaling Strategy.}
Inspired by the T-second lease mechanism introduced in SplitStream, which removes underutilized instances after a fixed interval, we propose a more structured approach to instance lifecycle management. For each application, a minimal set of instances, referred to as \textit{primitive} instances, would be deployed and preserved regardless of system state. These serve as the baseline service assignment and are selected based on variant performance characteristics. Additional instances, introduced through scaling, would be designated as \textit{reserved} and subject to removal under two conditions: when the application is no longer saturated, thereby allowing resource reclamation, or when another instance requires upscaling and would yield greater performance benefits. Any downscaling event would trigger a reconfiguration of \gls{dnn} placement to rebalance computational load and minimize interference across the system.

\paragraph{Pre-emptive Scaling and Rollback Mechanisms.}
To improve responsiveness under dynamic conditions, we envision a pre-emptive scaling strategy that anticipates saturation by identifying critical pipeline segments likely to require expansion. This includes instances positioned mid-pipeline or near the output stage, as well as those deployed last during initial configuration. Placement decisions within this framework will be guided by {\roomie}, whose kernel-aware profiling capabilities enable interference-aware scheduling across edge clusters. However, as identified in our limitations, {\roomie} may occasionally fail to provide optimal placement due to its reliance on kernel-level parameters and limited system-level context. To mitigate the impact of such suboptimal deployments, we propose the integration of a rollback mechanism. Following the deployment of a new variant, system performance will be monitored; if throughput degrades beyond acceptable thresholds, the orchestration system will revert to a previous configuration or selectively remove the variant(s) responsible. This feedback loop will enhance system adaptability and ensure that placement decisions remain aligned with performance goals.

\paragraph{Dynamic and Static Strategies for Batch Management.}
To further improve performance under dynamic deployment conditions, we propose two complementary approaches to batch size management. The first involves runtime batch size adaptation, where each variant enters a transient state following any change in worker configuration and dynamically recalibrates its batch size to optimize throughput and latency. This recalibration is performed collaboratively, with variants synchronizing to determine the most effective batch size for each, thereby avoiding contention and ensuring balanced resource utilization. The second approach leverages {\roomie}'s batch-aware profiling capabilities to determine a fixed batch size during placement. Since {\roomie} captures how different batch sizes influence kernel execution patterns, it can select a configuration that minimizes interference and maximizes performance from the outset. By supporting both adaptive and static batch sizing strategies, the orchestration framework can tailor its behavior to the specific demands of each deployment scenario.