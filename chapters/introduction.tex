\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\section{Motivation}

% The rapid evolution of artificial intelligence over the past decade has transformed it from a research curiosity into a powerful force that is revolutionizing industries, societies, and our understanding of automation. This surge in AI applications, spanning domains such as video surveillance, healthcare diagnostics, autonomous systems, and real-time analytics, demands an infrastructure capable of handling vast computational workloads with speed and efficiency.

% A significant contributor to this demand is the increasing deployment of cameras used for safety, security, and traffic control applications. These cameras generate a massive amount of video data, which, when transmitted to centralized clouds for processing, poses substantial challenges in terms of bandwidth overheads. The sheer volume of data required to be transmitted and processed not only strains network resources but also leads to increased latency, making real-time analytics and response increasingly difficult.

% To address these challenges, there is a growing shift towards processing video streams at the edge, closer to where the data is generated. Edge devices, such as smart cameras and IoT devices, are being equipped with AI capabilities to analyze video streams in real-time, reducing the need for transmitting large volumes of data to centralized clouds. However, these edge devices typically have limited resources in terms of processing power, memory, and energy, posing significant constraints on the complexity and accuracy of AI models that can be deployed.

% The need for efficient AI infrastructure is thus becoming increasingly critical, driving innovations in edge computing, specialized hardware accelerators, and optimized AI algorithms. These advancements aim to enable the efficient deployment of AI at the edge, ensuring that the benefits of real-time video analytics can be realized without compromising on performance or resource utilization. Furthermore, the proliferation of mobile cameras, such as those mounted on cars, drones, and other vehicles, presents new opportunities for expanding the scope of video analytics. Equipped with high-quality camera sensors, these mobile cameras can provide unique perspectives and insights, often being in the right place at the right time to capture critical events. Integrating them into existing architectures will support a range of analytics applications that interest camera owners, such as accident detection, crash analytics, and reconstruction. However, the dynamic nature of mobile cameras, which capture scenes that vary more rapidly than those from fixed cameras, introduces new challenges in terms of data processing and analytics, underscoring the need for adaptable and robust AI models that can effectively handle these complexities.

% The integration of mobile cameras into video analytics systems presents several challenges, including the dynamic and unpredictable nature of the workload generated by these cameras. Specifically, the workload generated by mobile cameras is more dynamic and unpredictable, requiring constant adjustments to the processing infrastructure. Additionally, the continuously changing scenes captured by mobile cameras make customary processing pipelines ineffective, such as background subtraction, which works perfectly for fixed cameras but is ineffective for mobile cameras. Moreover, as mobile cameras appear and disappear from the deployment, they generate constant changes in the deployment configuration and the number of sources to process.

% As the scale and diversity of AI workloads grow, especially in video-centric environments, the challenge of distributing these workloads across available resources becomes increasingly important. We are entering a new era of workload distribution, where systems must adaptively allocate resources, balance latency constraints, and handle surges in demand. Effective workload distribution requires intelligent scheduling and resource awareness, taking into account the dynamic nature of mobile camera data and the need for real-time processing. By distributing workloads efficiently, systems can ensure that video analytics are performed in a timely and accurate manner, even in the presence of mobile cameras.

% In addition, the growing need to integrate AI functionality into edge devices and resource-constrained environments introduces additional challenges. In such contexts, colocating multiple AI models on shared hardware is often necessary to meet performance expectations. However, colocation introduces its own set of concerns, including resource contention, model interference, and degradation in quality-of-service metrics. The limited resources of edge devices make it essential to optimize the deployment of AI models, taking into account the risk of interference and conflicts between models.

% In this context, the use of specialized hardware accelerators such as graphics processing units (GPUs) can play a crucial role. Initially designed for complex graphics rendering in video games, the use of GPUs has become indispensable in modern AI systems. Their highly parallel architecture enables them to process massive datasets and run deep learning algorithms far more efficiently than traditional Central Processing Units (CPUs). Unlike CPUs, which excel in sequential task execution, GPUs can simultaneously execute thousands of threads making them particularly well-suited for training and inference workloads common in machine learning and deep neural networks (DNNs). Frameworks like Compute Unified Device Architecture (CUDA) and Open Computing Language (OpenCL) have further enhanced GPU utility by offering granular control over kernel execution, memory access, and thread parallelism. These APIs translate raw GPU power into scalable and optimized model deployment pipelines. CUDA, in particular, has become the de facto standard for programming NVIDIA GPUs, allowing researchers and engineers to harness low-level optimization capabilities and unlock breakthrough AI performance.

% To fully harness the potential of GPUs, it is essential to understand the underlying structure of these devices, including kernel scheduling and resource constraints. GPUs consist of multiple streaming multiprocessors, each with its own set of processing cores, registers, and shared memory. The efficient scheduling of GPU resources requires careful consideration of these constraints, including the management of register allocation, shared memory access, and warp scheduling. By optimizing GPU resource allocation and scheduling, it is possible to maximize the performance of AI workloads and ensure efficient processing of video analytics.

% Effective GPU scheduling in edge devices requires a deep understanding of the interplay between model characteristics, resource constraints, and performance objectives. This includes considering factors such as model complexity, input data rates, and latency requirements, as well as the available GPU resources, such as processing cores, memory, and bandwidth. By carefully balancing these factors, it is possible to develop scheduling strategies that optimize GPU utilization, minimize latency, and ensure efficient processing of video analytics workloads, even in power-constrained edge environments. Additionally, techniques such as dynamic voltage and frequency scaling, and power gating, can be employed to further optimize power consumption and performance.


% ---

% Edge computing is a transformative approach that brings computation and data storage closer to the data source, reducing latency and bandwidth usage. This proximity to the data source makes edge computing ideal for a wide range of applications, particularly those that require real-time processing and low latency, such as video analytics. Video analytics can be efficiently represented as pipelines of functions, where each function is an advanced algorithm or AI model that processes data and passes it to the next function, forming a Directed Acyclic Graph (DAG). This modular approach allows for scalable and flexible processing. However, deploying multiple models on the same device can lead to interference and degraded performance. Therefore, smartly distributing these models across available resources, taking into account their computational requirements and dependencies, can significantly enhance overall system performance and efficiency.

\subsection{The Evolution of Video Analytics and the Need for Edge Computing}

The proliferation of video streams has revolutionized various fields, from navigation and security to control systems, by providing an indispensable source of information. Whether it's monitoring traffic for efficient navigation, enhancing security through surveillance, or controlling industrial processes, video streams offer real-time insights that drive decision-making and automation. However, the exponential growth in data volume has made cloud-based processing increasingly impractical. The limitations of bandwidth and latency, coupled with the dependence on centralized resources, create significant bottlenecks. These issues make it difficult to process video data in real-time, leading to delays and inefficiencies.

This has driven a shift towards edge computing, which brings processing closer to the data source. By moving computation to the edge, we can reduce latency, conserve bandwidth, and improve the overall responsiveness of video analytics systems. Edge computing offers numerous benefits, but it also presents unique challenges, particularly in terms of resource constraints. Even with powerful GPUs, resources are not unlimited, compelling us to rethink traditional deployment strategies.

\subsection{Optimizing Resource Utilization in Edge Computing}

One effective approach to optimize resource utilization in edge computing is to decouple the processing pipeline into multiple functions. In live video analytics, for example, each function can process data and send results to the next stage in the pipeline. This modular approach allows for more flexible and efficient use of edge resources. However, existing solutions often deploy the entire pipeline on a single device or split it across resources without adequately considering the interference that co-deployed models can have on each other's performance. This interference can lead to reduced efficiency and increased latency.

To address this, we need to recognize and measure the interference between co-deployed models before deploying them on a target resource. By doing so, we can strategically deploy functions across available resources, maximizing the efficiency of edge devices. This requires a good understanding of how the GPU execution model works during inference. Such an understanding hinges on grasping the internal mechanics of GPU architecture. Designed to manage intensive workloads, GPUs rely on a highly parallel structure made up of multiple streaming multiprocessors, each with its own set of processing cores, registers, and shared memory. This configuration enables GPUs to excel at the training and inference tasks central to machine learning and deep neural networks (DNNs).

Frameworks like Compute Unified Device Architecture (CUDA) and Open Computing Language (OpenCL) have enhanced GPU utility by offering granular control over kernel execution, memory access, and thread parallelism. CUDA, in particular, has become the standard for programming NVIDIA GPUs, enabling researchers and engineers to harness low-level optimization capabilities and achieve breakthrough AI performance. However, understanding how kernels are scheduled and executed in the GPU during inference is crucial. Each model will eventually execute multiple kernels, and each kernel will require a certain amount of register allocation, shared memory access, and warps to execute efficiently. By optimizing these aspects, we can maximize the performance of AI workloads and ensure efficient processing of video analytics.

\subsection{Addressing Fluctuating Workloads and Mobile Cameras}

Beyond deployment complexities, the performance of video analytics systems is often constrained by fluctuating workloads from camera sources. These temporal imbalances in data generation can cause certain nodes to be overloaded while leaving others underutilized, potentially leading to data gaps and diminished analytical precision. To mitigate this, architects have explored distribution strategies, either by offloading excess traffic vertically between edge devices and centralized cloud resources, or by dispersing workloads horizontally across edge clusters. However, such strategies implicitly rely on the predictability and stability of video input, typically generated from fixed cameras with consistent monitoring patterns.

This assumption begins to break down with the emergence of mobile cameras. Unlike their fixed counterparts, mobile cameras offer advantages such as increased coverage, dynamic perspectives, and enhanced situational awareness. Yet, they also introduce distinct challenges—most notably, the rapid and unpredictable variation in scene content. This volatility disrupts traditional workload planning and demands adaptive processing strategies that can respond in real-time to fluctuating video streams and computational needs.

% ---

% The objective of this thesis is to propose and develop a novel architecture that addresses the aforementioned challenges of deploying video analytics applications in resource-constrained environments. This architecture will define how processing pipelines can be efficiently distributed and managed to handle workload variations effectively. By optimizing resource utilization and ensuring efficient distribution of tasks, the proposed architecture will aim to enhance the performance and reliability of video analytics applications in diverse and dynamic settings.

This thesis aims to introduce and design an innovative architecture tailored to the aforementioned challenges associated with deploying video analytics applications in resource-constrained environments. The proposed framework will establish efficient methods for distributing and managing processing pipelines to adapt to fluctuating workloads. By optimizing resource efficiency and streamlining task distribution, we will be able to improve the overall performance and reliability of video analytics systems in varied and constantly evolving scenarios.

\section{Contributions of this Thesis}

This thesis makes several key contributions to the field of edge computing and video analytics:

Self-Balancing Architecture for Live Video Analytics: VideoJam implements a distributed load balancing system by deploying load balancers co-located with each task in the analytics pipeline. Each load balancer continuously monitors the incoming flow of frames or objects and periodically shares this information with neighboring load balancers. Based on the collected data, each load balancer independently decides how much traffic to process locally and whether to offload some of its workload to less-burdened neighbors using a lightweight machine learning model that predicts the incoming workload for each processing component and its neighbors in the near future. Additionally, the load balancers employ a congestion prevention signaling system to correct any prediction errors. VideoJam operates autonomously, adapting dynamically to changes such as new camera arrivals or departures without requiring system reboots, and balances incoming traffic accordingly. Our approach uniquely combines horizontal distribution and per-function type load balancing, driven by short-term forecasts of incoming loads.

Efficient Model Cohabitation in Edge Computing Model Serving: an orchestration architecture designed to maximize system performance in scenarios where model colocation is necessary, particularly in resource-constrained edge environments. The key innovation of Roomie is its kernel-aware interference profiling, which captures the sequential nature of GPU kernel execution patterns when multiple models share hardware resources. By analyzing how specific kernel sequences from different models interact, Roomie constructs accurate interference profiles that predict performance degradation under various colocation scenarios. This detailed approach allows Roomie to make optimal placement decisions, determining which models can efficiently coexist on the same hardware and which combinations should be avoided to ensure performance guarantees.

Building upon these contributions, the next chapter (Chapter 2) will explore the foundational background and relevant literature that inform our research. This contextual groundwork is essential for grasping the significance of our innovations and the methodological choices made throughout the thesis, thereby enabling a holistic understanding of our strategy for optimizing edge computing and video analytics.