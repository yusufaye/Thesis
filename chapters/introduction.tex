\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\section{Motivation}

% The rapid evolution of artificial intelligence over the past decade has transformed it from a research curiosity into a powerful force that is revolutionizing industries, societies, and our understanding of automation. This surge in AI applications, spanning domains such as video surveillance, healthcare diagnostics, autonomous systems, and real-time analytics, demands an infrastructure capable of handling vast computational workloads with speed and efficiency.

% A significant contributor to this demand is the increasing deployment of cameras used for safety, security, and traffic control applications. These cameras generate a massive amount of video data, which, when transmitted to centralized clouds for processing, poses substantial challenges in terms of bandwidth overheads. The sheer volume of data required to be transmitted and processed not only strains network resources but also leads to increased latency, making real-time analytics and response increasingly difficult.

% To address these challenges, there is a growing shift towards processing video streams at the edge, closer to where the data is generated. Edge devices, such as smart cameras and IoT devices, are being equipped with AI capabilities to analyze video streams in real-time, reducing the need for transmitting large volumes of data to centralized clouds. However, these edge devices typically have limited resources in terms of processing power, memory, and energy, posing significant constraints on the complexity and accuracy of AI models that can be deployed.

% The need for efficient AI infrastructure is thus becoming increasingly critical, driving innovations in edge computing, specialized hardware accelerators, and optimized AI algorithms. These advancements aim to enable the efficient deployment of AI at the edge, ensuring that the benefits of real-time video analytics can be realized without compromising on performance or resource utilization. Furthermore, the proliferation of mobile cameras, such as those mounted on cars, drones, and other vehicles, presents new opportunities for expanding the scope of video analytics. Equipped with high-quality camera sensors, these mobile cameras can provide unique perspectives and insights, often being in the right place at the right time to capture critical events. Integrating them into existing architectures will support a range of analytics applications that interest camera owners, such as accident detection, crash analytics, and reconstruction. However, the dynamic nature of mobile cameras, which capture scenes that vary more rapidly than those from fixed cameras, introduces new challenges in terms of data processing and analytics, underscoring the need for adaptable and robust AI models that can effectively handle these complexities.

% The integration of mobile cameras into video analytics systems presents several challenges, including the dynamic and unpredictable nature of the workload generated by these cameras. Specifically, the workload generated by mobile cameras is more dynamic and unpredictable, requiring constant adjustments to the processing infrastructure. Additionally, the continuously changing scenes captured by mobile cameras make customary processing pipelines ineffective, such as background subtraction, which works perfectly for fixed cameras but is ineffective for mobile cameras. Moreover, as mobile cameras appear and disappear from the deployment, they generate constant changes in the deployment configuration and the number of sources to process.

% As the scale and diversity of AI workloads grow, especially in video-centric environments, the challenge of distributing these workloads across available resources becomes increasingly important. We are entering a new era of workload distribution, where systems must adaptively allocate resources, balance latency constraints, and handle surges in demand. Effective workload distribution requires intelligent scheduling and resource awareness, taking into account the dynamic nature of mobile camera data and the need for real-time processing. By distributing workloads efficiently, systems can ensure that video analytics are performed in a timely and accurate manner, even in the presence of mobile cameras.

% In addition, the growing need to integrate AI functionality into edge devices and resource-constrained environments introduces additional challenges. In such contexts, colocating multiple AI models on shared hardware is often necessary to meet performance expectations. However, colocation introduces its own set of concerns, including resource contention, model interference, and degradation in quality-of-service metrics. The limited resources of edge devices make it essential to optimize the deployment of AI models, taking into account the risk of interference and conflicts between models.

% In this context, the use of specialized hardware accelerators such as graphics processing units (GPUs) can play a crucial role. Initially designed for complex graphics rendering in video games, the use of GPUs has become indispensable in modern AI systems. Their highly parallel architecture enables them to process massive datasets and run deep learning algorithms far more efficiently than traditional Central Processing Units (CPUs). Unlike CPUs, which excel in sequential task execution, GPUs can simultaneously execute thousands of threads making them particularly well-suited for training and inference workloads common in machine learning and deep neural networks (DNNs). Frameworks like Compute Unified Device Architecture (CUDA) and Open Computing Language (OpenCL) have further enhanced GPU utility by offering granular control over kernel execution, memory access, and thread parallelism. These APIs translate raw GPU power into scalable and optimized model deployment pipelines. CUDA, in particular, has become the de facto standard for programming NVIDIA GPUs, allowing researchers and engineers to harness low-level optimization capabilities and unlock breakthrough AI performance.

% To fully harness the potential of GPUs, it is essential to understand the underlying structure of these devices, including kernel scheduling and resource constraints. GPUs consist of multiple streaming multiprocessors, each with its own set of processing cores, registers, and shared memory. The efficient scheduling of GPU resources requires careful consideration of these constraints, including the management of register allocation, shared memory access, and warp scheduling. By optimizing GPU resource allocation and scheduling, it is possible to maximize the performance of AI workloads and ensure efficient processing of video analytics.

% Effective GPU scheduling in edge devices requires a deep understanding of the interplay between model characteristics, resource constraints, and performance objectives. This includes considering factors such as model complexity, input data rates, and latency requirements, as well as the available GPU resources, such as processing cores, memory, and bandwidth. By carefully balancing these factors, it is possible to develop scheduling strategies that optimize GPU utilization, minimize latency, and ensure efficient processing of video analytics workloads, even in power-constrained edge environments. Additionally, techniques such as dynamic voltage and frequency scaling, and power gating, can be employed to further optimize power consumption and performance.


% ---

% Edge computing is a transformative approach that brings computation and data storage closer to the data source, reducing latency and bandwidth usage. This proximity to the data source makes edge computing ideal for a wide range of applications, particularly those that require real-time processing and low latency, such as video analytics. Video analytics can be efficiently represented as pipelines of functions, where each function is an advanced algorithm or AI model that processes data and passes it to the next function, forming a Directed Acyclic Graph (DAG). This modular approach allows for scalable and flexible processing. However, deploying multiple models on the same device can lead to interference and degraded performance. Therefore, smartly distributing these models across available resources, taking into account their computational requirements and dependencies, can significantly enhance overall system performance and efficiency.

Over the past decade, the widespread availability of data from various sources, including camera steams, has become a ubiquitous feature in urban environments and beyond~\cite{}. This data is increasingly being used to gather valuable insights, contributing to improved public safety, operational efficiency, and decision-making in a variety of fields. This wealth of information not only supports surveillance activities, but also plays a critical role in traffic management, crime prevention, urban planning, and other critical applications~\cite{}. By leveraging this data, local authorities and organizations can make informed decisions that improve quality of life, safety, and optimize resource allocation.
On the other hand, the rapid transformation of artificial intelligence has made it a powerful tool that profoundly influences industries and societies. Artificial intelligence plays a crucial role in analyzing and interpreting these vast data sets, facilitating a variety of applications such as video surveillance, medical diagnostics, autonomous systems, and real-time analysis. This expansion of AI applications requires infrastructure capable of efficiently and rapidly processing large computational workloads, while offering innovative solutions to contemporary challenges.

% The rapid evolution of artificial intelligence over the past decade has transformed it from a research curiosity into a powerful force that is revolutionizing industries, societies, and our understanding of automation. This surge in AI applications, spanning domains such as video surveillance, healthcare diagnostics, autonomous systems, and real-time analytics, demands an infrastructure capable of handling vast computational workloads with speed and efficiency.
% In the other hand, the proliferation of video streams has revolutionized various fields, from navigation and security to control systems, by providing an indispensable source of information. Whether it's monitoring traffic for efficient navigation, enhancing security through surveillance, or controlling industrial processes, video streams offer real-time insights that drive decision-making and automation.

As video data becomes increasingly central to AI-driven applications, live video analytics plays a pivotal role in extracting real-time insights from visual streams~\cite{}. Live video analytics involves the real-time processing of camera feeds using algorithmic and computer vision techniques to derive actionable intelligence. These streams typically pass through a structured pipeline, often modeled as a directed acyclic graph, comprising sequential functions such as object detection, classification, and tracking~\cite{}. Video analytics has become a fundamental part of intelligent automation, enabling systems to interpret and respond to visual data in real-time. Its applications span a wide range of areas, including automated traffic management, where it facilitates the detection of traffic jams and accidents, as well as surveillance, crowd control, and assisted and autonomous driving~\cite{}. By leveraging the growing volume of video data and the insights it provides, video analytics enables individuals and organizations to make more informed, faster, and more strategic decisions.

Traditionally, video analytics has relied heavily on cloud-based processing, where raw video streams are transmitted to centralized data centers for analysis~\cite{}. However, this approach introduces significant limitations. The massive volume of video data generated by high-resolution cameras places immense strain on network bandwidth, leading to latency issues and potential bottlenecks~\cite{}. These delays are especially problematic in time-sensitive scenarios such as emergency response, industrial safety, or autonomous navigation, where milliseconds matter.

To overcome these challenges, the field is increasingly shifting toward edge computing, processing data locally at or near the source~\cite{}. This approach reduces latency, conserves bandwidth, and enhances the responsiveness of video analytics systems. Yet, edge computing introduces its own set of constraints. Despite the presence of advanced GPUs, edge devices have limited computational capacity and can quickly become overwhelmed by high-volume video streams, leading to data loss and degraded performance.

This thesis addresses two critical challenges in the design of scalable and efficient video analytics systems, each representing a distinct contribution to the field.

First, it explores the integration of mobile cameras with distributed edge computing architectures. Mobile cameras offer flexibility and dynamic coverage; however, their unpredictable nature, as they can appear and disappear at any time, requires efficient data processing to avoid longer response times. This work proposes a new approach strategy for distributing video workloads across multiple edge nodes to balance computational demands, reduce latency, and improve overall system responsiveness. By optimizing data flow and task allocation, the proposed approach enhances throughput while maintaining low-latency performance a key requirement for real-time analytics.

Secondly, this research examines the deployment of deep neural network (DNN) models in resource-constrained environments, such as edge devices. In these settings, the co-location of multiple computing tasks is often required; however, if not managed effectively, this can result in performance degradation and diminished model accuracy. This study offers new insights into the efficient placement and execution of DNN components within these environments, ensuring that analytics pipelines maintain both accuracy and responsiveness despite stringent resource limitations.

In summary, this thesis focuses on the integration of mobile camera systems with intelligent workload distribution strategies oriented towards the periphery and the optimization of deep neural network (DNN) deployment in resource-constrained environments. This work contributes to the advancement of scalable, highly accurate, and low-latency video analysis systems.
It addresses the placement problem, namely the challenge of determining the optimal deployment locations for deep neural network (DNN) functions in video analysis pipelines, taking into account resource constraints and heterogeneous camera workloads.

% ---

% Despite the broad scope of video analytics applications, this thesis focuses on two main areas. First, the integration of mobile cameras, which offer a significant advantage, and the distribution of data among different edge nodes to balance the workload, thereby increasing system throughput and responsiveness while reducing latency. Second, the deployment of the DNN model in resource-constrained environments, such as the edge, where colocation becomes a necessity and, if not managed properly, can lead to performance degradation and loss of accuracy.

% Historically, research has focused on the placement problem—deciding where to deploy components (DNN functions) of the analytics pipeline based on available resources and camera workloads. Early solutions emphasized transmitting video to centralized clouds, but the resulting network congestion necessitated either on-premises preprocessing or reduced video quality, both of which compromise application performance.

% Modern solutions aim to distribute workloads intelligently across the network. For instance, Distream~\cite{zeng2020distream} leverages dynamic video flow patterns to split processing between cameras and edge devices, while Chameleon adapts pipeline configurations using temporal and spatial predictions of video content. These innovations reflect a broader rethinking of deployment strategies, balancing the benefits of edge computing with its inherent limitations to ensure scalable, efficient, and accurate video analytics.

% ---

% The proliferation of video streams has revolutionized various fields, from navigation and security to control systems, by providing an indispensable source of information. Whether it's monitoring traffic for efficient navigation, enhancing security through surveillance, or controlling industrial processes, video streams offer real-time insights that drive decision-making and automation. However, the exponential growth in data volume has made cloud-based processing increasingly impractical. The limitations of bandwidth and latency, coupled with the dependence on centralized resources, create significant bottlenecks. These issues make it difficult to process video data in real-time, leading to delays and inefficiencies. This has driven a shift towards edge computing, which brings processing closer to the data source. By moving computation to the edge, we can reduce latency, conserve bandwidth, and improve the overall responsiveness of video analytics systems. Edge computing offers numerous benefits, but it also presents unique challenges, particularly in terms of resource constraints. Even with powerful GPUs, resources are not unlimited, compelling us to rethink traditional deployment strategies and workload distribution.

% Live video analytics involves processing video streams in real-time using algorithmic and computer vision techniques to extract valuable information. These streams pass through a series of interconnected modules—such as object detection, classification, and tracking—structured as a directed acyclic graph, where each module's output feeds into the next. Traditionally, the focus of video analytics research has been on the placement of these processing pipelines, determining where in the infrastructure to deploy them based on available computing resources and camera workloads. Early solutions relied heavily on centralized cloud processing, which, despite offering vast computational power, often led to network congestion and compromised video quality due to bandwidth limitations.

% To address these limitations, newer approaches have shifted toward edge computing, placing analytics pipelines closer to the video source. This reduces transmission delays and alleviates network strain, but introduces new constraints due to the limited resources of edge devices. Solutions like Distream and Chameleon have emerged to optimize workload distribution and pipeline configuration by leveraging predictable patterns in fixed camera feeds. However, these systems are built on the assumption of static camera behavior. The rise of mobile cameras—embedded in smartphones, vehicles, and drones—challenges this paradigm. Their dynamic nature and unpredictable movement introduce variability that current architectures struggle to accommodate, highlighting the need for more adaptive and responsive video analytics systems capable of integrating mobile feeds effectively.

% ---

% The proliferation of video streams has revolutionized various fields, from navigation and security to control systems, by providing an indispensable source of information. Whether it's monitoring traffic for efficient navigation, enhancing security through surveillance, or controlling industrial processes, video streams offer real-time insights that drive decision-making and automation. However, the exponential growth in data volume has made cloud-based processing increasingly impractical. The limitations of bandwidth and latency, coupled with the dependence on centralized resources, create significant bottlenecks. These issues make it difficult to process video data in real-time, leading to delays and inefficiencies.

% This has driven a shift towards edge computing, which brings processing closer to the data source. By moving computation to the edge, we can reduce latency, conserve bandwidth, and improve the overall responsiveness of video analytics systems. Edge computing offers numerous benefits, but it also presents unique challenges, particularly in terms of resource constraints. Even with powerful GPUs, resources are not unlimited, compelling us to rethink traditional deployment strategies and workload distribution.


\section{Challenges in Incorporating Mobile Cameras}

Mobile cameras offer a dynamic and flexible viewpoint for video analytics, capturing scenes that fixed cameras might miss due to their static nature. However, this mobility poses significant challenges for video analytics architectures, especially when both types of cameras coexist in a deployment. The first major challenge lies in the heterogeneous performance profiles of processing pipelines. Fixed cameras benefit from stable backgrounds, which allow for effective object isolation using background subtraction. Mobile cameras, on the other hand, constantly change scenes, making background subtraction unreliable and often leading to false detections. To address this, mobile camera pipelines often rely on single-pass object recognition models such as YOLO, which, while more consistent, are also more resource-intensive. This divergence in processing requirements underscores the importance of tailoring analysis pipelines to the specific camera type.

The second challenge involves managing highly variable workloads. Traditional workload balancing strategies rely on predictable patterns based on camera location and time of day. Distream~\cite{zeng2020distream}'s architecture attempts to optimize workload distribution using cross-device balancing and adaptive controllers. However, mobile cameras disrupt these assumptions by introducing unpredictable scene changes and intermittent presence in the network, making long-term prediction models ineffective. The third challenge centers on deployment configuration flexibility. Fixed camera setups typically allow for slow, deliberate changes based on long-term usage patterns. Mobile cameras, however, demand real-time adaptability, as they can join or leave the network at any moment. This necessitates an online load balancing system capable of reconfiguring processing pipelines within seconds, without requiring full system reboots. Together, these challenges highlight the need for a fundamentally more agile and responsive video analytics architecture.

% Beyond the complexities of deployment, the performance of video analytics systems is often limited by fluctuations in workload from camera sources, due to the constant evolution of captured scenes over time. Since the number and nature of objects in images vary (\eg, sudden crowd formation or vehicle movements), different functions in the processing pipeline (\eg, object detection, tracking, and classification) experience uneven computational loads, leading to temporal imbalance and resource conflicts across the system. These temporal imbalances in data generation can cause certain nodes to be overloaded while leaving others underutilized, potentially leading to data gaps and diminished analytical precision. To address this issue, recent research has explored distribution strategies, either by vertically offloading excess traffic between edge devices and centralized cloud resources, or by horizontally distributing workloads across edge clusters. However, such strategies implicitly rely on the predictability and stability of video input, typically generated from fixed cameras with consistent monitoring patterns.

% This assumption begins to break down with the emergence of mobile cameras. Unlike their fixed counterparts, mobile cameras offer advantages such as increased coverage, dynamic perspectives, and enhanced situational awareness. Yet, they also introduce distinct challenges—most notably, the rapid and unpredictable variation in scene content. This volatility disrupts traditional workload planning and demands adaptive processing strategies that can respond in real-time to fluctuating video streams and computational needs.


\section{Deployment Challenges on Edge-Cloud Devices}

Optimizing resource utilization in edge computing environments, such as mobile platforms, low-power nodes, or remote devices, requires a modular approach to AI deployment. Decoupling the processing pipeline into separate functions (\eg, background subtraction, object detection, tracking, and classification), as in live video analytics, allows each stage to process data independently and pass results downstream, providing greater flexibility and efficiency. However, due to the limited computing and energy resources available in these environments, colocating multiple AI models on shared hardware often becomes a necessity to meet real-time performance requirements and reduce deployment costs. This pragmatic choice, while effective in theory, introduces a series of challenges: resource contention, interference between models, and degradation of quality of service metrics. Existing solutions often overlook these dynamics, deploying entire pipelines on single devices or distributing them without considering the negative interactions between co-deployed models. To mitigate these risks and ensure fair, predictable, and high-performance AI inference, sophisticated planning and optimization strategies are essential.

A fundamental first step in this process is to recognize and measure interference between co-deployed models before assigning them to a target GPU resource. By doing so, we can strategically deploy functions across available resources, maximizing the efficiency of edge devices. This requires a good understanding of how the GPU execution model works during inference. Such an understanding hinges on grasping the internal mechanics of GPU architecture. Designed to manage intensive workloads, GPUs rely on a highly parallel structure made up of multiple streaming multiprocessors (SM), each with its own set of processing cores, registers, and shared memory. This configuration enables GPUs to excel at the training and inference tasks central to machine learning and deep neural networks (DNNs).

Frameworks like Compute Unified Device Architecture (CUDA) and Open Computing Language (OpenCL) have enhanced GPU utility by offering granular control over kernel execution, memory access, and thread parallelism. CUDA, in particular, has become the standard for programming NVIDIA GPUs, enabling researchers and engineers to harness low-level optimization capabilities and achieve breakthrough AI performance. However, understanding how kernels are scheduled and executed in the GPU during inference is crucial. Once deployed, each model will execute several kernels successively, where each kernel requires a certain amount of register allocation, shared memory, and warps to run efficiently. This low-level view complements high-level planning strategies, enabling smarter placement of models, particularly in edge environments where resource constraints necessitate the co-deployment of deep neural networks (DNNs).



\section{Contributions of this Thesis}

This thesis aims to introduce and design an innovative architecture tailored to the aforementioned challenges associated with deploying video analytics applications in resource-constrained environments. The proposed framework will establish efficient methods for distributing and managing processing pipelines to adapt to fluctuating workloads. By optimizing resource efficiency and streamlining task distribution, we will be able to improve the overall performance and reliability of video analytics systems in varied and constantly evolving scenarios.

This thesis presents several significant contributions to the fields of edge computing and video analytics:

\paragraph{Self-Balancing Architecture for Live Video Analytics.} VideoJam implements a distributed load balancing system by deploying load balancers colocated with each task in the analytics pipeline. Each load balancer continuously monitors the incoming flow of frames or objects and periodically shares this information with neighboring load balancers. Based on the collected data, each load balancer independently decides how much traffic to process locally and whether to offload some of its workload to less-burdened neighbors using a lightweight machine learning model that predicts the incoming workload for each processing component and its neighbors in the near future. Additionally, the load balancers employ a congestion prevention signaling system to correct any prediction errors. VideoJam operates autonomously, adapting dynamically to changes such as new camera arrivals or departures without requiring system reboots, and balances incoming traffic accordingly. Our approach uniquely combines horizontal distribution and per-function type load balancing, driven by short-term forecasts of incoming loads.

\paragraph{Efficient Model Cohabitation in Edge Computing Model Serving.} An orchestration architecture designed to maximize system performance in scenarios where model colocation is necessary, particularly in resource-constrained edge environments. The key innovation of Roomie is its kernel-aware interference profiling, which captures the sequential nature of GPU kernel execution patterns when multiple models share hardware resources. By analyzing how specific kernel sequences from different models interact, Roomie constructs accurate interference profiles that predict performance degradation under various colocation scenarios. This detailed approach allows Roomie to make optimal placement decisions, determining which models can efficiently coexist on the same hardware and which combinations should be avoided to ensure performance guarantees.

Building upon these contributions, the next chapter (Chapter 2) will explore the foundational background and relevant literature that inform our research. This contextual groundwork is essential for grasping the significance of our innovations and the methodological choices made throughout the thesis, thereby enabling a holistic understanding of our strategy for optimizing edge computing and video analytics.