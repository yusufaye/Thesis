\section{Related Work}\label{sec:related}

With the proliferation of deep learning-based applications offered as online services, managing and scheduling large-scale inference workloads in \gls{gpu} data centers has become increasingly critical. Unlike resource-intensive training workloads, inference tasks have distinct characteristics that require specialized scheduling solutions. The objectives of inference scheduling (accuracy, latency, and cost-effectiveness) are closely linked. Accuracy efficiency involves selecting the most suitable model for each query and allocating resources intelligently. Latency efficiency requires meeting response time constraints, even in the face of irregular or fluctuating loads. Cost efficiency aims to minimize financial expenditure, particularly in public cloud environments. These objectives are often contradictory, requiring flexible and comprehensive planning systems capable of balancing trade-offs between different performance dimensions.

\paragraph{Inference Serving Systems.}
Several systems have been proposed to address the challenges of inference scheduling. Clipper~\cite{2017Clipper} supports multiple ML frameworks and simplifies model deployment for real-time applications, but does not account for resource interference. TensorFlow-Serving~\cite{olston2017tensorflowserving} adapts to traffic changes by scaling replicas, yet similarly overlooks interference among colocated models. Clockwork~\cite{gujarati2020servingdnnslikeclockwork} offers predictable performance by executing one inference at a time with full \acrshort{gpu} capacity, but this design underutilizes \acrshort{gpu} resources due to limited concurrency. Proteus~\cite{ahmad2024proteus} introduces adaptive batching and dynamic model variant selection to optimize throughput and accuracy. However, its restriction of one model variant per device limits co-location and parallel execution, leaving \acrshort{gpu} resources under exploited.

\paragraph{Multi-Tenant \acrshort{dnn} Inference on Shared \acrshort{gpu}s.}
To improve resource utilization, recent work has explored concurrent execution of multiple models on shared \acrshort{gpu}s. INFaaS~\cite{francisco2021infaas} enables model-less serving by dynamically selecting variants based on query load and performance constraints. It co-locates variants and scales workers reactively, but struggles with profiling complexity and lacks proactive interference prediction. Colti~\cite{mobin2023colti} enhances throughput by colocating models for training and inference. Yu et al.~\cite{yu2021automated} exploit operator-level independence to schedule concurrent execution across streams, reducing latency. REEF~\cite{han2022microsecond} supports kernel preemption and priority-based sharing to maintain predictable performance. Miriam~\cite{zhao2023miriam} introduces elastic kernels for real-time prioritization, enabling flexible scheduling across tasks with varying criticality. While these systems optimize post-deployment execution, they often neglect initial placement strategies that could preemptively mitigate interference.

\paragraph{Interference-Aware Inference Serving.}
Recent studies have focused on proactive scheduling by modeling and predicting interference between concurrently running models. Mendoza et al.~\cite{mendoza2021interference} propose a latency degradation model based on global buffer and PCIe utilization, but the coarse granularity limits predictive accuracy. Scrooge~\cite{hu2021scrooge} profiles concurrency thresholds for identical \acrshort{dnn}s to determine optimal co-location, yet its approach is infeasible for heterogeneous model combinations due to profiling overhead. Abacus~\cite{cui2021Abacus} schedules operators from multiple models jointly to maintain QoS, but its hardware-agnostic duration model and reactive execution lead to underutilization and increased latency. iGnifer~\cite{xu2023iGniter} adopts a low-level perspective, using \acrshort{gpu} metrics such as L2 cache usage and core launch counts to characterize interference. However, coarse indicators like power consumption prove less predictive. Usher~\cite{shubha2024usher} refines interference modeling by analyzing kernel occupancy and \gls{dram} usage, distinguishing between compute- and memory-intensive workloads. Both Usher and iGnifer rely on NVIDIA's \gls{mps}~\cite{nvidiaMPS575} for spatial sharing, which limits their applicability in edge environments such as NVIDIA Jetson, where \gls{mps} is unsupported.