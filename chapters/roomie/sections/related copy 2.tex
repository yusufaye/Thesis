\section{Related Work}\label{sec:related}

As more deep learning-based applications are released as online services, managing and scheduling large-scale inference workloads in GPU datacenters has become increasingly critical. Unlike resource-intensive training workloads, inference jobs have unique characteristics and requirements that demand new scheduling solutions. The goals of inference scheduling are multifaceted, including accuracy efficiency, which can be achieved by selecting the best model for each input query and intelligently allocating resources; latency efficiency, which requires optimizing resource allocation to meet response time requirements, even for bursty and fluctuating query requests; and cost-efficiency, which involves minimizing monetary costs when using public cloud resources. These objectives are interdependent, and improving one goal may compromise another if not designed properly, highlighting the need for flexible and comprehensive scheduling systems that can balance tradeoffs between accuracy, latency, and cost.

Despite the growing need for efficient resource allocation, several solutions have been proposed to address this challenge. Clipper~\cite{2017Clipper} is a notable example of an ML inference serving system designed for real-time applications, providing support for a variety of machine learning frameworks and models, and aiming to simplify and accelerate the deployment and serving of ML models. In the other hands, TensorFlow-Serving~\cite{olston2017tensorflow}, a system developed for serving machine learning models for making predictions in real-time. It automatically adjusts to changes in traffic by adding or removing replicas. Nevertheless, none of these designs consider interference prior to deployment models share resources, instead relying on an adaptation mechanism that can lead to further performance degradation. In the other hand, Clockwork~\cite{gujarati2020servingDNNlikeclockwork} was proposed to provide predictable performance for model serving systems, by acknowledging that DNN inference has deterministic performance when running with full GPU capacity. However, their design only execute one inference at a time even when there are multiple models loaded on the GPU. In fact, Clockwork workers only overlap execution of an inference with data loading through two different CUDA Streams. Which leads to GPU underutilization as during inference all kernels that are launched would leave some resources. Proteus~\cite{ahmad2024proteus} investigates the architectural design of an inference-serving system capable of sustaining high throughput while dynamically scaling accuracy to optimize overall performance. The system addresses three interrelated optimization challenges: selecting the appropriate model variants (\eg, lightweight versions with reduced accuracy), distributing these variants across heterogeneous hardware platforms, and distributing query workloads across devices to balance latency and resource utilization. One of Proteus' notable strengths is its adaptive batch processing strategy, which defers query execution within time constraints to aggregate larger batches, thereby improving processing efficiency. However, the framework has limitations in its model placement strategy, as it restricts each device to a single model variant, thereby neglecting the possibilities of simultaneous execution and colocation that could better leverage underutilized GPU resources.
While these designs effectively optimize throughput and accuracy, their reliance on rigid mappings between models and devices tends to overlook opportunities for co-location and parallel execution, highlighting the need for more flexible placement strategies that better leverage shared GPU resources.


Multi-Tenant DNN Inference on Shared GPUs.
A subsequent line of research investigates techniques for optimizing model performance in environments characterized by concurrent execution, where multiple models or processes must contend for shared GPU resources. For instance, INFaaS~\cite{francisco2021infaas}, a model-less automated system for distributed inference serving that streamlines the deployment of deep neural networks (DNNs) by allowing users to specify performance and accuracy requirements without selecting specific model variants per query. It dynamically selects or switches model variants based on query arrival and load changes, co-locates variants for efficient resource use, and leverages a VM-Autoscaler to provision workers according to utilization and interference levels. However, the system faces limitations in profiling and generating variants, particularly when considering variant colocation, as the profiling space becomes exponentially large and time-consuming. Additionally, INFaaS lacks a proactive strategy to predict performance degradation due to variant interference, relying instead on reactive state tracking mechanisms to identify overloaded or interfered variants and trigger worker scaling.
Colti~\cite{mobin2023colti} offers a technique that improves the efficiency of training and inference tasks by colocating deep neural networks on GPUs, thereby increasing overall throughput and reducing execution times.
Another work in~\cite{yu2021automated}, leverage the fact that DNN inference consists of a series of operators (convolution, dense, etc.) and exploits their independence between different DNNs to schedule their execution concurrently, assigning each model to a dedicated stream, splitting operator sequences into shorter steps, and orchestrating their execution to balance resources and minimize latency.
REEF~\cite{han2022microsecond}, a DNN inference serving system that enables efficient concurrent execution and kernel preemption on GPUs, allowing multiple models with different priorities to share resources while minimizing latency and maintaining predictable performance.
Miriam~\cite{zhao2023miriam}, in the other hand, introduces a framework for GPUs that enables the simultaneous execution of DNN tasks with varying real-time requirements, using elastic kernels, which are smaller, more flexible units that can be dynamically scheduled and reassigned according to their criticality and priority. Overall, these approaches focus on post-deployment optimization of models sharing the same GPU, without addressing the issue of optimal initial placement or co-location strategy. Other work stands out by directly addressing this deployment phase, integrating co-location and inter-GPU planning strategies from the outset to anticipate interference and improve overall efficiency.



  Interference-Aware Inference Serving.
  Recent studies have focused on proactive scheduling by modeling and predicting interference between concurrently running models. For instance, the authors in~\cite{mendoza2021interference} have developed a unified approach to predict latency degradation for colocated models and across a variety of platforms (CPU/GPU). This latency degradation can be used in inference serving systems to evaluate model placement. However, their approach is not fine-grained as it is based solely on model features that are the utilization of the global buffer and PCIE connection for running on the GPU device. While we recognize that these parameters play an important role, considering them does not provide a more accurate measure of their execution during inference. 
  Another recent investigation into cloud-based inference serving highlights the challenges of model interference when multiple DNNs are run concurrently on a single GPU~\cite{hu2021scrooge}. Scrooge proposes profiling to determine the optimal concurrency level, beyond which adding more DNNs reduces throughput and increases latency. Although the profiling cost is reported to be low, the method focuses solely on colocating identical DNNs, making it infeasible to profile all potential combinations where different DNNs share the same resources. The sheer number of such combinations would be overwhelming to profile.
  Operator-level scheduling frameworks, like Abacus~\cite{cui2021Abacus} addresses DNN interference by scheduling operators (\eg, Convolution, ReLU) from multiple models to run concurrently. It treats each DNN as a sequence of operators and groups them for joint execution, aiming to maintain quality-of-service guarantees. This operator-level scheduling simplifies coordination and enables deterministic execution.
  While this approach models DNNs as sequences of operators (\eg, Convolution, ReLU), it overlooks the finer granularity of GPU execution, where each operator may launch multiple kernels with distinct resource demands. Moreover, Abacus's duration model is agnostic to GPU hardware characteristics, which are critical to inference performance. Collecting sufficient profiling data across diverse GPU architectures and collocation scenarios presents a significant challenge. Additionally, the system enforces deterministic execution by waiting for all operators in a group to complete, potentially leading to underutilization of GPU resources and increased latency. Notably, Abacus operates reactively, analyzing execution only after model deployment, without offering mechanisms to assess model compatibility or predict performance degradation beforehand.
  In contrast, iGnifer~\cite{xu2023iGniter} adopts a low-level perspective on GPU resource management by introducing an interference-sensitive inference server tailored for cloud environments. This system aims to mitigate performance degradation caused by concurrent model execution on shared GPU resources. To characterize interference, iGnifer employs several hardware-level metrics, including GPU L2 cache usage, the number of launched cores, and power consumption. While the number of launched cores is a meaningful indicator of contention, other parameters such as power and frequency are less predictive of interference severity. More influential factors, such as the configuration and scheduling of cores during kernel launches, play a critical role in shaping performance outcomes. These nuances are often overlooked in coarse-grained models, underscoring the need for more precise profiling techniques.
  This is acknowledged in a recent article, Usher~\cite{shubha2024usher}, which proposes a kernel-level approach to interference mitigation by analyzing the achieved kernel achieved occupancy and DRAM usage during DNN inference. Usher introduces a model classification scheme that distinguishes between compute-intensive and memory-intensive workloads, recognizing that large language models (LLMs), for instance, demand significantly more memory bandwidth than computational throughput. However, from the GPU's perspective, each model ultimately translates into a set of kernels with varying resource demands, independent of the model's high-level classification.

  In addition, both Usher and iGnifer rely on NVIDIA's Multi-Process Service (MPS)~\cite{nvidiaMPS575} to enable spatial sharing of GPU resources among concurrent inference tasks. While MPS facilitates efficient resource partitioning in cloud environments, it is not supported on edge platforms such as NVIDIA Jetson. This limitation restricts the applicability of these approaches in edge computing scenarios, where hardware constraints and the absence of MPS demand alternative interference-aware scheduling strategies that operate without relying on such infrastructure.