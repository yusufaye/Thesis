\section{Conclusion}\label{sec:conclusion}

Across both cloud and edge environments,~\roomie{} consistently delivers superior performance under diverse and increasingly demanding workloads, by achieving up to 17× lower latency and 1.5× higher throughput than competing strategies. Its heuristic maintains bounded error relative to the optimal deployment, often within 10\% even under high concurrency, despite the challenges posed by profiling overhead, model saturation, and architectural variability. These results hold across real-world and synthetic datasets, confirming~\roomie{}'s robustness and scalability. By intelligently minimizing cross-model interference and adapting to resource constraints,~\roomie{} proves to be a practical and interference-aware solution for multi-model inference in modern GPU systems.