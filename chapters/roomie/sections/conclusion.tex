\section{Conclusion}\label{sec:conclusion}

{\roomie} consistently delivers high-performance deployments across both cloud and edge \acrshort{gpu} environments. In cloud settings, it achieves up to 17× lower latency and maintains over 97\% processing rate, outperforming competing strategies through interference-aware colocation and saturation resilience. On edge devices, {\roomie} sustains its advantage with up to 9× lower response times and 1.5× higher throughput, effectively navigating resource constraints. {\roomie}'s heuristic maintains deployment error within 7--8\% of the optimal, even under high concurrency, despite challenges such as profiling overhead and architectural variability. These results hold across real-world and synthetic datasets, confirming {\roomie}'s robustness and scalability. By intelligently minimizing cross-model interference and adapting to resource constraints, {\roomie} proves to be a practical and interference-aware solution for multi-model inference in modern \acrshort{gpu} systems.