\section{Conclusion}\label{sec:conclusion}

\roomie{} consistently delivers high-performance deployments across both cloud and edge \acrshort{gpu} environments. In cloud settings, it achieves up to 17× lower latency and maintains over 97\% processing rate, outperforming competing strategies through interference-aware colocation and saturation resilience. On edge devices,~\roomie{} sustains its advantage with up to 9× lower response times and 1.5× higher throughput, effectively navigating resource constraints. Its heuristic maintains bounded error relative to the optimal deployment, often within 10\% even under high concurrency, despite challenges such as profiling overhead and architectural variability. These results hold across real-world and synthetic datasets, confirming~\roomie{}'s robustness and scalability. By intelligently minimizing cross-model interference and adapting to resource constraints,~\roomie{} proves to be a practical and interference-aware solution for multi-model inference in modern \acrshort{gpu} systems.
