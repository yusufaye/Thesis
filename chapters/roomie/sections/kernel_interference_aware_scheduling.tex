\section{Kernel Interference-Aware Scheduling}\label{sec:kernel_interference_aware_scheduling}

This section explains the main concept behind {\roomie}. First, we introduce the idea of interference and describe a method for estimating it. Next, we present an algorithm that efficiently calculates the interference among various models. Finally, we detail a placement algorithm that utilizes the interference estimation from the first procedure to efficiently allocate new incoming models to the available \glspl{gpu}.


% To determine this value: 1. first we determine how many warps from a kernel we can have, and if that is greater than W, so, Wb would be equal to W, since this is the maximum number of W an SM can support. In that case we say that the kernel is limited by warps. 2. and 3. since threads may require registers and shared memory for work, after we determine how many warps a block can have, we can then look at how feasible many blocks can reside in a SM while the resources (registers and shared mem) can satisfy them. This will limit the number of blocks (evenly the number of warps), and this is called limited by register (shared mem).
% The minimum number of blocks is consider as the number of blocks (b). Then time with a the number of warps a block has (Wb), we then get the total number of warps that can be active. Divide this by the W we get the theoretical occupancy.


\subsection{Kernel Execution and Occupancy Modeling}\label{subsec:kernel_execution_and_occupancy_modeling}

Modern deep learning inference on NVIDIA \glspl{gpu} relies on the efficient execution of \gls{cuda} kernels—parallel routines that process data across thousands of threads. Each layer (such as convolution, activation, pooling, or normalization) of a model typically launches one or more kernels. Put simply, \gls{dnn} inference is the sequential execution of these kernels on the \gls{gpu}, and their performance depends heavily on how well they utilize the GPU's hardware resources.
%Modern deep learning inference on NVIDIA GPUs relies on the efficient execution of \gls{cuda} kernels—parallel routines that process data across thousands of threads. Each layer of a model typically launches one or more kernels, and their performance depends heavily on how well they utilize the GPU's hardware resources.
Understanding this execution model is essential for analyzing and optimizing \gls{gpu} occupancy, which reflects how many computational units are actively engaged during kernel execution. In this section, we describe the computation of the theoretical occupancy in three key steps. For further details on occupancy modeling and constraints, we invite the reader to refer to the \gls{cuda} documentation~\cite{nvidia2025cuda} and kernel tuning studies such as~\cite{lim2017autotuninggpukernelsstatic}.
%In this section, we break down the process into three key steps: determining the number of warps per block, calculating the maximum number of blocks that can be scheduled per Streaming Multiprocessor (SM), and finally computing the theoretical occupancy. For further details on occupancy modeling and constraints, refer to the \gls{cuda} documentation~\cite{nvidia2025cuda} and kernel tuning studies such as~\cite{lim2017autotuninggpukernelsstatic}.

\paragraph{Warp configuration.} \gls{cuda} threads are organized into warps, groups of 32 threads that execute instructions in lockstep. These warps are further grouped into blocks, which serve as the basic scheduling units on the \gls{gpu}. The number of warps per block ($W_b$) depends on the kernel's thread configuration and remains fixed for that kernel. This value is the starting point for occupancy analysis, as it determines how many warps each block contributes to the \gls{sm}. Understanding this structure is crucial because it defines the granularity of parallelism and the baseline resource demand per block.

\paragraph{Maximum blocks given by \gls{sm} constraints.}
After establishing the number of warps per block ($W_b$), the next step is to determine the mximum number of blocks ($b$) that can be scheduled concurrently on a \gls{sm}. This scheduling is governed by a combination of hardware limits and resource demands. While the \gls{sm} has a fixed upper bound on the number of warps it can support ($W$), this limit is not always the only element that influences $b$. Indeed, each thread within a block consumes registers and shared memory, and these resources are finite per \gls{sm}. If the required registers or shared memory are exhausted before reaching the warp limit, the number of blocks that can be scheduled is reduced accordingly. In such cases, we say the kernel is \textit{limited by registers} or \textit{limited by shared memory}. The most restrictive constraint—whether warp count, register usage, or shared memory—ultimately determines the maximum number of blocks $b$ that can be accommodated, as explained in~\cite{lim2017autotuninggpukernelsstatic}.
% \qa{This part reads weird. It says that one or the other might be the bottleneck; however, we mostly just talk about the blocks/warps later and not about registers /shared memory. Maybe we want to say something about why we do not consider them later?}{Yes, I mostly mentioned only warps and blocks as going the way we determine $b$ is not explicitly address, rather I provided reference for additional details on the calculation (see~\cite{lim2017autotuninggpukernelsstatic}). Furthermore, in "Interference and Adjusted Occupancy" the $\phi$ (or $\Phi$ for the GPU) represent all the three: warps, registers, and shared mem}\FF{It should be clear now that the maximum number of blocks $b$ is computed as a combination of constraints, as explained in reference~\cite{lim2017autotuninggpukernelsstatic}. We do not describe in details how $b$ is computed.}

\paragraph{Occupancy calculation.}  
With both the warps per block ($W_b$) and the maximum number of blocks ($b$)
% \qa{Why is b the maximum?}{The previous paragraph explains how we determine the number of block with respect to warps, registers, and shared mem} 
determined, we can now compute the total number of active warps on the \gls{sm} as $W_b \times b$. The theoretical occupancy ($o$) of a kernel is then calculated by dividing this value by the maximum number of warps the \gls{sm} can support:

\begin{equation}\label{eq:occupancy}
	o = \frac{W_b \times b}{W}
\end{equation}

Note that the occupancy formulation and its constraints are based on the model described in~\cite{lim2017autotuninggpukernelsstatic}.

This ratio expresses the proportion of active warps relative to the \gls{sm}'s maximum capacity. Theoretical occupancy offers a clean upper bound on how effectively a kernel can utilize \gls{gpu} resources—assuming ideal conditions and exclusive access to the hardware. Yet in practice, such isolation is rare. Real-world deployments often involve multiple kernels or models running concurrently, each competing for shared resources like registers, shared memory, and warp slots.

This concurrency introduces a new layer of complexity. The assumptions behind theoretical occupancy begin to break down as resource contention reshapes the effective scheduling landscape. To understand how performance degrades under these conditions, we must move beyond static occupancy formulas and examine how interference alters kernel execution.

\subsection{Interference and Adjusted Occupancy}

Interference occurs when one kernel prevents another from reaching its intended performance level due to simultaneous scheduling. In simpler terms, it arises when the total resource demand from multiple kernels exceeds the GPU's capacity. Formally, interference is present when the cumulative resource demand across all active kernels exceeds the hardware budget:

\begin{equation}\label{eq:interf}
	\sum_{k \in K} \varphi_k > \Phi
\end{equation}
where $K$ is the set of active kernels, $\varphi_k$ denotes the resource usage of kernel $k$, and $\Phi$ represents the total resource capacity of the \gls{gpu}. This formulation captures the condition under which resource contention begins to affect execution.

A kernel designed to operate at a certain occupancy may be forced to share resources, resulting in fewer warps or blocks than expected. This leads to reduced performance and increased latency. Depending on the severity of contention, kernels may still execute concurrently but with diminished efficiency—for example, a kernel that would normally schedule the maximum number of blocks allowed by its resource configuration may be restricted to fewer due to limited shared memory or register availability. In more extreme cases, one kernel may delay the other entirely, forcing sequential execution and increasing overall latency.

This degradation can be quantified by recomputing the occupancy of each kernel based on the reduced share of resources it receives under interference. When multiple kernels are scheduled concurrently, the number of blocks that can reside on an \gls{sm}—and thus the number of active warps—is constrained by the availability of shared memory, registers, and warps. These resources, allocated across all active kernels, may limit each kernel to fewer blocks than its standalone configuration would allow. The recomputed occupancy, referred to as \textit{adjusted occupancy}, captures how resource contention alters the kernel's execution profile and provides a basis for estimating performance loss in multi-kernel deployments.

The adjusted occupancy also depends on the specific scheduling policy used by the \gls{gpu} to re-allocate resources for the models' kernels. However, as reported in Nvidia CUDA documentation~\cite{nvidia2025cuda}, it is difficult to rely on a fixed policy for handling different concurrent operations. This translates to the impossibility of determining in advance a well defined order for the kernels' processing. For this reason, we approximate scheduling policies using simplified classical strategies such as equal partitioning, priority-based assignment, or first-come-first-served ordering. While these policies do not reflect the precise behavior of the \gls{gpu} scheduler, they offer a practical basis for estimating performance under interference and evaluating deployment scenarios.
% \qa{We need to say somewhere why we can't just use the real scheduling policy.}{Francescomaria, can you please help me on this if you have time}
%\ff{Since the exact scheduling policy used by the GPU is not publicly documented and may vary across architectures, we approximate scheduling policies using simplified classical strategies such as equal partitioning, priority-based assignment, or first-come-first-served ordering.} While these policies do not reflect the precise behavior of the GPU scheduler, they offer a practical basis for estimating performance under interference and evaluating deployment scenarios. \qa{We need to say somewhere why we can't just use the real scheduling policy.}{Francescomaria, can you please help me on this if you have time}

Let $\tilde{b}_k$ denote the adjusted number of blocks that kernel $k$ can launch under constrained resources and this is determined, as derived from the \gls{gpu} \gls{sm} resource constraints outlined in Section~\ref{subsec:kernel_execution_and_occupancy_modeling}. We compute the adjusted occupancy accordingly. With $\tilde{b}_k$, we can derive the new occupancy $\tilde{o}_k$ using the same formulation as in~\Cref{eq:occupancy}. This allows us to estimate the new execution time of the kernel under interference:

\begin{equation}
	\tilde{d}_k = d_k \times \frac{o_k}{\tilde{o}_k}
\end{equation}
where $d_k$ is the execution time of kernel $k$ in isolation, and $o_k$ is its original theoretical occupancy. In this way, the new execution time of kernel $k$ is inversely proportional to its adjusted occupancy $\tilde{o}_k$, thereby modeling the effect of interference, where reduced kernel occupancy results in increased execution time. If $\tilde{o}_k = o_k$, then no interference occurs and $\tilde{d}_k = d_k$.
% \qa{I am not sure why there is a direct relationship between occupancy and duration. Can you give an intuition of why that is the case? Or if it is just an approximation, say so and explain why it is sufficient.}{A kernel with a given occupancy will complete its execution in $d$ duration. If occupancy decreases, the same work is done with fewer active blocks, leading to longer execution. Think of it like transporting goods: with fewer containers, more trips are needed to move everything, increasing the total time.}

Interference ends once the first kernel completes its execution. Assuming at most two kernels are interfering, the remaining kernel continues alone without interference. We define the interference period as the time required for the first kernel to finish, $\Delta = \min \left\{ \tilde{d}_k \right\}_{\forall k \in K}$.
For the remaining kernel, its total duration is composed of two phases: the initial interference period $\Delta$, during which it runs with reduced occupancy $\tilde{o}_k$, and the remaining portion of its execution, which proceeds at full occupancy $o_k$. To account for the change in execution speed, we adjust the remaining time accordingly. Specifically, the portion $\tilde{d}_k - \Delta$, originally computed under reduced occupancy, is scaled by $\frac{\tilde{o}_k}{o_k}$ to reflect the normal execution once interference ends. Formally expressed:

\begin{equation}
	\tilde{d}_k =
	\begin{cases}
		\tilde{d}_k & \text{if } \tilde{d}_k \leq \Delta \\
		\Delta + \left( \tilde{d}_k - \Delta \right) \times \frac{\tilde{o}_k}{o_k} & \text{otherwise}
	\end{cases}
\end{equation}

This unified notation allows us to express the adjusted duration for all kernels, whether they complete during the interference window or continue beyond it.

\paragraph{Performance Drop.} Now that we have established a method for estimating interference among simultaneously executing kernels, we can generalize this approach to the inference phase of multiple \glspl{dnn}. Each \gls{dnn} launches a sequence of kernels, and we begin by aligning the first kernel of each \gls{dnn} to form an initial set of concurrently executing kernels. This set is evaluated using the interference model described above. Once the first kernel in the set completes, it is replaced by the next kernel from the same \gls{dnn}, and the process continues iteratively until all models have completed their execution, that is, until all final kernels have been processed.

For a model with an original inference time $T$ and a total of $q$ kernels, the new inference time under interference is given by:
\begin{equation}
	\tilde{T} = \sum_{i=1}^{q} \tilde{d}_{k_i}
\end{equation}

The performance drop experienced by model $m$ due to interference is quantified by the relative increase in inference time. This is computed as:
\begin{equation}\label{eq:performance_drop}
	\mu = \frac{\sum_{i=1}^{q} (\tilde{d}_{k_i} - d_{k_i})}{\sum_{i=1}^{q} d_{k_i}} = \frac{\tilde{T} - T}{T}
\end{equation}

This formulation captures the cumulative slowdown introduced by resource contention across all kernels in the model's execution pipeline.

\paragraph{Kernel Alignment.}
We present a detailed analysis of how kernels from distinct \glspl{dnn} interfere when executed concurrently on a shared \gls{gpu}, and how this interference contributes to performance degradation. To fully characterize this behavior, it is necessary to consider how interference arises during execution. A key factor is the temporal overlap of kernel executions—referred to as alignment—which can occur at any point within a model's kernel sequence. This is the first point in the paper where we actually introduce the meaning of alignment. Perhaps we should give an idea of this concept also in the previous sections where we use this term? In other words, alignment determines which kernels from each model are likely to execute simultaneously and thus compete for \gls{gpu} resources. Each alignment scenario can lead to a distinct performance drop, making it essential to explore a wide range of configurations. Yet, the number of possible alignments grows rapidly with the number of models and kernel positions, making exhaustive evaluation impractical. This concept of alignment is central to understanding and modeling multi-model interference.
% We have developed a detailed understanding of how kernels from distinct \glspl{dnn} may interfere when executed concurrently on a shared GPU and how we can determine the performance drop of \glspl{dnn}. This interference can manifest at any point within a model's kernel sequence during inference, depending on the temporal overlap of execution with other models. Such overlap, which we term alignment, serves as a foundational concept for analyzing performance degradation in multi-model deployments.
% To determine the performance loss of a DNN when sharing a GPU with multiple \glspl{dnn}, we need to test as many kernel alignments as possible to represent as many overlapping cases as possible during their execution.
% In general, alignment refers to the relative positioning of kernels from different models in time. When kernels from separate models are aligned — that is, they execute simultaneously or within overlapping intervals — they are more likely to contend for GPU resources, leading to slowdowns. Each alignement araise a different performance drop.
% This notion of alignment is central to understanding and modeling interference. However, evaluating all possible alignments across multiple models is computationally prohibitive. The challenge lies in the combinatorial nature of the problem: each model can begin interfering from any of its kernels, and the number of possible alignment configurations grows exponentially with the number of models.

To address this, we introduce a greedy heuristic that approximates the impact of alignment without exhaustively enumerating all possibilities.

\subsection{Greedy Algorithm for Estimating Model Interference}

% \qa{The concept of alignment comes too late. I think you should hint at it earlier when you first talk about multiple kernels running together and slow downs. While reading I was wondering how would you calculate $\tilde{o}_k$. And I guess this answer is in this subsection but might be too late.}{Describe earlier (see \#1)}
The analytical framework developed above provides a way to estimate performance degradation due to kernel interference across multiple \glspl{dnn} sharing a \gls{gpu}. However, applying this model exhaustively, by evaluating all possible combinations of kernel alignments across models, is computationally infeasible. For instance, while our previous formulation assumed that all models begin execution with their first kernel simultaneously, a more general scenario would allow each \gls{dnn} to start from any of its $i^{th}$ kernels (where $1 \leq i \leq q$). Enumerating all such combinations would require constructing a full Cartesian product of starting indices, which leads to exponential growth in the search space. For $N$ concurrent \glspl{dnn}, this results in a combinatorial explosion, making real-time evaluation impractical.

To address this, we propose a heuristic algorithm that approximates the interference impact efficiently. The pseudocode is presented in~\Cref{algo:kernel_interference_algorithm}. The key idea is to reduce the search space by:

1. Limiting the number of starting points per model to a subset of $n$ evenly spaced indices from the full set of $q$ kernels.
2. Focusing on \textit{pairwise interference} rather than evaluating all $N$-way combinations.

For each model pair $(m_i, m_j)$, we define a reduced set of starting indices $S_i$ and $S_j$, and construct the set of concurrent execution scenarios as:

\begin{equation}
	\mathcal{C}_{i,j} = S_i \times S_j
\end{equation}

Each scenario corresponds to a pair of starting indices $(s_i, s_j)$, which determine the positions in the kernel sequences where concurrent execution begins (line 8 in~\Cref{algo:kernel_interference_algorithm}). These serve as the basis for simulating localized interference effects between the two models.

This greedy, pairwise strategy dramatically reduces computational overhead while preserving the fidelity needed to estimate performance degradation due to kernel interference. This approximation is motivated by the need to avoid the exponential complexity of evaluating all possible kernel alignments across models. By considering only selected pairs of starting indices, we retain enough coverage to capture meaningful interference effects while keeping the simulation tractable.
% \FF{Here we should give the idea of why considering just the pairs of indexes gives a good approximation of all the huge space. I think this is not here yet.}

\paragraph{Interference Simulation.} For each pair $(m_i, m_j)$, $i \neq j$, and each starting index pair $c_{i,j}=(s_i, s_j)$, we simulate kernel-by-kernel execution. Interference occurs when the combined resource demand of two concurrently executing kernels exceeds the available GPU capacity, as defined by the condition in ~\Cref{eq:interf} : $\varphi_{k_i} + \varphi_{k_j} > \Phi$.

In such cases, the delay added to $k_{s_i}$ (\ie, the kernel identified by the starting point $s_i$) is:

\begin{equation}
	\delta_{k_{s_i}} = d_{k_{s_i}} \cdot \frac{o_{k_{s_i}}}{o_{k_{s_i}} + o_{k_{s_j}}}
\end{equation}

The total delay (additional time) for a given starting pair is:

\begin{equation}
	\Delta^{c_{i,j}} = \sum_{ s_i \leq t \leq q_i} \delta_{k_t}
\end{equation}

We can define the representative additional duration as the median:

\begin{equation}
	\Delta_{i,j} = \text{median}\left(\left\{ \Delta^{c_{i,j}} \right\}_{\forall c_{i,j} \in \mathcal{C}_{i,j}}\right)
\end{equation}

To account for the amount of time two models interact during execution, we introduce a scaling factor that reflects their relative kernel sequence lengths:

\begin{equation}
	\gamma_{i,j} = \max\left(\frac{q_i}{q_j}, 1\right)
\end{equation}

\paragraph{Determine the new duration} The new duration after interference of model $m_i$ is:

\begin{equation}
	\tilde{T_{m_i}} = T_{m_i} + \sum_{\substack{j=1 \\ j \neq i}}^{N} \gamma_{i,j} \cdot \Delta_{i,j}
\end{equation}

Finally, the performance drop can be determined as defined in~\Cref{eq:performance_drop}.


\begin{algorithm}[h!]
	\caption{Greedy Estimation of Model Interference}
	\label{algo:kernel_interference_algorithm}
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{end} % Function name(args) [...] end
	\Fn{performance\_drop($M$)}
	{
		\begin{small}
			Generate starting index $S_i$ for each model as Cartesian product of starting indices across all models.

			Initialize performance drop $\mu \gets []$

			\ForEach{model $m_i \in M$}{
				Initialize $\tilde{T_{m_i}} \gets \sum d_k$

				\ForEach{model $m_j$ where $j \neq i$}{
					Initialize delay set $\mathcal{D}_{i,j} \gets []$

					\ForEach{pair $(s_i, s_j) \in S_i \times S_j$}{
						$k_i \gets s_i$, $k_j \gets s_j$, $\delta \gets 0$

						\While{$k_i < q_i$ and $k_j < q_j$}{
							\If{$\varphi_{k_i} + \varphi_{k_j} > \Phi$}{
								Compute additional duration:

								$\delta \gets \delta + d_{k_i} \cdot \frac{o_{k_i}}{o_{k_i} + o_{k_j}}$
							}
							Increment $k_i$, $k_j$
						}
						Append $\delta$ to $\mathcal{D}_{i,j}$
					}
					Compute overlap factor $\gamma_{i,j} \gets \max\left(\frac{q_i}{q_j}, 1\right)$

					Update $\tilde{T_{m_i}} \gets \tilde{T_{m_i}} + \gamma_{i,j} \cdot \text{median}(\mathcal{D}_{i,j})$
				}
				Finally, the performance drop.

				$\mu_{m_i} \gets \frac{\tilde{T}_m - T_m}{\tilde{T}_m}$

				Append $\mu_{m_i}$ to $\mu$
			}
			\Return{$\mu$}
		\end{small}
	}
\end{algorithm}


\subsection{Placement Algorithm}

The performance drop resulting from \gls{gpu} resource sharing can be exploited to efficiently place new arriving models after deployment query for a new model variant or in case of upscaling meet the workload demand.
When a new model arrives, the objective is to place it in a \gls{gpu} $g$ so that the average performance drop, denoted by $\bar{\mu}^g$, of all running models and the incoming model is minimal. \Cref{algo:model_placement} describes the proposed procedure to place a new model that needs to be deployed. When a new model $m_{arr}$ arrives, \Cref{algo:kernel_interference_algorithm} is applied sequentially to each \gls{gpu} $g$ (line 4 in~\Cref{algo:model_placement}). The algorithm monitors the average performance drop across all deployed models to ensure it does not exceed an acceptable value, $\lambda$, which serves as a tunable parameter. The value of this parameter is chosen to ensure that the arriving model will not cause a significant performance drop in the already running models. Among all the candidate \glspl{gpu} that satisfy the constraint $\bar{\mu}^g < \lambda$ (lines 7--9), the one that offers the lowest average performance drop is selected as the final deployment target. It is worth noting that this algorithm can be easily adapted to other objectives, \eg, selecting the \gls{gpu} that offers the highest throughput or lowest latency. If no suitable \gls{gpu} is identified, the deployment is deferred.

\begin{algorithm}[h!]
	\caption{Model Placement Algorithm}
	\label{algo:model_placement}
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{end} % Function name(args) [...] end
	\Fn{schedule($m_{arr}, G, \lambda$)}
	{
		\begin{small}
			Initialize $p \gets []$ be sequence of performance drops of new model $m_{arr}$.

			Initialize performance drop $\mathcal{P} \gets []$
			% Initialize $s \in \mathbb{R}^p$ be sequence of performance drops of new model $m_{arr}$.

			\ForEach{ GPU $g$ }{
			$M^g \gets M^g \cup \{m_{arr}\}$

			$\mathbf{\mu}^g \gets performance\_drop($M$)$

			\tcc{Ensure no variant has a performance drop beyond $\lambda$.}

			\If{ $\bar{\mu^g} < \lambda$ }{
			Append $\mu^g_{m_{arr}}$ to $p$

			Append $\mathbf{\mu}^g$ to $\mathcal{P}$
			}
			}

			%1. Peak highest throughput.

			%$g* \gets \max \left\{ thr_{m_{arr}} - thr_{m_{arr}} \times \mu^{g}_{m_{arr}} \right\}_{\forall \mu^g_{m_{arr}} \in p}$

			Peak lowest average performance drop.

			$g* \gets \min \left\{ \bar{\mu}^g \right\}_{\forall \mathbf{\mu}^g \in \mathcal{P}}$

			\Return{$g^*$}
		\end{small}
	}
\end{algorithm}