\section{Kernel Interference-Aware Scheduling}\label{sec:kernel_interference_aware_scheduling}

This section explains the main concept behind~\roomie. First, we introduce the idea of interference and describe a method for estimating it. Next, we present an algorithm that efficiently calculates the interference among various models. Finally, we detail a placement algorithm that utilizes the interference estimation from the first procedure to efficiently allocate new incoming models to the available \acrshort{gpu}s.

\subsection{Kernel Interference Estimation}

Understanding how kernels execute is crucial for recognizing how they can interfere with one another. Interference happens when one kernel prevents another from reaching its intended performance level while both are scheduled to run simultaneously. In simpler terms, interference occurs when the demand for resources from multiple kernels at the same time exceeds the total resources that the \acrshort{gpu} can provide. If a kernel is designed to operate at a certain capacity but must share resources with another kernel, it may end up with fewer resources available for its tasks. In this case, we say the kernel is experiencing interference. This interference can lead to lower performance, which can be quantified by examining the kernel's occupancyâ€”a metric that reflects how effectively the \acrshort{gpu}'s computational resources are being utilized during kernel execution. Depending on the severity of resource contention, kernels may still run concurrently but with degraded efficiency, or one kernel may delay the other entirely, forcing sequential execution. In both cases, interference increases latency and can reduce overall throughput. We will demonstrate that accurately estimating this interference at the kernel level is essential for effectively managing the allocation of new models on \acrshort{gpu}s, as this helps to improve overall performance.

\paragraph{Model kernels.}

When a model runs inference on an NVIDIA \acrshort{gpu}, each layer is executed by launching one or multiple kernels, a computational routine that runs in parallel across many threads. These threads are grouped into warps, typically sets of 32 threads that execute instructions in lockstep. Warps ($W$) themselves are organized into blocks ($b$), which serve as the basic scheduling units on the \acrshort{gpu}. Blocks are assigned to \acrlong{sm}s (\acrshort{sm}s), the core computational engines that execute instructions and manage local resources such as registers and shared memory. The efficiency of each kernel depends on how it uses \acrshort{gpu} resources, such as registers allocated per thread and shared memory available during execution. While the full execution model involves additional structural details, we focus here on threads, warps, blocks, and resource usage to highlight the core mechanisms relevant to performance. For a comprehensive description of the \acrshort{cuda} execution model, refer to the official documentation~\cite{nvidia2025cuda}.

From this understanding of kernel execution, we can determine its theoretical occupancy ($o$), which reflects how many warps are active relative to the hardware's capacity by the following equation:

\begin{equation}\label{eq:occupancy}
	o = \frac{W_b \times b}{W}
\end{equation}
,where $W_b$ is the number of warps per block, $b$ is the number of blocks assigned to an \acrshort{sm}, and $W$ is the maximum number of warps the \acrshort{sm} can support. This occupancy is defined at the \acrshort{sm} level and depends on the kernel's configuration.

This value provides an overview of the number of active tasks in relation to the hardware's capacity to execute them simultaneously. It is called theoretical utilization because it represents the optimal scenario in terms of the \acrshort{gpu}'s potential workload. However, it does not guarantee that the kernel will operate efficiently. For a thorough understanding of how it is calculated, please refer to the source in~\cite{lim2017autotuninggpukernelsstatic}.

\paragraph{Kernel Interference.} We characterize a \acrshort{gpu} by its capacity $\Phi$, which encompasses hardware limits such as the maximum number of warps, the size of the register file, and the amount of shared memory available per \acrfull{sm}. These resources constrain the execution of a kernel, \eg, the maximum number of warps, the size of the register file, or the shared memory per multiprocessor~\cite{lim2017autotuninggpukernelsstatic}. For a set of kernels $K$ launched to run on the same \acrshort{gpu}, interference occurs when the total resource demand exceeds the device's capacity. More formally,

\begin{equation}\label{eq:occurrence}
	\sum_{k \in K} \varphi_k > \Phi
\end{equation}
,where $\varphi_k$ (\eg, warps, registers, and shared memory) denotes the resource usage of kernel $k$, and $K$ is the set of active kernels.

\paragraph{Interference Modeling via Theoretical Occupancy Adjustment.} The main challenge lies in estimating the interference each kernel experiences when executed alongside others. This depends on how the theoretical occupancy is recalculated relative to the capacity of the \acrshort{gpu} resources $\Phi$. Since \acrshort{gpu} resources are shared among concurrent kernels, they can be redistributed in different ways, and each allocation yields a different occupancy outcome. Various strategies are possible: equal distribution, first-come-first-served, or allocation based on priorities. Each of these affects the amount of resources allocated to the kernel and its performance. Given that the available resources constrain the maximum number of blocks a kernel can launch, we can compute a new maximum blocks $\tilde{b}_k$ for each kernel $k$, based on the redistribution of the \acrshort{gpu} resources $\Phi$. For details on this calculation, refer to~\cite{lim2017autotuninggpukernelsstatic}, or consult the implementation provided in our open-source code~\sidenote{The~\roomie{} implementation code is freely available at \href{https://github.com/ENSL-NS/RoomieImplementation}{https://github.com/ENSL-NS/RoomieImplementation}}.

With the new block limit $\tilde{b}_k$, we can derive the new occupancy $\tilde{o}_k$ using the same formulation as in~\Cref{eq:occupancy}. This allows us to estimate the new execution time of the kernel under interference:

\begin{equation*}
	\tilde{d}_k = d_k \times \frac{o_k}{\tilde{o}_k}
\end{equation*}
, where $d_k$ is the execution time of kernel $k$ when running in isolation. Worth mentioned that, if $o_k = \tilde{o}_k$, then no interference occurs, and $\tilde{d}_k = d_k$.

Interference ends once the first kernel completes its execution. Assuming at most two kernels are interfering, the remaining kernel continues alone without interference. We define the interference period as the time required for the first kernel to finish, $\Delta = \min \left\{ \tilde{d}_k \right\}_{\forall k \in K}$.
For the remaining kernel, its total duration is composed of two phases: the initial interference period $\Delta$, during which it runs with reduced occupancy $\tilde{o}_k$, and the remaining portion of its execution, which proceeds at full occupancy $o_k$. To account for the change in execution speed, we adjust the remaining time accordingly. Specifically, the portion $\tilde{d}_k - \Delta$, originally computed under reduced occupancy, is scaled by $\frac{\tilde{o}_k}{o_k}$ to reflect the normal execution once interference ends. Formally expressed:

\begin{equation}
	\tilde{d}_k =
	\begin{cases}
		\tilde{d}_k & \text{if } \tilde{d}_k \leq \Delta \\
		\Delta + \left( \tilde{d}_k - \Delta \right) \times \frac{\tilde{o}_k}{o_k} & \text{otherwise}
	\end{cases}
\end{equation}

This unified notation allows us to express the adjusted duration for all kernels, whether they complete during the interference window or continue beyond it.

\paragraph{Performance Drop.} Now that we have established a method for estimating interference among simultaneously executing kernels, we can generalize this approach to the inference phase of multiple \acrshort{dnn}s. Each \acrshort{dnn} launches a sequence of kernels, and we begin by aligning the first kernel of each \acrshort{dnn} to form an initial set of concurrently executing kernels. This set is evaluated using the interference model described above. Once the first kernel in the set completes, it is replaced by the next kernel from the same \acrshort{dnn}, and the process continues iteratively until all models have completed their execution, that is, until all final kernels have been processed.

For a model with an original inference time $T$ and a total of $q$ kernels, the new inference time under interference is given by:
\begin{equation*}
	\tilde{T} = \sum_{i=1}^{q} \tilde{d}_{k_i}
\end{equation*}

The performance drop experienced by model $m$ due to interference is quantified by the relative increase in inference time. This is computed as:
\begin{equation}\label{eq:performance_drop}
	\mu = \frac{\sum_{i=1}^{q} (\tilde{d}_{k_i} - d_{k_i})}{\sum_{i=1}^{q} d_{k_i}} = \frac{\tilde{T} - T}{T}
\end{equation}

This formulation captures the cumulative slowdown introduced by resource contention across all kernels in the model's execution pipeline.

\subsection{Greedy Algorithm for Estimating Model Interference}

The analytical framework developed above provides a way to estimate performance degradation due to kernel interference across multiple deep neural networks (\acrshort{dnn}s) sharing a \acrshort{gpu}. However, applying this model exhaustively, by evaluating all possible combinations of kernel alignments across models, is computationally infeasible. For instance, while our previous formulation assumed that all models begin execution with their first kernel simultaneously, a more general scenario would allow each \acrshort{dnn} to start from any of its $i$-th kernels (where $1 \leq i \leq q$). Enumerating all such combinations would require constructing a full Cartesian product of starting indices, which leads to exponential growth in the search space. For $N$ concurrent \acrshort{dnn}s, this results in a combinatorial explosion, making real-time evaluation impractical.

To address this, we propose a heuristic algorithm that approximates the interference impact efficiently. The pseudocode is presented in Algorithm~\cref{algo:kernel_interference_algorithm}. The key idea is to reduce the search space by:

1. Limiting the number of starting points per model to a subset of $n$ evenly spaced indices from the full set of $q$ kernels.
2. Focusing on \textit{pairwise interference} rather than evaluating all $N$-way combinations.

For each model pair $(m_i, m_j)$, we define a reduced set of starting indices $S_i$ and $S_j$, and construct the set of concurrent execution scenarios as:

\begin{equation*}
	\mathcal{C}_{i,j} = S_i \times S_j
\end{equation*}

Each scenario corresponds to a pair of starting indices $(s_i, s_j)$, which determine the positions in the kernel sequences where concurrent execution begins (line 8 in~\Cref{algo:kernel_interference_algorithm}). These serve as the basis for simulating localized interference effects between the two models.

This greedy, pairwise strategy dramatically reduces computational overhead while preserving the fidelity needed to estimate performance degradation due to kernel interference.

\paragraph{Interference Simulation.} For each pair $(m_i, m_j)$, $i \neq j$, and each starting index pair $c_{i,j}=(s_i, s_j)$, we simulate kernel-by-kernel execution. We approximate the occurrence of interference with the following new condition:
\begin{equation}
	\varphi_{k_i} + \varphi_{k_j} > \Phi
\end{equation}
In such cases, the delay added to $k_{s_i}$ (\ie, the kernel identified by the starting point $s_i$) is:

$$
	\delta_{k_{s_i}} = d_{k_{s_i}} \cdot \frac{o_{k_{s_i}}}{o_{k_{s_i}} + o_{k_{s_j}}}
$$

The total delay (additional time) for a given starting pair is:

$$
	\Delta^{c_{i,j}} = \sum_{ s_i \leq t \leq q_i} \delta_{k_t}
$$

We can define the representative additional duration as the median:

$$
	\Delta_{i,j} = \text{median}\left(\left\{ \Delta^{c_{i,j}} \right\}_{\forall c_{i,j} \in \mathcal{C}_{i,j}}\right)
$$

To account for the amount of time two models interact during execution, we introduce a scaling factor that reflects their relative kernel sequence lengths:

$$
	\gamma_{i,j} = \max\left(\frac{q_i}{q_j}, 1\right)
$$

\paragraph{Determine the new duration} The new duration after interference of model $m_i$ is:

$$
	\tilde{T_{m_i}} = T_{m_i} + \sum_{\substack{j=1 \\ j \neq i}}^{N} \gamma_{i,j} \cdot \Delta_{i,j}
$$

Finally, the performance drop can be determined as defined in~\Cref{eq:performance_drop}.


\begin{algorithm}[t]
	\caption{Greedy Estimation of Model Interference}
	\label{algo:kernel_interference_algorithm}
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{end} % Function name(args) [...] end
	\Fn{performance\_drop($M$)}
	{
		\begin{small}
			Generate starting index $S_i$ for each model as Cartesian product of starting indices across all models.

			Initialize performance drop $\mu \gets []$

			\ForEach{model $m_i \in M$}{
				Initialize $\tilde{T_{m_i}} \gets \sum d_k$

				\ForEach{model $m_j$ where $j \neq i$}{
					Initialize delay set $\mathcal{D}_{i,j} \gets []$

					\ForEach{pair $(s_i, s_j) \in S_i \times S_j$}{
						$k_i \gets s_i$, $k_j \gets s_j$, $\delta \gets 0$

						\While{$k_i < q_i$ and $k_j < q_j$}{
							\If{$\varphi_{k_i} + \varphi_{k_j} > \Phi$}{
								Compute additional duration:

								$\delta \gets \delta + d_{k_i} \cdot \frac{o_{k_i}}{o_{k_i} + o_{k_j}}$
							}
							Increment $k_i$, $k_j$
						}
						Append $\delta$ to $\mathcal{D}_{i,j}$
					}
					Compute overlap factor $\gamma_{i,j} \gets \max\left(\frac{q_i}{q_j}, 1\right)$

					Update $\tilde{T_{m_i}} \gets \tilde{T_{m_i}} + \gamma_{i,j} \cdot \text{median}(\mathcal{D}_{i,j})$
				}
				Finally, the performance drop.

				$\mu_{m_i} \gets \frac{\tilde{T}_m - T_m}{\tilde{T}_m}$

				Append $\mu_{m_i}$ to $\mu$
			}
			\Return{$\mu$}
		\end{small}
	}
\end{algorithm}


\subsection{Placement Algorithm}

The performance drop resulting from \acrshort{gpu} resource sharing can be exploited to efficiently place new arriving models after deployment query for a new model variant or in case of upscaling meet the workload demand.
When a new model arrives, the objective is to place it in a \acrshort{gpu} $g$ so that the average performance drop, denoted by $\bar{\mu}^g$, of all running models and the incoming model is minimal. \Cref{algo:model_placement} describes the proposed procedure to place a new model that needs to be deployed. When a new model $m_{arr}$ arrives, \Cref{algo:kernel_interference_algorithm} is applied sequentially to each \acrshort{gpu} $g$ (line 4). The algorithm monitors the average performance drop across all deployed models to ensure it does not exceed an acceptable value, $\lambda$, which serves as a tunable parameter. The value of this parameter is chosen to ensure that the arriving model will not cause a significant performance drop in the already running models. Among all the candidate \acrshort{gpu}s that satisfy the constraint $\bar{\mu}^g < \lambda$ (lines 7--9), the one that offers the lowest average performance drop is selected as the final deployment target. It is worth noting that this algorithm can be easily adapted to other objectives, \eg, selecting the \acrshort{gpu} that offers the highest throughput or lowest latency. If no suitable \acrshort{gpu} is identified, the deployment is deferred.

\begin{algorithm}[t]
	\caption{Model Placement Algorithm}
	\label{algo:model_placement}
	\SetAlgoLined
	\SetKwProg{Fn}{Function}{}{end} % Function name(args) [...] end
	\Fn{schedule($m_{arr}, G, \lambda$)}
	{
		\begin{small}
			Initialize $p \gets []$ be sequence of performance drops of new model $m_{arr}$.

			Initialize performance drop $\mathcal{P} \gets []$
			% Initialize $s \in \mathbb{R}^p$ be sequence of performance drops of new model $m_{arr}$.

			\ForEach{ \acrshort{gpu} $g$ }{
			$M^g \gets M^g \cup \{m_{arr}\}$

			$\mathbf{\mu}^g \gets performance\_drop($M$)$

			\tcc{Ensure no variant has a performance drop beyond $\lambda$.}

			\If{ $\bar{\mu^g} < \lambda$ }{
			Append $\mu^g_{m_{arr}}$ to $p$

			Append $\mathbf{\mu}^g$ to $\mathcal{P}$
			}
			}

			%1. Peak highest throughput.

			%$g* \gets \max \left\{ thr_{m_{arr}} - thr_{m_{arr}} * \mu^{g}_{m_{arr}} \right\}_{\forall \mu^g_{m_{arr}} \in p}$

			Peak lowest average performance drop.

			$g* \gets \min \left\{ \bar{\mu}^g \right\}_{\forall \mathbf{\mu}^g \in \mathcal{P}}$

			\Return{$g^*$}
		\end{small}
	}
\end{algorithm}