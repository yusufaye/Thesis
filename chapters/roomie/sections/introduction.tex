\section{Introduction}\label{sec:roomie_intro}

\Gls{ml} inference serving has become a foundational task for a variety of domains, with organizations increasingly deploying \gls{ml} models that applications spanning from computer vision to natural language processing~\cite{taye2023understanding}. Unfortunately, the growing demand for \gls{ml} inference requests is now outpacing hardware availability, creating a critical resource gap that forces organizations to maximize utilization of existing computational infrastructure. To process the incoming data streams in real-time while working within these constraints, scalable model serving architectures have been proposed to support a multitude of applications across video analytics, language understanding, recommendation systems, anomaly detection, and more~\cite{ahmad2024proteus,olston2017tensorflowserving,shubha2024usher,francisco2021infaas,mendoza2021interference}. The increasing data volume demands of state-of-the-art models, coupled with tight \glspl{slo} (\eg, latency), has made efficient resource utilization a core challenge across all deployment scenarios.

To address these resource constraints, recent work has explored deploying \gls{ml} inference pipelines across diverse computational environments, ranging from cloud-grade server clusters to resource-constrained edge devices~\cite{mendoza2021interference,hu2021scrooge,ahmad2024proteus,shubha2024usher,cui2021Abacus,2017clipper}.
Cloud platforms such as AWS SageMaker or Google Cloud AI provide scalable compute and storage, enabling high-throughput inference across large workloads. However, these deployments often suffer from network latency and data locality requirements, motivating interest in edge execution where resources are limited but proximity to data is critical.
Conversely, edge deployments, such as running inference on NVIDIA Jetson modules embedded in traffic cameras or smart manufacturing sensors, can reduce latency and mitigate privacy risks by processing data near its source. Yet, these edge devices typically have limited compute and memory, as they are co-located with existing infrastructure like routers or \gls{iot} gateways.
Across both cloud and edge settings, efficient serving depends on two complementary tasks: model placement, which determines where each model is deployed, and query distribution, which governs how inference requests are routed across available resources~\cite{olston2017tensorflowserving,francisco2021infaas}. Existing \gls{ml} inference serving frameworks, such as TensorFlow Serving~\cite{olston2017tensorflowserving} or TorchServe, often assume homogeneous, resource-rich environments and overlook the constraints of edge settings, leading to suboptimal hardware utilization and degraded performance.

While significant research has addressed orchestrating model placement and query distribution~\cite{2017clipper,olston2017tensorflowserving,gujarati2020servingdnnslikeclockwork}, most placement techniques operate by consulting performance profiles to decide where each model should be deployed. These profiles typically capture latency or throughput under isolated conditions, and are then used to guide scheduling decisions across the serving infrastructure. However, such approaches yield limited benefits when the performance profiles are imprecisely characterized or when models operate concurrently, since interference effects are not reflected in the placement logic.
This is particularly problematic in resource-constrained environments where multiple models must share limited hardware and can significantly interfere with each other's execution. Recent work such as Usher~\cite{shubha2024usher} attempted to address this gap by analyzing models at the kernel level to better understand performance under interference conditions. However, their approach fails to account for the complex execution patterns of models on \gls{gpu} architectures, resulting in inaccurate performance predictions (as discussed in detail in Section~\ref{sec:motivation}). This limitation becomes especially critical in resource-constrained deployment scenarios, where even minor performance estimation errors can dramatically impact overall system efficiency, potentially rendering carefully orchestrated deployments infeasible. A more nuanced understanding of model execution characteristics is therefore essential to enable truly efficient inference serving across all computational environments.

In this chapter, we present {\roomie}, a model serving orchestration architecture designed to maximize system performance in scenarios where colocation is necessary across resource-constrained environments. The key contribution of {\roomie} is its kernel-aware offline profiling, which characterizes the execution behavior of individual DNN kernels at runtime. Interference effects are then derived by leveraging these profiles to capture the sequential nature of \gls{gpu} kernel execution when multiple models share hardware resources. By understanding how specific kernel sequences from different models interact, {\roomie} builds accurate interference profiles that predict performance degradation under various colocation scenarios. This fine-grained approach enables {\roomie} to make better informed placement decisions, identifying which models can efficiently coexist on the same hardware and which combinations should be avoided to maintain performance across both goodput and latency.

Our evaluation demonstrates that {\roomie} delivers substantial improvements across both cloud and edge deployments. By accurately modeling interference and guiding colocation, it sustains responsiveness under heavy workloads, reducing \gls{slo} violations to 3$\times$ lower than INFaaS and 2$\times$ lower than Usher in cloud clusters. On edge devices, it achieves similar or slightly better performance, keeping violations far below those of competing baselines even under tight resource constraints. Randomized deployment experiments further confirm scalability, with {\roomie} achieving near-optimal accuracy in about 90\% of trials, clearly outperforming Usher under the same conditions. Taken together, these results establish {\roomie} as a robust and general solution for multi-tenant inference serving, capable of balancing efficiency and responsiveness across diverse \gls{gpu} platforms and deployment contexts.