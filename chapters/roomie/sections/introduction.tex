\section{Introduction}\label{sec:intro}

Machine learning (ML) inference serving has become a foundational task for a variety of domains, with organizations  increasingly deploying ML models that applications spanning from computer vision to natural language processing~\cite{}. Unfortunately, the growing demand for ML inference requests is now outpacing hardware availability, creating a critical resource gap that forces organizations to maximize utilization of existing computational infrastructure. To process the incoming data streams in real-time while working within these constraints, scalable model serving architectures have been proposed to support a multitude of applications across video analytics, language understanding, recommendation systems, anomaly detection, and more~\cite{ahmad2024proteus,olston2017tensorflowserving,shubha2024usher,francisco2021infaas,mendoza2021interference}. The increasing data volume demands of state-of-the-art models, coupled with tight \acrlong{slo}s (\acrshort{slo}s) (\eg, latency), has made efficient resource utilization a core challenge across all deployment scenarios.

To address these resource constraints, recent work has explored deploying ML inference pipelines across diverse computational environments, ranging from cloud-grade server clusters to resource-constrained edge devices~\cite{mendoza2021interference,hu2021scrooge,ahmad2024proteus,shubha2024usher,cui2021Abacus,2017clipper}. For instance, cloud platforms like AWS SageMaker or Google Cloud AI offer scalable compute and storage, enabling high-throughput inference for applications such as real-time fraud detection or large-scale recommendation systems. However, these deployments often suffer from network latency and raise data privacy concerns, especially in domains like healthcare or finance where sensitive data must remain local. Conversely, edge deployments—such as running inference on NVIDIA Jetson modules embedded in traffic cameras or smart manufacturing sensors—can reduce latency and mitigate privacy risks by processing data near its source. Yet, these edge devices typically have limited compute and memory, as they are co-located with existing infrastructure like routers or \acrshort{iot} gateways. Regardless of the deployment environment, efficiently managing available resources requires intelligent distribution of inference requests across the infrastructure~\cite{olston2017tensorflowserving,francisco2021infaas}. Unfortunately, existing ML inference serving frameworks, such as TensorFlow Serving or TorchServe, often assume homogeneous, resource-rich environments and overlook the constraints of edge settings, leading to suboptimal hardware utilization and degraded performance.

While significant research has addressed orchestrating model placement and query distribution~\cite{2017clipper,olston2017tensorflowserving,gujarati2020servingdnnslikeclockwork}, these approaches yield limited benefits when the performance profiles of deployed models are imprecisely characterized or when models operate concurrently. This is particularly problematic in resource-constrained environments where multiple models must share limited hardware and can significantly interfere with each other's execution. Recent work such as Usher~\cite{shubha2024usher} attempted to address this gap by analyzing models at the kernel level to better understand performance under interference conditions. However, their approach fails to account for the complex execution patterns of models on \acrshort{gpu} architectures, resulting in inaccurate performance predictions (as discussed in detail in Section~\ref{sec:motivation}). This limitation becomes especially critical in resource-constrained deployment scenarios, where even minor performance estimation errors can dramatically impact overall system efficiency, potentially rendering carefully orchestrated deployments infeasible. A more nuanced understanding of model execution characteristics is therefore essential to enable truly efficient inference serving across all computational environments.

In this paper, we present~\roomie{}, a model serving orchestration architecture that maximizes system performance in scenarios where colocation is necessary across resource-constrained environments.~\roomie{}'s key contribution is its kernel-aware interference profiling that captures the sequential nature of \acrshort{gpu} kernel execution patterns when multiple models share hardware resources. By understanding how specific kernel sequences from different models interact,~\roomie{} builds accurate interference profiles that predict performance degradation under various colocation scenarios. This fine-grained approach enables~\roomie{} to make better informed placement decisions, identifying which models can efficiently coexist on the same hardware and which combinations should be avoided to maintain performance across both goodput and latency.

Our experimental evaluation demonstrates~\roomie{}'s effectiveness across a diverse set of inference models and deployment scenarios. In cloud deployments,~\roomie{} achieves up to 17× lower latency and sustains over 97\% processing rate, outperforming state-of-the-art solutions~\cite{shubha2024usher,francisco2021infaas} through interference-aware colocation and resilience to saturation. On edge devices, it maintains up to 9× lower response times and 1.5× higher throughput, effectively navigating resource constraints. These improvements come from~\roomie{}'s smart scheduling, which stays close to the best possible setup even under high concurrency. By precisely characterizing interference patterns and adapting to system dynamics,~\roomie{} enables scalable, high-efficiency inference serving across modern \acrshort{gpu} platforms.