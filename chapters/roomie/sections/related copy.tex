\section{Related Work}\label{sec:related}

As more deep learning-based applications are released as online services, managing and scheduling large-scale inference workloads in GPU datacenters has become increasingly critical. Unlike resource-intensive training workloads, inference jobs have unique characteristics and requirements that demand new scheduling solutions. The goals of inference scheduling are multifaceted, including accuracy efficiency, which can be achieved by selecting the best model for each input query and intelligently allocating resources; latency efficiency, which requires optimizing resource allocation to meet response time requirements, even for bursty and fluctuating query requests; and cost-efficiency, which involves minimizing monetary costs when using public cloud resources. These objectives are interdependent, and improving one goal may compromise another if not designed properly, highlighting the need for flexible and comprehensive scheduling systems that can balance tradeoffs between accuracy, latency, and cost.

Despite the growing need for efficient resource allocation, several solutions have been proposed to address this challenge. Clipper is a notable example of an ML inference serving system designed for real-time applications, providing support for a variety of machine learning frameworks and models, and aiming to simplify and accelerate the deployment and serving of ML models. In the other hands, TensorFlow-Serving, a system developed for serving machine learning models for making predictions in real-time. It automatically adjusts to changes in traffic by adding or removing replicas. Nevertheless, none of these designs consider interference prior to deployment models share resources, instead relying on an adaptation mechanism that can lead to further performance degradation. In the other hand, Clockwork was proposed to provide predictable performance for model serving systems, by acknowledging that DNN inference has deterministic performance when running with full GPU capacity. However, their design only execute one inference at a time even when there are multiple models loaded on the GPU. In fact, Clockwork workers only overlap execution of an inference with data loading through two different CUDA Streams. Which leads to GPU underutilization as during inference all kernels that are launched would leave some resources.

"Post scheduling optimization"

Some studies have recognized that deploying multiple models on the same GPU causes interference and can lead to performance. Colti introduce a new technique to improve the efficiency of deep neural network training and inference tasks on GPUs. By colocating DNN tasks on GPUs, they were able to improve system throughput and job completion times.
The others in, they leverage the fact that DNN inference consists of a series of operators executed in certain order according to the data flow dependency, but these operators are independent across multiple DNN. Thus, there is opportunity of scheduling them in flexible way with a certain degree of concurrency by balancing resource utilization to minimize the overall latency of the operators. They achieved this by (1) addressing each DNN to dedicated stream, (2) split and synchronize each stream's full sequence of operators into several shorter stages; (3) then schedule per stage operators while manage the concurrency level and resource utilization. This is a finer-grained concurrency control compare to only using CUDA streams for concurrency. However, considering operators rather than kernels makes it less finer grained not to mention the cost of finding the right slitting and ordonnancing strategy and eventually final latency that synchronization will bring.
Miriam proposes a framework for edge GPU that allows for concurrent running of multiple DNN task with varying levels of real-time performances requirements. It uses elastic kernels, which are smaller, more flexible units that can be dynamically scheduled and remapped to different GPU resources based on priority and cricality.
All of these studies focus on optimizing models that share GPU resources rather than on planning deep neural networks (DNNs) across GPUs. They do not seek to determine the best model deployment (or best colocation strategy), but rather act after deployment or for a given deployment in order to optimize resource utilization or minimize inference time.

"Interference aware DNN scheduling"

For instance, the authors in have developed a unified approach to predict latency degradation for colocated models and across a variety of platforms (CPU/GPU). This latency degradation can be used in inference serving systems to evaluate model placement. However, their approach is not fine-grained as it is based solely on model features that are the utilization of the global buffer and PCIE connection for running on the GPU device. While we recognize that these parameters play an important role, considering them does not provide a more accurate measure of their execution during inference.\\
Another recent investigation into cloud-based inference serving highlights the challenges of model interference when multiple DNNs are run concurrently on a single GPU. Scrooge proposes profiling to determine the optimal concurrency level, beyond which adding more DNNs reduces throughput and increases latency. Although the profiling cost is reported to be low, the method focuses solely on colocating identical DNNs, making it infeasible to profile all potential combinations where different DNNs share the same resources. The sheer number of such combinations would be overwhelming to profile. Proteus investigates the architectural design of an inference-serving system capable of sustaining high throughput while dynamically scaling accuracy to optimize overall performance. The system addresses three interrelated optimization challenges: selecting the appropriate model variants (\eg, lightweight versions with reduced accuracy), distributing these variants across heterogeneous hardware platforms, and distributing query workloads across devices to balance latency and resource utilization. One of Proteus' notable strengths is its adaptive batch processing strategy, which defers query execution within time constraints to aggregate larger batches, thereby improving processing efficiency. However, the framework has limitations in its model placement strategy, as it restricts each device to a single model variant, thereby neglecting the possibilities of simultaneous execution and colocation that could better leverage underutilized GPU resources.

Recent studies have explored the causes of interference in deep neural network (DNN) inference with varying levels of granularity. For instance, Abacus introduces an operator-level scheduling framework that groups operators from multiple DNNs to execute concurrently, aiming to preserve quality-of-service (QoS) guarantees. While this approach models DNNs as sequences of operators (\eg, Convolution, ReLU), it overlooks the finer granularity of GPU execution, where each operator may launch multiple kernels with distinct resource demands. Moreover, Abacus's duration model is agnostic to GPU hardware characteristics, which are critical to inference performance. Collecting sufficient profiling data across diverse GPU architectures and collocation scenarios presents a significant challenge. Additionally, the system enforces deterministic execution by waiting for all operators in a group to complete, potentially leading to underutilization of GPU resources and increased latency. Notably, Abacus operates reactively, analyzing execution only after model deployment, without offering mechanisms to assess model compatibility or predict performance degradation beforehand.\\
In addition, INFaaS, a model-less automated system for distributed inference serving that streamlines the deployment of deep neural networks (DNNs) by allowing users to specify performance and accuracy requirements without selecting specific model variants per query. It dynamically selects or switches model variants based on query arrival and load changes, co-locates variants for efficient resource use, and leverages a VM-Autoscaler to provision workers according to utilization and interference levels. However, the system faces limitations in profiling and generating variants, particularly when considering variant colocation, as the profiling space becomes exponentially large and time-consuming. Additionally, INFaaS lacks a proactive strategy to predict performance degradation due to variant interference, relying instead on reactive state tracking mechanisms to identify overloaded or interfered variants and trigger worker scaling.

"Low level consideration for DNN scheduling on GPUs"

In contrast, iGniter adopts a low-level perspective on GPU resource management by introducing an interference-sensitive inference server tailored for cloud environments. This system aims to mitigate performance degradation caused by concurrent model execution on shared GPU resources. To characterize interference, iGniter employs several hardware-level metrics, including GPU L2 cache usage, the number of launched cores, and power consumption. While the number of launched cores is a meaningful indicator of contention, other parameters such as power and frequency are less predictive of interference severity. More influential factors, such as the configuration and scheduling of cores during kernel launches, play a critical role in shaping performance outcomes. These nuances are often overlooked in coarse-grained models, underscoring the need for more precise profiling techniques.\\
This is acknowledged in a recent article, Usher, which proposes a kernel-level approach to interference mitigation by analyzing the achieved kernel achieved occupancy and DRAM usage during DNN inference. Usher introduces a model classification scheme that distinguishes between compute-intensive and memory-intensive workloads, recognizing that large language models (LLMs), for instance, demand significantly more memory bandwidth than computational throughput. However, from the GPU's perspective, each model ultimately translates into a set of kernels with varying resource demands, independent of the model's high-level classification.

In addition, both Usher and iGniter rely on NVIDIA's Multi-Process Service (MPS) to enable spatial sharing of GPU resources among concurrent inference tasks. While MPS facilitates efficient resource partitioning in cloud environments, it is not supported on edge platforms such as NVIDIA Jetson. This limitation restricts the applicability of these approaches in edge computing scenarios, where hardware constraints and the absence of MPS demand alternative interference-aware scheduling strategies that operate without relying on such infrastructure.










