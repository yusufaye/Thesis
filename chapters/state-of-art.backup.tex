\setchapterpreamble[u]{\margintoc}
\chapter{Related work}

\labch{related_work}


\section{Edge Computing and Video Analytics}

Edge computing represents a transformative shift in distributed computing, wherein computational tasks and data storage are relocated closer to the data source. This paradigm addresses several limitations inherent in centralized cloud architectures, notably by reducing latency, minimizing bandwidth consumption, enhancing data security, and enabling real-time responsiveness. As edge devices—such as sensors, cameras, and IoT nodes—typically operate under constrained computational and energy resources, a variety of architectural models have emerged. These range from hybrid edge-cloud systems, which balance local and remote processing, to fully autonomous edge analytics platforms tailored for specific domains like industrial IoT.

The integration of Graphics Processing Units (GPUs) in edge computing has brought significant advantages. GPUs are designed to handle parallel processing, making them ideal for compute-intensive tasks such as machine learning, computer vision, and data analytics. The use of GPUs in edge computing provides several benefits, including improved performance, increased efficiency, and enhanced scalability. These advantages have paved the way for GPUs to become a cornerstone in enabling advanced edge computing capabilities. By harnessing their parallel processing power, edge systems now support a wide array of real-time applications ranging from autonomous vehicles and smart surveillance to immersive AR/VR experiences and responsive healthcare diagnostics. These applications benefit from edge computing's ability to process data in real-time, reducing latency and improving decision-making. For instance, in IoT, edge computing enables real-time monitoring and control of industrial equipment, improving efficiency and reducing downtime.

% The versatility of edge computing has catalyzed its adoption across numerous sectors. In the Internet of Things (IoT), edge computing facilitates rapid decision-making by processing sensor data locally. Autonomous vehicles rely on edge systems to interpret environmental inputs in real time, ensuring safe navigation. Healthcare applications benefit from immediate diagnostics and monitoring, while surveillance and security systems leverage edge analytics for prompt threat detection. Additionally, augmented and virtual reality (AR/VR) platforms utilize edge computing to deliver immersive experiences with minimal latency.

Among these applications, video analytics stands out as a particularly demanding and impactful use case~\cite{}. In contexts such as smart cities, autonomous driving, and public safety, edge-based video analytics enables timely interpretation of visual data. A typical video analytics pipeline comprises several stages: data ingestion, preprocessing, feature extraction, model inference, and post-processing. Each stage imposes distinct computational and memory requirements, which are often challenging to meet within the limited capabilities of edge devices.

These constraints underscore the need for innovative solutions. Edge devices may lack sufficient processing power, storage capacity, or communication bandwidth to support complex video analytics workloads. Consequently, researchers have explored a range of strategies to optimize resource utilization while preserving real-time performance. These include lightweight model architectures, distributed processing frameworks, and adaptive scheduling algorithms that dynamically allocate tasks across heterogeneous edge nodes.

To address these issues, researchers have proposed a spectrum of solutions aimed at optimizing performance while preserving the benefits of edge locality. Techniques such as model compression and pruning reduce the computational footprint of deep learning algorithms, enabling their deployment on resource-constrained devices. Distributed processing frameworks allow workloads to be partitioned and executed collaboratively across multiple edge nodes, thereby enhancing scalability and fault tolerance. Additionally, adaptive scheduling algorithms and energy-aware resource management strategies have been developed to balance performance with sustainability.



% Edge computing is a distributed computing paradigm that brings computing and data storage closer to the source where the data is produced. This approach aims to reduce latency, bandwidth usage, security risks, and improve real-time processing~\cite{}. Given the limited resources of edge devices, several architectures have emerged. Edge computing systems exist in a wide range of deployments, from hybrid edge and cloud to fully hosted IoT edge analytics~\cite{xu2023edge,hu2023edge}. Edge devices, such as sensors, cameras, and IoT devices, generate data that is then collected and processed by edge gateways or the cloud in a hybrid environment. There is a wide range of applications for edge computing, including: IoT, autonomous vehicles, healthcare, surveillance and security, AR/VR (augmented reality/virtual reality), etc.

% Video analytics is a key application of edge computing, particularly in areas such as surveillance, autonomous vehicles, and smart cities. The typical architecture of a video analytics pipeline includes data ingestion, preprocessing, feature extraction, model inference, and post-processing. However, video analytics in edge environments presents unique challenges, such as real-time processing requirements and resource limitations. For example, edge devices may not have sufficient power, processing power, storage, or communication resources to handle video analytics workloads. A significant range of research has been conducted to improve resource utilization while maintaining high real-time performance. Studies cover various aspects, from architecture deployment to resource and energy consumption efficiency.

% The benefits of edge computing for video analytics include reduced latency, improved real-time processing, and bandwidth conservation. For example, in a surveillance system, edge computing can enable real-time object detection and tracking, allowing for quick responses to security threats. In addition, edge computing can reduce the amount of data that needs to be transmitted to the cloud or central server, thereby reducing bandwidth usage. However, edge computing also presents several challenges, including resource limitations, security, and management. To address these challenges, researchers and practitioners have proposed various techniques, such as edge computing-based video analysis frameworks, model optimization, and distributed processing. For example, a study by Zhang et al. (2020) [4] proposes a distributed video analysis framework that leverages edge computing and cloud computing to optimize processing and reduce latency.

% Edge computing is a distributed computing paradigm that brings computation and data storage closer to the source where data is produced. This approach aims to reduce latency, bandwidth usage, security, and improve real-time processing~\cite{}. With the limited resources of edge devices multiple architectures have emerged. Edge computing systems exist across a spectrum of deployments, from hybrid edge and cloud to fully hosted IoT edge analytics~\cite{xu2023edge,hu2023edge}. Edge devices, such as sensors, cameras, and IoT devices, generate data, which is then collected and processed by edge gateways or cloud in hybrid. There is a wide range of applications for edge computing, including: IoT, Autonomous vehicles, Healthcare, Surveillance and security, AR/VR (Augmented Reality/Virtual Reality), etc.

% Video analytics is a critical application of edge computing, particularly in domains such as surveillance, autonomous vehicles, and smart cities. The typical pipeline architecture of video analytics consists of data ingestion, preprocessing, feature extraction, model inference, and post-processing. However, video analytics in edge environments presents unique challenges, such as real-time processing requirements and resource limitations. For instance, edge devices may not have sufficient energy, processing, storage, or communication resources to handle video analytics workloads. Un ensemble important de recherches ont ete produits pour ameliorer l'utilisation des resources while maitaining a high performance in real-time. The studies vary from architecture deployment to efficient resource and power consumption.

% The benefits of edge computing for video analytics include reduced latency, improved real-time processing, and conservation of bandwidth. For example, in a surveillance system, edge computing can enable real-time object detection and tracking, allowing for swift response to security threats. Additionally, edge computing can reduce the amount of data that needs to be transmitted to the cloud or central server, thereby reducing bandwidth usage. However, edge computing also presents several challenges, including resource limitations, security, and management. 

% To address these challenges, researchers and practitioners have proposed various techniques, such as edge-based video analytics frameworks, model optimization, and distributed processing. For instance, a study by Zhang et al. (2020) [4] proposes a distributed video analytics framework that leverages edge computing and cloud computing to optimize processing and reduce latency.

\section{Load Balancing for Live Video Analytics}
% "Load Balancing for Live Video Analytics"
% "Efficient Workload Distribution in Live Video Analytics"
% "Load Balancing Techniques for Real-Time Video Analytics"
% "Optimizing Workload Distribution in Live Video Analytics"
% "Load Balancing Strategies for Live Video Processing"
% "Workload Management in Real-Time Video Analytics"
% "Balancing Loads in Live Video Analytics Systems"
% "Live Video Analytics: Load Balancing Techniques"
% "Distributing Workloads in Real-Time Video Analytics"
% "Load Balancing for Real-Time Video Processing"

% Model Serving Orchestration: Review existing model serving orchestration architectures, focusing on their strengths and weaknesses. Discuss the challenges of model colocation in resource-constrained environments, including interference and performance degradation.
% Load Balancing Techniques: Survey existing load balancing techniques for edge computing, with a focus on those relevant to video analytics. Discuss distributed load balancing approaches and the use of machine learning models in load balancing.

\section{Inference Serving and Resource Management in Edge Cloud Computing}

% For the past years, many works has been proposed for inference serving in cloud. Many existing works have addressed this challenge for hardware optimization. Que ça soit dans une architecture le cloud, ou bien dans une architecture edge, ou hybrid, l'allocation de resources est devenue un problème centrale pour les applications de service d'inference où plusieurs modèles DNN doivent être déployé dans des resources limités.

% Clipper has disigned a ML inference serving system for real-time applications. It offers support for a variety of machine learning frameworks and models. Clipper aims to make deploying and serving machine learning models easier and fast. However, their desing doesn't take into account the interference when multiple models have to share resources, rather use an adaptation mechanism which can lead to further degradation of performance.

As more deep learning-based applications are released as online services, managing and scheduling large-scale inference workloads in GPU datacenters has become increasingly critical. Unlike resource-intensive training workloads, inference jobs have unique characteristics and requirements that demand new scheduling solutions~\sidecite{10.1145/3638757}. The goals of inference scheduling are multifaceted, including accuracy efficiency, which can be achieved by selecting the best model for each input query and intelligently allocating resources; latency efficiency, which requires optimizing resource allocation to meet response time requirements, even for bursty and fluctuating query requests; and cost-efficiency, which involves minimizing monetary costs when using public cloud resources. However, these objectives are interdependent, and improving one goal may compromise another if not designed properly, highlighting the need for flexible and comprehensive scheduling systems that can balance tradeoffs between accuracy, latency, and cost.

Over the past few years, numerous works have been proposed for inference serving in cloud environments, with many existing studies addressing the challenge of hardware optimization. However, as machine learning models are increasingly deployed in cloud, edge, or hybrid architectures, resource allocation has become a central problem for inference serving applications, particularly when multiple Deep Neural Network (DNN) models need to be deployed on limited resources.

---
Abacus~\sidecite{cui2021Abacus} aims to efficiently schedule and execute simultaneous DNN queries in the lower framework level. By considering DNN as sequence of operator (layers), they adapt an operator grouping mechanism that group operators from different DNNs to run simultaneously while ensuring QoS guarantees. The interference is determined in a coarse grained manner by consider operators (DNN layers) rather than the actual kernels that are launched and executed in the GPU. What's more their proposed \textit{duration model} is agnostic to the GPU hardware which plays a fundamental rule during inference. Collecting sufficient data by also considering the GPU types and different level of collocation would be very difficult. Furthermore, while waiting for all operators in an operator group to finish, in order to guarantee a deterministic execution order, this will result in underutilization of the GPU and increased latency, as operators outside the group may wait unnecessarily. Also, they only consider low-level execution of operator after models are already deployed but didn't develop a proactive solution that determine the compatibility of models or able to measure the performance drop before deployment.

Clockwork~\sidecite{gujarati2020servingdnnslikeclockwork} aims to provide predictable performance for model serving systems, by acknowledging that DNN inference has deterministic performance. Their design only execute one inference at a time even when there are multiple models loaded on the GPU. However, this leads to GPU underutilization.
---

Despite the growing need for efficient resource allocation, several solutions have been proposed to address this challenge. Clipper~\sidecite{2017Clipper} is a notable example of an ML inference serving system designed for real-time applications, providing support for a variety of machine learning frameworks and models, and aiming to simplify and accelerate the deployment and serving of ML models. In the other hands, TensorFlow Serving, a system developed for serving machine learning models for making predictions in real-time. It automatically adjust to changes in traffic by adding or removing replicas. Nevertheless, non of these designs counsider interference prior to deployment models share resources, instead relying on an adaptation mechanism that can lead to further performance degradation.

Some studies ont reconnu le fait que le déploiement de plusieurs modèles dans le meme gpu présente des interference et conduit éventuellement à une perte de performance. For instance, the others of~\sidecite{mendoza2021interference} has develop a unified approach to predict letency degradation for colocated models and across a varity number of platform (CPU/GPU). This latency degradation can be used in inference serving system to evaluate model placement. Cependant, their approach is not fine grained as they sollely based on model features that are the utilisation of the Global buffer, PCIE connection for running on the GPU device. Meme si nous reconnaissons que ces paramètres jouent un role important, leur considération ne donnent pas une mesure plus fine de leur execution pendant l'inférence.
An other recent investigation into cloud-based inference serving, \sidecite{hu2021scrooge} highlights the challenges of model interference when multiple DNNs are run concurrently on a single GPU. The study proposes profiling to determine the optimal concurrency level, beyond which adding more DNNs reduces throughput and increases latency. Although the profiling cost is reported to be low, the method focuses solely on colocating identical DNNs, making it infeasible to profile all potential combinations where different DNNs share the same resources. The sheer number of such combinations would be overwhelming to profile.

Previous research has delved deeper into the root causes of interference. The authors of~\sidecite{xu2022igniter} presented an interference-sensitive GPU inference server designed for cloud environments. Their work focused on mitigating interference that occurs when multiple models simultaneously use GPU resources. They used a set of critical parameters, including GPU L2 cache usage, number of cores launched, and GPU power consumption, to evaluate interference between models. While we recognize the importance of the number of cores launched in determining interference between multiple models, other parameters such as power consumption and frequency alone are not the most significant factors in causing interference between models sharing the same GPU resources. Later sections will get more into details. Other key factors, such as the configuration of cores during launches, play a more important role. Ceci est reconnue in a recent article which has used the achieved occupancy of kernel and DRAM Utilization to caraterize resource utilization required by a each kernel which are launched during DNN inference. After avoir reconnue l'interference de models DNN during inference which leads to performance drop, Usher, propose a new approach that leverage from kernel level understand when they are launched for execution in the GPU. They, in fact, consider models as either computer heavy or memory, where the latter requires more memory than computation (such as LLM), than the former which relies more on computation power. However, when it comes to executing in the GPU, each layers on the. Furthermore, both Usher and iGnifer approaches rely on the use of MPS (multi-process service) which allows multiple inference workloads to spatially share the GPU, However, this is not available on Edge platforms such as Jetson.


% Interference in Edge Computing: Define interference in the context of edge computing and its impact on performance. Review existing interference profiling techniques, with a focus on kernel-aware approaches.
% Model Cohabitation Techniques: Survey existing techniques for model cohabitation in edge computing, highlighting their strengths and limitations. Discuss methods for predicting performance degradation under various colocation scenarios and approaches for making optimal placement decisions to maximize system performance.